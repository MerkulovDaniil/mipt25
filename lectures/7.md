---
title: Двойственность
author: Даня Меркулов
institute: Методы оптимизации. МФТИ
format: 
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back9.jpeg}
---

# Двойственность

## Мотивация

Двойственность позволяет связать любую задачу оптимизации с ограничениями с задачей **вогнутой максимизации**, решения которой дают **нижнюю границу** для оптимального значения исходной задачи.  
Интересно то, что существуют случаи, когда можно решить **прямую задачу**, сначала решив **двойственную**.  
Рассмотрим общую задачу оптимизации с ограничениями:

. . .

$$
\text{ Primal: }f(x) \to \min\limits_{x \in S}  \qquad \text{ Dual: } g(y) \to \max\limits_{y \in \Omega} 
$$

. . .

Строим функцию $g(y)$, удовлетворяющую **универсальному (равномерному) неравенству**:

$$
g(y) \leq f(x) \qquad \forall x \in S, \forall y \in \Omega
$$

. . .

В результате получаем:

$$
\max\limits_{y \in \Omega} g(y) \leq \min\limits_{x \in S} f(x)  
$$


## Лагранжева двойственность

Рассмотрим один из возможных способов построения функции $g(y)$ в случае, когда у нас есть общая задача математического программирования с функциональными ограничениями:

. . .

$$
\begin{split}
& f_0(x) \to \min\limits_{x \in \mathbb{R}^n}\\
\text{s.t. } & f_i(x) \leq 0, \; i = 1,\ldots,m\\
& h_i(x) = 0, \; i = 1,\ldots, p
\end{split}
$$

. . .

И лагранжиан, соответствующий этой задаче:

$$
L(x, \lambda, \nu) = f_0(x) + \sum\limits_{i=1}^m \lambda_i f_i(x) + \sum\limits_{i=1}^p\nu_i h_i(x) = f_0(x) + \lambda^\top f(x) + \nu^\top h(x)
$$


## Двойственная функция

Пусть $\mathcal{D} = \bigcap\limits_{i=0}^m\textbf{dom } f_i \cap \bigcap\limits_{i=1}^p\textbf{dom } h_i$ — непустое множество.  
Определим **лагранжеву двойственную функцию** (или просто **двойственную функцию**)  
$g: \mathbb{R}^m \times \mathbb{R}^p \to \mathbb{R}$ как минимальное значение лагранжиана по $x$, для $\lambda \in \mathbb{R}^m$, $\nu \in \mathbb{R}^p$

. . .

$$
g(\lambda, \nu) = \inf_{x \in \mathcal{D}} L(x, \lambda, \nu) = \inf_{x \in \mathcal{D}} \left( f_0(x) +\sum\limits_{i=1}^m \lambda_i f_i(x) + \sum\limits_{i=1}^p\nu_i h_i(x) \right)
$$

. . .

Если лагранжиан не ограничен снизу по $x$, то двойственная функция принимает значение $-\infty$.  
Так как двойственная функция является **поточечным инфимумом семейства аффинных функций** от $(\lambda, \nu)$,  
она является **вогнутой**, даже если исходная задача не является выпуклой.


## Двойственная функция как нижняя граница

:::: {.columns}

::: {.column width="50%"}
Покажем, что двойственная функция даёт **нижнюю границу** для оптимального значения $p^*$ исходной задачи при любых $\lambda \succeq 0, \nu$.  
Пусть некоторая точка $\hat{x}$ является допустимой для исходной задачи, то есть $f_i(\hat{x}) \leq 0$ и $h_i(\hat{x}) = 0$, при $\lambda \succeq 0$. Тогда имеем:

. . .

$$
L(\hat{x}, \lambda, \nu) = f_0(\hat{x}) + \underbrace{\lambda^\top f(\hat{x})}_{\leq 0} + \underbrace{\nu^\top h(\hat{x})}_{= 0} \leq f_0(\hat{x})
$$

. . .

Следовательно,

$$
g(\lambda, \nu) = \inf_{x \in \mathcal{D}} L(x, \lambda, \nu) \leq L(\hat{x}, \lambda, \nu)  \leq f_0(\hat{x})
$$

. . .

$$
g(\lambda, \nu) \leq p^*
$$

:::

. . .

::: {.column width="50%"}

Возникает естественный вопрос: **какую наилучшую нижнюю границу** можно получить из лагранжевой двойственной функции?  
Это приводит нас к следующей задаче оптимизации:

. . .

$$
\begin{split}
& g(\lambda, \nu) \to \max\limits_{\lambda \in \mathbb{R}^m, \; \nu \in \mathbb{R}^p }\\
\text{s.t. } & \lambda \succeq 0
\end{split}
$$

. . .

Теперь становится понятным термин **«допустимая для двойственной задачи» (dual feasible)**, обозначающий пару $(\lambda, \nu)$, для которой $\lambda \succeq 0$ и $g(\lambda, \nu) > -\infty$.  
Это означает, как следует из названия, что $(\lambda, \nu)$ является допустимым решением для двойственной задачи.  
Если пара $(\lambda^*, \nu^*)$ оптимальна для этой задачи, то её называют **оптимальными лагранжевыми множителями** или **оптимальными двойственными переменными**.

:::
::::


## Саммари

|  | Прямая задача | Двойственная задача |
|:--:|:--:|:--:|
| Функция | $f_0(x)$ | $g(\lambda, \nu) = \min\limits_{x \in \mathcal{D}} L(x, \lambda, \nu)$ |
| | | |
| Переменные | $x \in S \subseteq \mathbb{R}^n$ | $\lambda \in \mathbb{R}^m_{+}, \nu \in \mathbb{R}^p$ |
| | | |
| Ограничения | $f_i(x) \leq 0$, $i = 1,\ldots,m$ $h_i(x) = 0, \; i = 1,\ldots, p$ | $\lambda_i \geq 0, \forall i \in \overline{1,m}$ |
| | | |
| Задача | $\begin{matrix}& f_0(x) \to \min\limits_{x \in \mathbb{R}^n}\\ \text{s.t. } & f_i(x) \leq 0, \; i = 1,\ldots,m\\ & h_i(x) = 0, \; i = 1,\ldots, p \end{matrix}$ | $\begin{matrix}  g(\lambda, \nu) &\to \max\limits_{\lambda \in \mathbb{R}^m, \nu \in \mathbb{R}^p }\\ \text{s.t. } & \lambda \succeq 0 \end{matrix}$ | 
| | | |
| Оптимум | $\begin{matrix} &x^* \text{ если допустимо},  \\ &p^* = f_0(x^*)\end{matrix}$ | $\begin{matrix} &\lambda^*, \nu^* \text{ если максимум достигается},  \\ &d^* = g(\lambda^*, \nu^*)\end{matrix}$ |


## Пример. Линейный МНК

Мы рассматриваем задачу в пределах непустого множества допустимых решений, определяемого следующим образом:

. . .

$$
\begin{aligned}
    & \text{min} \quad x^T x \\
    & \text{s.t.} \quad Ax = b,
\end{aligned}
$$

где матрица $A \in \mathbb{R}^{m \times n}$.

. . .

В данной задаче отсутствуют неравенства — присутствуют только $m$ **линейных равенств**.  
Лагранжиан имеет вид $L(x, \nu) = x^T x + \nu^T (Ax - b)$ и определён на области $\mathbb{R}^n \times \mathbb{R}^m$.  
Двойственная функция обозначается как $g(\nu) = \inf_x L(x, \nu)$.  
Так как $L(x, \nu)$ является **выпуклой квадратичной функцией** по $x$, минимизирующее значение $x$ можно найти из **условия оптимальности**

. . .

:::: {.columns}

::: {.column width="50%"}

$$
\nabla_x L(x, \nu) = 2x + A^T \nu = 0,
$$

. . .

откуда следует $x = -(1/2)A^T \nu$.  
В результате двойственная функция имеет вид:

. . .

$$
g(\nu) = L(-(1/2)A^T \nu, \nu) = -(1/4)\nu^T A A^T \nu - b^T \nu,
$$

:::

. . .

::: {.column width="50%"}

что представляет собой **вогнутую квадратичную функцию** на области $\mathbb{R}^p$.  
Согласно свойству нижней границы, для любого $\nu \in \mathbb{R}^p$ выполняется неравенство:

. . .

$$
-(1/4)\nu^T A A^T \nu - b^T \nu \leq \inf\{x^T x \,|\, Ax = b\}.
$$

Это простая, но нетривиальная **нижняя граница** без необходимости решать саму задачу.
:::
::::


## Пример. Задача двустороннего разбиения

:::: {.columns}

::: {.column width="60%"}

Мы рассматриваем (невыпуклую) задачу:
$$
\begin{aligned}
    & \text{min} \quad x^T W x \\
    & \text{s.t.} \quad x_i^2 =1, \quad i=1,\ldots,n,
\end{aligned}
$$

. . .

![Иллюстрация задачи двустороннего разбиения](partition.pdf){width=100%}
:::

. . .

::: {.column width="40%"}

Эту задачу можно рассматривать как задачу двустороннего разбиения множества из $n$ элементов, обозначаемого как $\{1, \ldots , n\}$: допустимый вектор $x$ соответствует разбиению
$$
\{1,\ldots,n\} = \{i|x_i =-1\} \cup \{i|x_i =1\}.
$$

. . .

Коэффициент $W_{ij}$ в матрице представляет собой стоимость помещения элементов $i$ и $j$ в одну и ту же группу, тогда как $-W_{ij}$ означает цену их разделения. Целевая функция отражает суммарную стоимость по всем парам элементов, и поставленная задача состоит в том, чтобы найти разбиение, минимизирующее общие затраты.

:::

::::


## Пример. Задача двустороннего разбиения

Теперь выведем двойственную функцию для этой задачи. Лагранжиан имеет вид
$$
L(x,\nu) = x^T W x + \sum_{i=1}^n \nu_i (x_i^2 -1) = x^T (W + \text{diag}(\nu)) x - \mathbf{1}^T \nu.
$$

. . .

Минимизируя по $x$, получаем двойственную функцию Лагранжа: 
$$
g(\nu) = \inf_x x^T (W + \text{diag}(\nu)) x - \mathbf{1}^T \nu
= \begin{cases}\begin{array}{ll}
    -\mathbf{1}^T\nu & \text{если } W+\text{diag}(\nu) \succeq 0 \\
    -\infty & \text{в противном случае},
\end{array} \end{cases}
$$

. . .

Используется тот факт, что инфимум квадратичной формы равен нулю, если форма положительно полуопределённая, и равен $-\infty$, если нет.

. . .

Эта двойственная функция задаёт нижние границы на оптимальное значение задачи. Например, можно выбрать конкретное значение двойственной переменной:
$$
\nu = -\lambda_{\text{min}}(W) \mathbf{1}
$$

. . .

Это значение допустимо в двойственной задаче, так как $W +\text{diag}(\nu)=W -\lambda_{\text{min}}(W) I \succeq 0.$

. . .

Отсюда получается простая нижняя граница для оптимального значения $p^*$: $p^* \geq -\mathbf{1}^T\nu = n \lambda_{\text{min}}(W).$

. . .

Код для этой задачи доступен здесь: [\faPython Open in Colab](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Partitioning.ipynb)


# Сильная двойственность

## Сильная двойственность

Связь между оптимальными значениями прямой и двойственной задач обычно называется **слабой двойственностью**. Для задачи выполняется:

$$
p^* \geq d^*
$$

. . .

Разность между ними часто называют **зазором двойственности** (*duality gap*):

$$
p^* - d^* \geq 0
$$

. . .

Заметьте, что слабая двойственность выполняется всегда, если прямая и двойственная задачи корректно сформулированы. Это означает, что если нам удалось решить двойственную задачу (которая всегда вогнутая, независимо от того, была ли исходная задача выпуклой), то мы получаем некоторую нижнюю границу. Удивительно, но существуют случаи, когда эти решения совпадают.

. . .

**Сильная двойственность** имеет место, если зазор двойственности равен нулю: $p^* = d^*$.

. . .

Замечание: как $p^*$, так и $d^*$ могут быть равны $\infty$.

* Известно несколько достаточных условий!
* “Простые” необходимые и достаточные условия — неизвестны.

## Сильная двойственность в задаче линейных наименьших квадратов

:::{.callout-exercise}
В примере с решением системы линейных уравнений методом наименьших квадратов выше вычислите оптимум прямой задачи $p^*$ и оптимум двойственной задачи $d^*$ и проверьте, выполняется ли для этой задачи сильная двойственность.
:::

## Полезные свойства двойственности

* **Построение нижней границы для решения прямой задачи.**

    Иногда исходную задачу решить крайне сложно. Но если у нас есть двойственная задача, можно взять произвольное $y \in \Omega$ и подставить его в $g(y)$ — мы сразу получим некоторую нижнюю границу.

* **Проверка разрешимости задачи и достижимости решения.** 

    Из неравенства $\max\limits_{y \in \Omega} g(y) \leq \min\limits_{x \in S} f_0(x)$ следует: если $\min\limits_{x \in S} f_0(x) = -\infty$, то $\Omega = \varnothing$, и наоборот.

* **Иногда проще решить двойственную задачу, чем прямую.** 

    В этом случае, если выполняется сильная двойственность $g(y^*) = f_0(x^*)$, мы ничего не теряем.

* **Получение нижней границы на остаток функции.** 

    $f_0(x) - f_0^* \leq f_0(x) - g(y)$ для произвольного $y \in \Omega$ (сертификат субоптимальности). Более того, $p^* \in [g(y), f_0(x)], \ d^* \in [g(y), f_0(x)]$

* **Двойственная функция всегда вогнута**

    Так как она является поточечным минимумом аффинных функций.


## Условие Слейтера 

:::{.callout-theorem}
Если для выпуклой задачи оптимизации (предполагаем min, функции $f_0,f_{i}$ выпуклые, а $h_{i}$ аффинные) существует точка $x$ такая, что $h(x)=0$ и $f_{i}(x)<0$ (существует строго допустимая точка), то зазор двойственности равен нулю, а условия KKT становятся необходимыми и достаточными.
:::

## Пример выпуклой задачи, когда условие Слейтера не выполняется

:::{.callout-example}

$$
\min \{ f_0(x) = x \mid f_1(x) = \tfrac{x^2}{2} \leq 0 \}, 
$$

. . .

Единственная точка допустимого множества: $x^* = 0$. Однако невозможно найти неотрицательное $\lambda^* \geq 0$ такое, что 
$$
\nabla f_0(0) + \lambda^* \nabla f_1(0) = 1 + \lambda^* x = 0.
$$

:::


## Невыпуклая квадратичная задача с сильной двойственностью

:::: {.columns}

::: {.column width="35%"}

Иногда сильная двойственность выполняется даже для невыпуклой задачи. В качестве важного примера рассмотрим задачу минимизации невыпуклой квадратичной функции на единичном шаре:

. . .

$$
\begin{split}
& x^\top A x  + 2b^\top x \to \min\limits_{x \in \mathbb{R}^{n}}\\
\text{s.t. } & x^\top x \leq 1 
\end{split}
$$

. . .

где $A \in \mathbb{S}^n, \ A \nsucceq 0$ и $b \in \mathbb{R}^n$. Поскольку $A \nsucceq 0$, задача невыпуклая. Эту задачу иногда называют *trust region problem* — задачей доверительной области; она возникает при минимизации второй аппроксимации функции на единичном шаре, в пределах которого эта аппроксимация считается примерно корректной.

:::

. . .

::: {.column width="65%"}

**Решение**

. . .

Лагранжиан и двойственная функция:

$$
L(x, \lambda) = x^\top A x + 2 b^\top x + \lambda (x^\top x - 1) = x^\top( A + \lambda I)x + 2 b^\top x - \lambda
$$

. . .

$$
g(\lambda) = \begin{cases} -b^\top(A + \lambda I)^{\dagger}b - \lambda &\text{если } A + \lambda I \succeq 0 \\ -\infty, &\text{иначе}  \end{cases}
$$

. . .

Двойственная задача:

$$
\begin{split}
& -b^\top(A + \lambda I)^{\dagger}b - \lambda \to \max\limits_{\lambda \in \mathbb{R}}\\
\text{s.t. } & A + \lambda I \succeq 0
\end{split}
$$

. . .

$$
\begin{split}
& -\sum\limits_{i=1}^n \dfrac{(q_i^\top b)^2}{\lambda_i + \lambda} - \lambda  \to \max\limits_{\lambda \in \mathbb{R}}\\
\text{s.t. } & \lambda \geq - \lambda_{min}(A)
\end{split}
$$
:::
::::


# Приложения

## Решение прямой задачи через двойственную

Важное следствие условия стационарности: при выполнении сильной двойственности, если известны решения двойственной задачи $\lambda^*, \nu^*$, то любое решение прямой задачи $x^*$ удовлетворяет
$$
\min_{x\in \mathbb{R}^n} f_0(x) + \sum\limits_{i=1}^m \lambda^*_i f_i(x) + \sum\limits_{i=1}^p\nu^*_i h_i(x)
$$
Часто решения этой задачи без ограничений можно записать **в явном виде**, что позволяет явно выразить решения прямой задачи через решения двойственной.

Более того, если решение этой задачи единственно, то оно и есть решение прямой задачи $x^*$.

Это бывает очень полезно, когда двойственную задачу решить проще, чем прямую.

## Решение прямой задачи через двойственную

:::: {.columns}

::: {.column width="50%"}

Рассмотрим, например:

$$
\min_{x} \sum_{i=1}^n f_i(x_i) \quad \text{s.t.} \quad a^T x = b
$$

. . .

где каждая $f_i(x_i) = \frac{1}{2} c_i x_i^2$ (гладкая и строго выпуклая). Двойственная функция:
$$
\begin{aligned}
\uncover<+->{g(\nu) &= \min_{x} \sum_{i=1}^n f_i(x_i) + \nu \big(b - a^T x\big) \\}
\uncover<+->{&= b\nu + \sum_{i=1}^n \min_{x_i} \big\{ f_i(x_i) - a_i \nu x_i \big\} \\}
\uncover<+->{&= b\nu - \sum_{i=1}^n f_i^*(a_i \nu),}
\end{aligned}
$$

. . .

где каждая $f_i^*(y) = \frac{1}{2c_i} y^2$ — это сопряжённая функция к $f_i$.

:::

. . .

::: {.column width="50%"}

Следовательно, двойственная задача имеет вид:
$$
\max_{\nu} \, b\nu - \sum_{i=1}^n f_i^*(a_i \nu) \quad \iff \quad \min_{\nu} \, \sum_{i=1}^n f_i^*(a_i \nu) - b\nu
$$

. . .

Это задача выпуклой минимизации с одной скалярной переменной — значительно проще, чем прямая.

. . .

Если известно $\nu^\star$, то решение прямой задачи $x^\star$ удовлетворяет:
$$
\min_{x} \sum_{i=1}^n \big(f_i(x_i) - a_i \nu^\star x_i\big)
$$

. . .

Строгая выпуклость каждой функции $f_i$ гарантирует, что решение единственно, и его можно найти из условия $f_i'(x_i) = a_i \nu^\star$ для каждого $i$. 

. . .

Отсюда получаем:
$$
x_i^\star = \frac{a_i \nu^\star}{c_i}.
$$

:::

::::


## Смешанные стратегии для матричных игр

:::: {.columns}

::: {.column width="65%"}

![Схема смешанной стратегии в матричной игре](msmg.pdf)

:::

. . .

::: {.column width="35%"}

В нулевой (zero-sum) матричной игре игроки 1 и 2 выбирают действия из множеств $\{1,...,n\}$ и $\{1,...,m\}$ соответственно. Результатом является выплата от игрока 1 игроку 2, определяемая платежной матрицей $P \in \mathbb{R}^{n \times m}$. Каждый игрок использует **смешанные стратегии**, то есть выбирает действия согласно вероятностному распределению: игрок 1 — с вероятностями $u_k$ для каждого действия $i$, а игрок 2 — с вероятностями $v_l$.

. . .

Математическое ожидание выигрыша (выплаты от игрока 1 игроку 2) задаётся выражением $\sum_{k=1}^{n} \sum_{l=1}^{m} u_k v_l P_{kl} = u^T P v$. Игрок 1 стремится **минимизировать** это ожидаемое значение, а игрок 2 — **максимизировать**.

:::
::::

## Смешанные стратегии для матричных игр. Перспектива игрока 1

:::: {.columns}

::: {.column width="30%"}
![](msmg_1.pdf)
:::
::: {.column width="70%"}
Пусть игрок 2 знает стратегию игрока 1 $u$. Тогда он выберет $v$, максимизирующее $u^T P v$. В наихудшем случае для игрока 1 ожидаемый выигрыш равен:

$$
\max_{v \geq 0, 1^T v = 1} u^T P v = \max_{i=1,...,m} (P^T u)_i
$$

. . .

Оптимальная стратегия игрока 1 минимизирует это наихудшее значение, что приводит к следующей задаче оптимизации:

$$
\begin{split}
& \min \max_{i=1,...,m} (P^T u)_i\\
& \text{s.t. } u \geq 0 \\
& 1^T u = 1
\end{split}
$$ {#eq-player1-problem}

Это выпуклая задача оптимизации, оптимальное значение которой обозначается как $p^*_1$.

:::
::::

## Смешанные стратегии для матричных игр. Перспектива игрока 2

:::: {.columns}

::: {.column width="30%"}
![](msmg_2.pdf)
:::
::: {.column width="70%"}

Аналогично, если игрок 1 знает стратегию игрока 2 $v$, его цель — минимизировать $u^T P v$. Это приводит к задаче:

$$
\min_{u \geq 0, 1^T u = 1} u^T P v = \min_{i=1,...,n} (P v)_i
$$

. . .

Игрок 2 затем максимизирует это значение, чтобы получить наибольшую гарантированную выплату, решая следующую задачу оптимизации:

$$
\begin{split}
& \max \min_{i=1,...,n} (P v)_i \\
& \text{s.t. }  v \geq 0 \\
& 1^T v = 1
\end{split}
$$ {#eq-player2-problem}

Оптимальное значение здесь обозначается как $p^*_2$.
:::
::::


## Смешанные стратегии для матричных игр

### Двойственность и эквивалентность

Обычно знание стратегии противника даёт преимущество, однако, что удивительно, в случае смешанных стратегий для матричных игр это преимущество исчезает. Ключ заключается в двойственности: приведённые выше задачи являются двойственными по Лагранжу. Если записать задачу игрока 1 как задачу линейного программирования и ввести множители Лагранжа, то двойственная задача совпадёт с задачей игрока 2. Благодаря сильной двойственности, выполняющейся для допустимых задач линейного программирования, имеем $p^*_1 = p^*_2$, что показывает отсутствие преимущества в знании стратегии противника.

. . .

### Постановка и решение двойственной задачи Лагранжа

Рассмотрим задачу (1), записав её в виде задачи линейного программирования (LP). Цель — минимизировать переменную $t$ при следующих ограничениях:

. . .

1. $u \geq 0$,
2. Сумма элементов вектора $u$ равна 1 ($1^T u = 1$),
3. $P^T u \leq t \mathbf{1}$ (вектор не превосходит $t$ поэлементно).

. . .

Здесь $t$ — дополнительная переменная, принимающая значения на множестве действительных чисел ($t \in \mathbb{R}$).

. . .

### Построение лагранжиана

. . .

Введём множители для ограничений: $\lambda$ — для $P^T u \leq t \mathbf{1}$, $\mu$ — для $u \geq 0$, и $\nu$ — для $1^T u = 1$. Тогда лагранжиан имеет вид:

. . .

$$
L = t + \lambda^T (P^T u - t \mathbf{1}) - \mu^T u + \nu (1 - 1^T u) = \nu + (1 - 1^T \lambda)t + (P\lambda - \nu \mathbf{1} - \mu)^T u
$$

. . .

## Смешанные стратегии для матричных игр

:::: {.columns}

::: {.column width="70%"}
### Определение двойственной функции

. . .

Двойственная функция $g(\lambda, \mu, \nu)$ определяется как:

. . .

$$
g(\lambda, \mu, \nu) = 
\begin{cases} 
\nu & \text{если } 1^T\lambda=1 \text{ и } P\lambda - \nu \mathbf{1} = \mu \\
-\infty & \text{в противном случае} 
\end{cases}
$$

. . .

### Решение двойственной задачи

Двойственная задача заключается в максимизации $\nu$ при следующих условиях:

1. $\lambda \geq 0$,
2. Сумма элементов вектора $\lambda$ равна 1 ($1^T \lambda = 1$),
3. $\mu \geq 0$,
4. $P\lambda - \nu \mathbf{1} = \mu$.

. . .

После исключения $\mu$ получаем двойственную по Лагранжу задачу к (1):

. . .

$$
\begin{split}
& \max \nu \\
& \text{s.t. }   \lambda \geq 0, \ \  1^T \lambda = 1, \ \ P\lambda \geq \nu \mathbf{1}
\end{split}
$$

:::

. . .

::: {.column width="30%"}

### Заключение

Эта постановка показывает, что двойственная по Лагранжу задача эквивалентна задаче @eq-player2-problem. Так как оба этих линейных программирования допустимы, выполняется сильная двойственность, а значит, оптимальные значения задач (1) и (2) совпадают.

:::
::::


## Источники
* [Lecture](http://www.csc.kth.se/utbildning/kth/kurser/DD3364/Lectures/KKT.pdf) on KKT conditions (very intuitive explanation) in the course "Elements of Statistical Learning" @ KTH.
* [One-line proof of KKT](https://link.springer.com/content/pdf/10.1007%2Fs11590-008-0096-3.pdf)
* [On the Second Order Optimality Conditions for
Optimization Problems with Inequality Constraints](https://www.scirp.org/pdf/OJOp_2013120315191950.pdf)
* [On Second Order Optimality Conditions in
Nonlinear Optimization](https://www.ime.usp.br/~ghaeser/secondorder.pdf)
* [Numerical Optimization](https://www.math.uci.edu/~qnie/Publications/NumericalOptimization.pdf) by Jorge Nocedal and Stephen J. Wright. 
* Duality Uses and Correspondences lecture by Ryan Tibshirani [course](https://www.stat.cmu.edu/~ryantibs/convexopt/).
