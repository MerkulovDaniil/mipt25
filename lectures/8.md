---
title: "Optimality conditions. KKT"
author: Daniil Merkulov
institute: Optimization methods. MIPT
format: 
    beamer:
        pdf-engine: pdflatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/header.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back8.jpeg}
---



:::: {.columns}
::: {.column width="65%"}
> The reader will find no figures in this work. The methods which I set forth do not require either constructions or geometrical or mechanical reasonings: but only algebraic operations, subject to a regular and uniform rule of procedure.

Preface to MÃ©canique analytique
:::

::: {.column width="35%"}
![Joseph-Louis Lagrange](lagrange.jpg)
:::

::::

# Optimization with inequality constraints

## Example of inequality constraints

$$
f(x) = x_1^2 + x_2^2 \;\;\;\; g(x) = x_1^2 + x_2^2 - 1
$$

$$
\begin{split}
& f(x) \to \min\limits_{x \in \mathbb{R}^n} \\
\text{s.t. } & g(x) \leq 0
\end{split}
$$

## Optimization with inequality constraints

![Illustration of KKT (inequality case)](ineq_constr_1.pdf)

## Optimization with inequality constraints

![Illustration of KKT (inequality case)](ineq_constr_2.pdf)

## Optimization with inequality constraints

![Illustration of KKT (inequality case)](ineq_constr_3.pdf)

## Optimization with inequality constraints

![Illustration of KKT (inequality case)](ineq_constr_4.pdf)

## Optimization with inequality constraints

Thus, if the constraints of the type of inequalities are inactive in the constrained problem, then don't worry and write out the solution to the unconstrained problem. However, this is not the whole story. Consider the second childish example

$$
f(x) = (x_1 - 1)^2 + (x_2 + 1)^2 \;\;\;\; g(x) = x_1^2 + x_2^2 - 1
$$

$$
\begin{split}
& f(x) \to \min\limits_{x \in \mathbb{R}^n} \\
\text{s.t. } & g(x) \leq 0
\end{split}
$$

## Optimization with inequality constraints

![Illustration of KKT (inequality case)](ineq_constr_5.pdf)

## Optimization with inequality constraints

![Illustration of KKT (inequality case)](ineq_constr_6.pdf)

## Optimization with inequality constraints

![Illustration of KKT (inequality case)](ineq_constr_7.pdf)

## Optimization with inequality constraints

![Illustration of KKT (inequality case)](ineq_constr_8.pdf)

## Optimization with inequality constraints

![Illustration of KKT (inequality case)](ineq_constr_9.pdf)

## Optimization with inequality constraints

![Illustration of KKT (inequality case)](ineq_constr_10.pdf)

## Optimization with inequality constraints

![Illustration of KKT (inequality case)](ineq_constr_11.pdf)

## Optimization with inequality constraints

So, we have a problem:

$$
\begin{split}
& f(x) \to \min\limits_{x \in \mathbb{R}^n} \\
\text{s.t. } & g(x) \leq 0
\end{split}
$$

Two possible cases:

:::: {.columns}

::: {.column width="40%"}
$g(x) \leq 0$ is inactive. $g(x^*) < 0$

* $g(x^*) < 0$
* $\nabla f(x^*) = 0$
* $\nabla^2 f(x^*) > 0$

:::

. . .

::: {.column width="60%"}
$g(x) \leq 0$ is active. $g(x^*) = 0$

* $g(x^*) = 0$
* Necessary conditions: $- \nabla f(x^*) = \lambda \nabla g(x^*)$, $\lambda > 0$
* Sufficient conditions: $\langle y, \nabla^2_{xx} L(x^*, \lambda^*) y \rangle > 0, \forall y \neq 0 \in \mathbb{R}^n : \nabla g(x^*)^\top y = 0$
:::

::::

## Lagrange function for inequality constraints

:::: {.columns}

::: {.column width="35%"}

Combining two possible cases, we can write down the general conditions for the problem:

$$
\begin{split}
& f(x) \to \min\limits_{x \in \mathbb{R}^n} \\
\text{s.t. } & g(x) \leq 0
\end{split}
$$

Let's define the Lagrange function:

$$
L(x, \lambda) = f(x) + \lambda g(x)
$$

The classical Karush-Kuhn-Tucker first and second-order optimality conditions for a local minimizer $x^*$, stated under some regularity conditions, can be written as follows.

:::

. . .

::: {.column width="65%"}

If $x^*$ is a local minimum of the problem described above, then there exists a unique Lagrange multiplier $\lambda^*$ such that:

$$
\begin{split}
\uncover<+->{& (1) \; \nabla_x L (x^*, \lambda^*) = 0 }\\
\uncover<+->{& (2) \; \lambda^* \geq 0 }\\
\uncover<+->{& (3) \; \lambda^* g(x^*) = 0 }\\
\uncover<+->{& (4) \; g(x^*) \leq 0}\\
\uncover<+->{& (5) \; \forall y \in C(x^*):  \langle y , \nabla^2_{xx} L(x^*, \lambda^*) y \rangle > 0 }\\
\uncover<+->{&  \text{where } C(x^*) = \{y \ \in \mathbb{R}^n |  \nabla f(x^*)^\top y \leq 0 \text{ and } \forall i \in I(x^*):  \nabla g_i(x^*)^T y \leq 0 \} \text{ is the critical cone.} }\\
\uncover<+->{& I(x^*) = \{i \mid g_i(x^*) = 0\}}
\end{split}
$$

:::
::::

# KKT

## General formulation

$$
\begin{split}
& f_0(x) \to \min\limits_{x \in \mathbb{R}^n}\\
\text{s.t. } & f_i(x) \leq 0, \; i = 1,\ldots,m\\
& h_i(x) = 0, \; i = 1,\ldots, p
\end{split}
$$

This formulation is a general problem of mathematical programming. 

The solution involves constructing a Lagrange function: 

$$
L(x, \lambda, \nu) = f_0(x) + \sum\limits_{i=1}^m \lambda_i f_i(x) + \sum\limits_{i=1}^p\nu_i h_i(x)
$$

## Necessary conditions
Let $x^*$, $(\lambda^*, \nu^*)$ be a solution to a mathematical programming problem with zero duality gap (the optimal value for the primal problem $p^*$ is equal to the optimal value for the dual problem $d^*$). Let also the functions $f_0, f_i, h_i$ be differentiable.

* $\nabla_x L(x^*, \lambda^*, \nu^*) = 0$
* $\nabla_\nu L(x^*, \lambda^*, \nu^*) = 0$
* $\lambda^*_i \geq 0, i = 1,\ldots,m$
* $\lambda^*_i f_i(x^*) = 0, i = 1,\ldots,m$
* $f_i(x^*) \leq 0, i = 1,\ldots,m$

## Some regularity conditions
These conditions are needed to make KKT solutions the necessary conditions. Some of them even turn necessary conditions into sufficient (for example, Slater's). Moreover, if you have regularity, you can write down necessary second order conditions $\langle y, \nabla^2_{xx} L(x^*, \lambda^*, \nu^*) y \rangle \geq 0$ with *semi-definite* hessian of Lagrangian.

* **Slater's condition.** If for a convex problem (i.e., assuming minimization, $f_0,f_{i}$ are convex and $h_{i}$ are affine), there exists a point $x$ such that $h(x)=0$ and $f_{i}(x)<0$ (existence of a strictly feasible point), then we have a zero duality gap and KKT conditions become necessary and sufficient.
* **Linearity constraint qualification.** If $f_{i}$ and $h_{i}$ are affine functions, then no other condition is needed.
* **Linear independence constraint qualification.** The gradients of the active inequality constraints and the gradients of the equality constraints are linearly independent at $x^*$.  
* For other examples, see [wiki](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions#Regularity_conditions_(or_constraint_qualifications)).

## Proof in simple case

:::{.callout-theorem}

## Subdifferential form of KKT

Let $X$ be a linear normed space, and let $f_j: X \to \mathbb{R}$, $j = 0, 1, \ldots, m$, be convex proper (it never takes on the value $-\infty$ and also is not identically equal to $\infty$) functions. Consider the problem
$$
\begin{split}
& f_0(x) \to \min\limits_{x \in X}\\
\text{s.t. } & f_j(x) \leq 0, \; j = 1,\ldots,m\\
\end{split}
$$
Let $x^* \in X$ be a minimum in problem above and the functions $f_j$, $j = 0, 1, \ldots, m$, be continuous at the point $x^*$. Then there exist numbers $\lambda_j \geq 0$, $j = 0, 1, \ldots, m$, such that
$$
\sum_{j=0}^{m} \lambda_j = 1,
$$
$$
\lambda_j f_j(x^*) = 0, \quad j = 1, \ldots, m,
$$
$$
0 \in \sum_{j=0}^{m} \lambda_j \partial f_j(x^*).
$$

:::

## Proof in simple case

::::{.columns}

::: {.column width="50%"}

**Proof**

1. Consider the function

    $$
    f(x) = \max\{f_0(x) - f_0(x^*), f_1(x), \ldots, f_m(x)\}.
    $$

    The point $x^*$ is a global minimum of this function. Indeed, if at some point $x_e \in X$ the inequality $f(x_e) < 0$ were satisfied, it would imply that $f_0(x_e) < f_0(x^*)$ and $f_j(x_e) < 0$, $j = 1, \ldots, m$, contradicting the minimality of $x^*$ in problem above. 

2. Then, from Fermat's theorem in subdifferential form, it follows that 

    $$
    0 \in \partial f(x^*).
    $$

:::
::: {.column width="50%"}

3. By the Dubovitskii-Milyutin theorem, we have

    $$
    \partial f(x^*) = \text{conv } \left( \bigcup\limits_{j \in I}\partial f_j(x^*)\right),
    $$

    where $I = \{0\} \cup \{j : f_j(x^*) = 0, 1 \leq j \leq m\}$. 

4. Therefore, there exist $g_j \in \partial f_j(x^*)$, $j \in I$, such that

    $$
    \sum_{j \in I} \lambda_j g_j = 0, \quad \sum\limits_{j \in I}\lambda_j = 1, \quad \lambda_j \geq 0, \quad j \in I.
    $$

    It remains to set $\lambda_j = 0$ for $j \notin I$.
:::
::::



## Example. Projection onto a hyperplane

$$
\min \frac{1}{2}\|\mathbf{x} - \mathbf{y}\|^2, \quad \text{s.t.} \quad \mathbf{a}^T\mathbf{x} = b.
$$

. . .

**Solution**

Lagrangian:

. . .

$$
L(\mathbf{x}, \nu) = \frac{1}{2}\|\mathbf{x} - \mathbf{y}\|^2 + \nu(\mathbf{a}^T\mathbf{x} - b)
$$

. . .

Derivative of $L$ with respect to $\mathbf{x}$:

$$
\frac{\partial L}{\partial \mathbf{x}} = \mathbf{x} - \mathbf{y} + \nu\mathbf{a} = 0, \qquad \mathbf{x} = \mathbf{y} - \nu\mathbf{a}
$$

. . .

$$
\mathbf{a}^T\mathbf{x} = \mathbf{a}^T\mathbf{y} - \nu\mathbf{a}^T\mathbf{a} \qquad \nu = \dfrac{\mathbf{a}^T\mathbf{y} - b}{\|\mathbf{a}\|^2}
$$

. . .

$$
\mathbf{x} = \mathbf{y} - \dfrac{\mathbf{a}^T\mathbf{y} - b}{\|\mathbf{a}\|^2}\mathbf{a}
$$


## Example. Projection onto simplex

$$
\min \frac{1}{2} \lVert x - y \rVert^2, \quad \text{s.t.} \quad x^\top 1 = 1, \quad x \geq 0. \quad x
$$

. . .

#### KKT Conditions

The Lagrangian is given by:

$$
L = \frac{1}{2} \lVert x - y \rVert^2 - \sum_i \lambda_i x_i + \nu (x^\top 1 - 1)
$$

. . .

Taking the derivative of $L$ with respect to $x_i$ and writing KKT yields:

* $\frac{\partial L}{\partial x_i} = x_i - y_i - \lambda_i + \nu = 0$
* $\lambda_i x_i = 0$
* $\lambda_i \geq 0$
* $x^\top 1 = 1, \quad x \geq 0$

. . .

::::{.columns}

::: {.column width="50%"}

:::{.callout-question}
Solve the above conditions in $O(n \log n)$ time.
:::
:::

. . .

::: {.column width="50%"}
:::{.callout-question}
Solve the above conditions in $O(n)$ time.
:::
:::
::::

## References
* [Lecture](http://www.csc.kth.se/utbildning/kth/kurser/DD3364/Lectures/KKT.pdf) on KKT conditions (very intuitive explanation) in the course "Elements of Statistical Learning" @ KTH.
* [One-line proof of KKT](https://link.springer.com/content/pdf/10.1007%2Fs11590-008-0096-3.pdf)
* [On the Second Order Optimality Conditions for
Optimization Problems with Inequality Constraints](https://www.scirp.org/pdf/OJOp_2013120315191950.pdf)
* [On Second Order Optimality Conditions in
Nonlinear Optimization](https://www.ime.usp.br/~ghaeser/secondorder.pdf)
* [Numerical Optimization](https://www.math.uci.edu/~qnie/Publications/NumericalOptimization.pdf) by Jorge Nocedal and Stephen J. Wright. 