---
title: "Градиентные методы для задач с ограничениями. Метод проекции градиента. Метод Франк-Вульфа. Метод зеркального спуска"
author: Даня Меркулов
institute: Методы оптимизации. МФТИ
format:
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
 - \newcommand{\bgimage}{../files/back17.jpeg}
---

# Методы с ограничениями

## Условная оптимизация

:::: {.columns}

::: {.column width="50%"}

### Безусловная оптимизация

$$
\min_{x \in \mathbb{R}^n} f(x)
$$

* Любая точка $x_0 \in \mathbb{R}^n$ допустима и может быть решением.

:::

. . .

::: {.column width="50%"}

### Условная оптимизация

$$
\min_{x \in S} f(x)
$$

* Не все $x \in \mathbb{R}^n$ допустимы и могут быть решением.
* Решение должно лежать внутри множества $S$.
* Пример:
    $$
    \frac12\|Ax - b\|_2^2 \to \min_{\|x\|_2^2 \leq 1}
    $$

:::

::::

. . .

Градиентный спуск — отличный способ решения безусловных задач
$$
\tag{GD}
x_{k+1} = x_k - \alpha_k \nabla f(x_k)
$$
Можно ли адаптировать градиентный спуск для задачи с ограничениями?

. . .

**Да**. Для этого нужно использовать проекции, чтобы обеспечить допустимость на каждой итерации.

## Пример: adversarial white-box attacks

:::: {.columns}

::: {.column width="55%"}

![[Источник](https://arxiv.org/abs/1811.07018)](adversarial.jpeg)

:::

::: {.column width="45%"}

* Математически нейронная сеть — это функция $f(w; x)$
* Обычно вход $x$ задан, а веса сети $w$ оптимизируются
* Но можно зафиксировать веса $w$ и оптимизировать $x$!

$$
\min_{\delta} \text{size}(\delta) \quad \text{s.t.} \quad \text{pred}[f(w;x+\delta)] \neq y
$$
или
$$
\max_{\delta} l(w; x+\delta, y) \; \text{s.t.} \; \text{size}(\delta) \leq \epsilon, \; 0 \leq x+\delta \leq 1
$$
:::
::::


## Идея метода проекции градиента

![Предположим, мы стартуем из точки $x_k$.](PGD1.pdf)

## Идея метода проекции градиента{.noframenumbering}

![И движемся в направлении $-\nabla f(x_k)$.](PGD2.pdf)

## Идея метода проекции градиента{.noframenumbering}

![Иногда мы можем оказаться за пределами допустимого множества.](PGD3.pdf)

## Идея метода проекции градиента{.noframenumbering}

![Решим эту маленькую проблему с помощью проекции!](PGD4.pdf)

## Идея метода проекции градиента

$$
x_{k+1} = \text{proj}_S\left(x_k - \alpha_k \nabla f(x_k) \right)  \qquad \Leftrightarrow \qquad \begin{aligned}
y_k &= x_k - \alpha_k \nabla f(x_k) \\
x_{k+1} &= \text{proj}_S\left( y_k\right)
\end{aligned}
$$

![Иллюстрация метода проекции градиента](PGD.pdf)

# Проекция

## Проекция

Расстояние $d$ от точки $\mathbf{y} \in \mathbb{R}^n$ до замкнутого множества $S \subset \mathbb{R}^n$:
$$
d(\mathbf{y}, S, \| \cdot \|) = \inf\{\|x - y\| \mid x \in S \}
$$

. . .

Мы сосредоточимся на евклидовой проекции (возможны и другие варианты) точки $\mathbf{y} \in \mathbb{R}^n$ на множество $S \subseteq \mathbb{R}^n$ — это точка $\text{proj}_S(\mathbf{y}) \in S$:
$$
\text{proj}_S(\mathbf{y}) =\underset{\mathbf{x} \in S}{\operatorname{argmin}}  \frac12 \|x - y\|_2^2
$$

. . .

* **Достаточные условия существования проекции**. Если $S \subseteq \mathbb{R}^n$ — замкнутое множество, то проекция на множество $S$ существует для любой точки.
* **Достаточные условия единственности проекции**. Если $S \subseteq \mathbb{R}^n$ — замкнутое выпуклое множество, то проекция на множество $S$ единственна для любой точки.
* Если множество открытое, а точка лежит вне этого множества, то её проекция на него может не существовать.
* Если точка принадлежит множеству, то её проекция — это она сама.

## Критерий проекции (неравенство Бурбаки-Чейни-Гольдштейна)

:::: {.columns}

::: {.column width="65%"}

:::{.callout-theorem}
\small
Пусть $S \subseteq \mathbb{R}^n$ — замкнутое и выпуклое множество, $\forall x \in S, y \in \mathbb{R}^n$. Тогда
$$
\langle y - \text{proj}_S(y), \mathbf{x} - \text{proj}_S(y)\rangle \leq 0
$$ {#eq-proj1}
$$
\|x - \text{proj}_S(y)\|^2 + \|y - \text{proj}_S(y)\|^2 \leq \|x-y\|^2
$$ {#eq-proj2}
:::

1. $\text{proj}_S(y)$ - минимизатор дифференцируемой выпуклой функции $d(y, S, \| \cdot \|) = \|x - y\|^2$ на множестве $S$. Оптимальность:
    $$
    \begin{aligned}
    \uncover<+->{\nabla d(\text{proj}_S(y))^T(x - \text{proj}_S(y))&\geq 0 \\ }
    \uncover<+->{2\left(\text{proj}_S(y) - y \right)^T(x - \text{proj}_S(y))&\geq 0 \\ }
    \uncover<+->{\left(y - \text{proj}_S(y) \right)^T(x - \text{proj}_S(y))&\leq 0}
    \end{aligned}
    $$
:::

::: {.column width="35%"}
![Угол должен быть тупым или прямым для любой точки $x \in S$](proj_crit.pdf)
:::
::::

2. Используем теорему косинусов $2x^Ty = \|x\|^2 + \|y\|^2 - \|x-y\|^2$ с $x = x - \text{proj}_S(y)$ и $y = y - \text{proj}_S(y)$.
    $$
    \begin{aligned}
    \uncover<+->{ 0 \geq 2x^Ty = \|x - \text{proj}_S(y)\|^2 + \|y + \text{proj}_S(y)\|^2 - \|x-y\|^2 \\ }
    \uncover<+->{ \|x - \text{proj}_S(y)\|^2 + \|y + \text{proj}_S(y)\|^2 \leq \|x-y\|^2 }
    \end{aligned}
    $$

## Оператор проекции - нерастягивающий

* Функция $f$ называется нерастягивающей, если она является $L$-липшицевой с константой $L \leq 1$ ^[Нерастягивающая функция становится сжимающей при $L < 1$.]. То есть для любых двух точек $x,y \in \text{dom} f$
    $$
    \|f(x)-f(y)\| \leq L\|x-y\|, \text{ где } L \leq 1.
    $$
    Это означает, что расстояние между образами точек не превосходит расстояния между самими точками.

* Оператор проекции является нерастягивающим:
    $$
    \| \text{proj}(x) - \text{proj}(y) \|_2 \leq \| x - y \|_2.
    $$

* Далее покажем, что из неравенства Бурбаки-Чейни-Гольдштейна следует свойство нерастягиваемости, а именно:
    $$
    \langle y - \text{proj}(y), x - \text{proj}(y) \rangle \leq 0 \quad \forall x \in S \qquad \Rightarrow \qquad \| \text{proj}(x) - \text{proj}(y) \|_2 \leq \| x - y \|_2.
    $$

## Оператор проекции - нерастягивающий

Сокращённая запись: пусть $\pi = \text{proj}$ и $\pi(x)$ обозначает $\text{proj}(x)$.

. . .

Начнём с неравенства Бурбаки-Чейни-Гольдштейна
$$
\langle y-\pi(y) , x-\pi(y) \rangle \leq 0 \quad \forall x \in S.
$$ {#eq-proj1}

. . .

:::: {.columns}

::: {.column width="50%"}
Заменим $x$ на $\pi(x)$ в (-@eq-proj1)
$$
\langle y-\pi(y), \pi(x)-\pi(y) \rangle \leq 0.
$$ {#eq-proj2}
:::

. . .

::: {.column width="50%"}
Заменим $y$ на $x$ и $x$ на $\pi(y)$ в (-@eq-proj1)

$$
\langle x-\pi(x), \pi(y)-\pi(x) \rangle \leq 0.
$$ {#eq-proj3}
:::

::::

. . .

(-@eq-proj2)+(-@eq-proj3) сократят $\pi(y) - \pi(x)$, что нежелательно. Поэтому сменим знак в (-@eq-proj3):
$$
\langle \pi(x)-x, \pi(x)-\pi(y) \rangle \leq 0.
$$ {#eq-proj4}

. . .

:::: {.columns}

::: {.column width="60%"}
$$
\begin{split}
\langle y-\pi(y)+\pi(x)-x , \pi(x)-\pi(y) \rangle & \leq 0 \\
\langle y - x, \pi(x) - \pi(y) \rangle & \leq -\langle \pi(x)-\pi(y), \pi(x)-\pi(y) \rangle \\
\langle y - x, \pi(y) - \pi(x) \rangle & \geq \lVert \pi(x) - \pi(y) \rVert^2_2 \\
\lVert (y - x)^\top (\pi(y) - \pi(x)) \rVert_2 & \geq \lVert \pi(x) - \pi(y) \rVert^2_2
\end{split}
$$
:::

. . .

::: {.column width="40%"}
По неравенству КБШ левая часть ограничена сверху величиной $\lVert y - x \rVert_2 \lVert \pi(y) - \pi(x) \rVert_2$, откуда следует $\lVert y - x \rVert_2 \lVert \pi(y) - \pi(x) \rVert_2 \geq \lVert \pi(x) - \pi(y) \rVert^2_2$. Сокращая на $\lVert \pi(x) - \pi(y) \rVert_2$, завершаем доказательство.
:::

::::

## Пример: проекция на шар

Найдём $\pi_S (y) = \pi$, если $S = \{x \in \mathbb{R}^n \mid \|x - x_0\| \le R \}$, $y \notin S$

. . .

Построим гипотезу по рисунку: $\pi = x_0 + R \cdot \frac{y - x_0}{\|y - x_0\|}$

. . .

:::: {.columns}

::: {.column width="60%"}
Проверим неравенство для замкнутого выпуклого множества: $(\pi - y)^T(x - \pi) \ge 0$

. . .

$$
\begin{split}
\left( x_0 - y + R \frac{y - x_0}{\|y - x_0\|} \right)^T\left( x - x_0 - R \frac{y - x_0}{\|y - x_0\|} \right) &= \\
\left( \frac{(y - x_0)(R - \|y - x_0\|)}{\|y - x_0\|} \right)^T\left( \frac{(x-x_0)\|y-x_0\|-R(y - x_0)}{\|y - x_0\|} \right) &= \\
\frac{R - \|y - x_0\|}{\|y - x_0\|^2} \left(y - x_0 \right)^T\left( \left(x-x_0\right)\|y-x_0\|-R\left(y - x_0\right) \right) &= \\
\frac{R - \|y - x_0\|}{\|y - x_0\|} \left( \left(y - x_0 \right)^T\left( x-x_0\right)-R\|y - x_0\| \right) &= \\
\left(R - \|y - x_0\| \right) \left( \frac{(y - x_0 )^T( x-x_0)}{\|y - x_0\|}-R \right) & \\
\end{split}
$$
:::

. . .

::: {.column width="40%"}
Первый множитель отрицателен по выбору точки $y$. Второй множитель также отрицателен, что следует из КБШ:

. . .

$$
\begin{split}
(y - x_0 )^T( x-x_0) &\le \|y - x_0\|\|x-x_0\| \\
\frac{(y - x_0 )^T( x-x_0)}{\|y - x_0\|} - R &\le \frac{\|y - x_0\|\|x-x_0\|}{\|y - x_0\|} - R = \|x - x_0\| - R \le 0
\end{split}
$$

![Шар](proj_ball.pdf){width=60%}

:::
::::

## Пример: проекция на полупространство

Найдём $\pi_S (y) = \pi$, если $S = \{x \in \mathbb{R}^n \mid c^T x = b \}$, $y \notin S$. Построим гипотезу по рисунку: $\pi = y + \alpha c$. Коэффициент $\alpha$ выбирается так, чтобы $\pi \in S$: $c^T \pi = b$, тогда:

. . .

:::: {.columns}

::: {.column width="50%"}
![Гиперплоскость](proj_half.pdf)
:::

. . .

::: {.column width="50%"}
$$
\begin{split}
c^T (y + \alpha c) &= b \\
c^Ty + \alpha c^T c &= b \\
c^Ty &= b - \alpha c^T c \\
\end{split}
$$
Проверим неравенство для замкнутого выпуклого множества: $(\pi - y)^T(x - \pi) \ge 0$
$$
\begin{split}
(y + \alpha c - y)^T(x - y - \alpha c) =& \\
\alpha c^T(x - y - \alpha c) =& \\
\alpha (c^Tx) - \alpha (c^T y) - \alpha^2 (c^Tc) =& \\
\alpha b - \alpha (b - \alpha c^T c) - \alpha^2 c^Tc =& \\
\alpha b - \alpha b + \alpha^2 c^T c - \alpha^2 c^Tc =& 0 \ge 0
\end{split}
$$
:::

::::

# Метод проекции градиента (PGD)

## Идея

$$
x_{k+1} = \text{proj}_S\left(x_k - \alpha_k \nabla f(x_k) \right)  \qquad \Leftrightarrow \qquad \begin{aligned}
y_k &= x_k - \alpha_k \nabla f(x_k) \\
x_{k+1} &= \text{proj}_S\left( y_k\right)
\end{aligned}
$$

![Иллюстрация алгоритма метода проекции градиента](PGD.pdf)

## Инструменты для доказательства сходимости \faGem \ \faGem[regular] \faGem[regular] \faGem[regular]

:::{.callout-theorem}
Пусть $f: \mathbb{R}^n \rightarrow \mathbb{R}$ — выпуклая функция с $L$-липшицевым градиентом. Тогда для любых $x, y \in \mathbb{R}^n$ выполняется следующее неравенство:
$$
\begin{aligned}
f(x) + \langle \nabla f(x), y - x \rangle + \frac{1}{2L} & \|\nabla f(x) - \nabla f(y)\|^2_2 \leq f(y) \text{ или, эквивалентно, }\\
\|\nabla f(y)-\nabla f (x)\|_2^2 = & \|\nabla f(x)-\nabla f (y)\|_2^2 \leq 2L\left(f(x)-f(y)-\langle\nabla f (y),x -y\rangle \right)
\end{aligned}
$$
:::

1. Для доказательства рассмотрим вспомогательную функцию $\varphi(y) = f(y) - \langle \nabla f(x), y\rangle$. Она является выпуклой (как сумма выпуклых функций). Легко проверить, что она является $L$-гладкой по определению, поскольку $\nabla \varphi(y) = \nabla f(y) - \nabla f(x)$ и $\|\nabla \varphi(y_1) - \nabla \varphi(y_2)\| = \|\nabla f(y_1) - \nabla f(y_2)\| \leq L\|y_1 - y_2\|$.
2. Теперь рассмотрим свойство липшицевой параболы для гладкой функции $\varphi(y)$:
  $$
  \begin{aligned}
  \uncover<+->{ \varphi(y) & \leq  \varphi(x) + \langle \nabla \varphi(x), y-x \rangle + \frac{L}{2}\|y-x\|_2^2 \\ }
  \uncover<+->{ \stackrel{x := y, y := y - \frac1L \nabla\varphi(y)}{ }\;\;\varphi\left(y - \frac1L \nabla\varphi(y)\right) &  \leq \varphi(y) + \left\langle \nabla \varphi(y), - \frac1L \nabla\varphi(y)\right\rangle + \frac{1}{2L}\|\nabla\varphi(y)\|_2^2 \\ }
  \uncover<+->{ \varphi\left(y - \frac1L \nabla\varphi(y)\right) &  \leq \varphi(y) - \frac{1}{2L}\|\nabla\varphi(y)\|_2^2 }
  \end{aligned}
  $$

## Инструменты для доказательства сходимости \faGem \ \faGem \ \faGem[regular] \faGem[regular]

3. Из условий оптимальности первого порядка для выпуклой функции $\nabla \varphi (y) =\nabla f(y) - \nabla f(x) = 0$. Можно заключить, что для любого $x$ минимум функции $\varphi(y)$ достигается в точке $y=x$. Следовательно:
  $$
  \varphi(x) \leq \varphi\left(y - \frac1L \nabla\varphi(y)\right) \leq \varphi(y) - \frac{1}{2L}\|\nabla\varphi(y)\|_2^2
  $$
4. Теперь подставим $\varphi(y) = f(y) - \langle \nabla f(x), y\rangle$:
  $$
  \begin{aligned}
  \uncover<+->{ & f(x) - \langle \nabla f(x), x\rangle \leq f(y) - \langle \nabla f(x), y\rangle - \frac{1}{2L}\|\nabla f(y) - \nabla f(x)\|_2^2 \\ }
  \uncover<+->{ & f(x) + \langle \nabla f(x), y - x \rangle + \frac{1}{2L} \|\nabla f(x) - \nabla f(y)\|^2_2 \leq f(y) \\ }
  \uncover<+->{ & \|\nabla f(y) - \nabla f(x)\|^2_2 \leq 2L \left( f(y) - f(x) - \langle \nabla f(x), y - x \rangle \right) \\ }
  \uncover<+->{ {\scriptsize \text{меняем x и y}} \quad & \|\nabla f(x)-\nabla f (y)\|_2^2 \leq 2L\left(f(x)-f(y)-\langle\nabla f (y),x -y\rangle \right)}
  \end{aligned}
  $$

. . .

Лемма доказана. На первый взгляд она не имеет очевидной геометрической интерпретации, но мы будем использовать её как удобный инструмент для оценки разности градиентов.

## Инструменты для доказательства сходимости \faGem \ \faGem \ \faGem \ \faGem[regular]

:::{.callout-theorem}
Пусть $f: \mathbb{R}^n \rightarrow \mathbb{R}$ — непрерывно дифференцируемая функция на $\mathbb{R}^n$. Тогда функция $f$ является $\mu$-сильно выпуклой тогда и только тогда, когда для любых $x, y \in \mathbb{R}^d$ выполняется:
$$
\begin{aligned}
\text{Сильно выпуклый случай } \mu >0 & &\langle \nabla f(x) - \nabla f(y), x - y \rangle &\geq \mu \|x - y\|^2 \\
\text{Выпуклый случай } \mu = 0 & &\langle \nabla f(x) - \nabla f(y), x - y \rangle &\geq 0
\end{aligned}
$$
:::

**Доказательство**

1. Приведём доказательство только для сильно выпуклого случая; выпуклый следует из него при $\mu=0$. Начнём с необходимости. Для сильно выпуклой функции
  $$
  \begin{aligned}
  & f(y) \geq f(x) + \langle \nabla f(x), y-x\rangle + \frac{\mu}{2}\|x-y\|_2^2 \\
  & f(x) \geq f(y) + \langle \nabla f(y), x-y\rangle + \frac{\mu}{2}\|x-y\|_2^2 \\
  {\scriptsize \text{сумма}} \;\; & \langle \nabla f(x) - \nabla f(y), x - y \rangle \geq \mu \|x - y\|^2
  \end{aligned}
  $$

## Инструменты для доказательства сходимости \faGem \ \faGem \ \faGem \ \faGem

2. Для достаточности предположим, что $\langle \nabla f(x) - \nabla f(y), x - y \rangle \geq \mu \|x - y\|^2$. Используя формулу Ньютона-Лейбница $f(x) = f(y) + \int_{0}^{1} \langle \nabla f(y + t(x - y)), x - y \rangle dt$:
  $$
  \begin{aligned}
  \uncover<+->{ f(x) - f(y) - \langle \nabla f(y), x - y \rangle &= \int_{0}^{1} \langle \nabla f(y + t(x - y)), x - y \rangle dt - \langle \nabla f(y), x - y \rangle \\ }
  \uncover<+->{ \stackrel{ \langle \nabla f(y), x - y \rangle = \int_{0}^{1}\langle \nabla f(y), x - y \rangle dt}{ }\qquad &= \int_{0}^{1} \langle \nabla f(y + t(x - y)) - \nabla f(y), (x - y) \rangle dt \\ }
  \uncover<+->{ \stackrel{ y + t(x - y) - y = t(x - y)}{ }\qquad&= \int_{0}^{1} t^{-1} \langle \nabla f(y + t(x - y)) - \nabla f(y), t(x - y) \rangle dt \\ }
  \uncover<+->{ & \geq \int_{0}^{1} t^{-1} \mu \| t(x - y) \|^2 dt } \uncover<+->{ = \mu \| x - y \|^2 \int_{0}^{1} t dt} \uncover<+->{= \frac{\mu}{2} \| x - y \|^2_2 }
  \end{aligned}
  $$

  . . .

  Таким образом, критерий сильной выпуклости выполнен
  $$
  \begin{aligned}
  \uncover<+->{ & f(x) \geq f(y) + \langle \nabla f(y), x - y \rangle + \frac{\mu}{2} \| x - y \|^2_2} \uncover<+->{ \text{ или, эквивалентно: }\\ }
  \uncover<+->{ {\scriptsize \text{меняем x и y}} \quad & - \langle \nabla f(x), x - y \rangle \leq - \left(f(x) - f(y) + \frac{\mu}{2} \| x - y \|^2_2 \right) }
  \end{aligned}
  $$

## Скорость сходимости для гладкого выпуклого случая \faGem \ \faGem[regular] \faGem[regular] \faGem[regular] \faGem[regular]

:::{.callout-theorem}
Пусть $f: \mathbb{R}^n \to \mathbb{R}$ — выпуклая и дифференцируемая функция. Пусть $S \subseteq  \mathbb{R}^n$ — замкнутое выпуклое множество, и существует минимизатор $x^*$ функции $f$ на $S$; кроме того, предположим, что $f$ является гладкой на $S$ с параметром $L$. Алгоритм метода проекции градиента с шагом $\frac1L$ обеспечивает следующую сходимость после $k > 0$ итераций:
$$
f(x_k) - f^* \leq \frac{L\|x_0 - x^*\|_2^2}{2k}
$$
:::

. . .

1. Докажем лемму о достаточном убывании, полагая $y_{k} = x_k - \frac1L\nabla f(x_k)$ и используя теорему косинусов $2x^Ty = \|x\|^2 + \|y\|^2 - \|x-y\|^2$:
    $$
    \begin{aligned}
    \uncover<+->{ &\text{Гладкость:} &f(x_{k+1})& \leq f(x_{k}) + \langle \nabla f(x_{k}), x_{k+1}-x_{k} \rangle +\frac{L}{2} \| x_{k+1}-x_{k}\|^2\\ }
    \uncover<+->{ &\text{Метод:} & &= f(x_{k})-L\langle y_{k} - x_k , x_{k+1}-x_{k} \rangle +\frac{L}{2} \| x_{k+1}-x_{k}\|^2\\ }
    \uncover<+->{ &\text{Теорема косинусов:} & &= f(x_{k})-\frac{L}{2}\left( \|y_{k} - x_k\|^2 + \|x_{k+1}-x_{k}\|^2 - \|y_{k} - x_{k+1}\|^2\right) +\frac{L}{2} \| x_{k+1}-x_{k}\|^2\\ }
    \uncover<+->{ & & &= f(x_{k})-\frac{1}{2L}\|\nabla f(x_k)\|^2 + \frac{L}{2} \|y_{k} - x_{k+1}\|^2 \\ }
    \end{aligned}
    $$ {#eq-suff_dec}

## Скорость сходимости для гладкого выпуклого случая \faGem \ \faGem \ \faGem[regular] \faGem[regular] \faGem[regular]

2. Пока что мы не получаем немедленного прогресса на каждом шаге. Снова используем теорему косинусов:
    $$
    \begin{aligned}
    \left\langle\frac1L \nabla f(x_k), x_k - x^* \right\rangle &=  \frac12\left(\frac{1}{L^2}\|\nabla f(x_k)\|^2 + \|x_k - x^*\|^2 -  \|x_k - x^* - \frac1L \nabla f(x_k)\|^2 \right) \\
    \langle \nabla f(x_k), x_k - x^* \rangle &=  \frac{L}{2}\left(\frac{1}{L^2}\|\nabla f(x_k)\|^2 + \|x_k - x^*\|^2 -  \|y_k - x^*\|^2 \right) \\
    \end{aligned}
    $$
3. Теперь используем свойство проекции: $\|x - \text{proj}_S(y)\|^2 + \|y - \text{proj}_S(y)\|^2 \leq \|x-y\|^2$ с $x = x^*, y = y_k$:
    $$
    \begin{aligned}
    \|x^* - \text{proj}_S(y_k)\|^2 + \|y_k - \text{proj}_S(y_k)\|^2 \leq \|x^*-y_k\|^2 \\
    \|y_k - x^*\|^2 \geq \|x^* - x_{k+1}\|^2 + \|y_k - x_{k+1}\|^2
    \end{aligned}
    $$
4. Теперь, используя выпуклость и предыдущую часть:
    $$
    \begin{aligned}
    \uncover<+->{ &\text{Выпуклость:} &f(x_k) - f^* &\leq  \langle \nabla f(x_k), x_k - x^* \rangle \\}
    \uncover<+->{ & & &\leq  \frac{L}{2}\left(\frac{1}{L^2}\|\nabla f(x_k)\|^2 + \|x_k - x^*\|^2 -  \|x_{k+1} - x^*\|^2 - \|y_k - x_{k+1}\|^2 \right) \\}
    \uncover<+->{&\text{Сумма для } i=0,k-1 &\sum\limits_{i=0}^{k-1} \left[f(x_i) - f^*\right]&\leq\sum\limits_{i=0}^{k-1} \frac{1}{2L}\|\nabla f(x_i)\|^2 + \frac{L}{2}\|x_0 - x^*\|^2  - \frac{L}{2} \sum\limits_{i=0}^{i-1} \|y_i - x_{i+1}\|^2}
    \end{aligned}
    $$

## Скорость сходимости для гладкого выпуклого случая \faGem \ \faGem \ \faGem \ \faGem[regular] \faGem[regular]

5. Оценим градиенты с помощью [неравенства достаточного убывания @eq-suff_dec]:
    $$
    \begin{aligned}
    \uncover<+->{\sum\limits_{i=0}^{k-1} \left[f(x_i) - f^*\right]&\leq \sum\limits_{i=0}^{k-1}\left[ f(x_{i}) - f(x_{i+1}) + \frac{L}{2} \|y_{i} - x_{i+1}\|^2 \right] + \frac{L}{2}\|x_0 - x^*\|^2  - \frac{L}{2} \sum\limits_{i=0}^{i-1} \|y_i - x_{i+1}\|^2  \\}
    \uncover<+->{&\leq f(x_0) - f(x_k) + \frac{L}{2} \sum\limits_{i=0}^{i-1} \|y_i - x_{i+1}\|^2 + \frac{L}{2}\|x_0 - x^*\|^2  - \frac{L}{2} \sum\limits_{i=0}^{i-1} \|y_i - x_{i+1}\|^2 \\}
    \uncover<+->{&\leq f(x_0) - f(x_k) + \frac{L}{2}\|x_0 - x^*\|^2 \\}
    \uncover<+->{\sum\limits_{i=0}^{k-1} f(x_i) - k f^* &\leq f(x_0) - f(x_k) + \frac{L}{2}\|x_0 - x^*\|^2\\}
    \uncover<+->{\sum\limits_{i=1}^{k} \left[ f(x_i) - f^*\right] &\leq \frac{L}{2}\|x_0 - x^*\|^2\\}
    \end{aligned}
    $$

## Скорость сходимости для гладкого выпуклого случая \faGem \ \faGem \ \faGem \ \faGem \ \faGem[regular]

6. Из неравенства достаточного убывания
    $$
    f(x_{k+1}) \le f(x_k) - \frac{1}{2L}\|\nabla f(x_k)\|^2 + \frac{L}{2}\|y_k - x_{k+1}\|^2,
    $$

    . . .

    используем тот факт, что $x_{k+1} = \mathrm{proj}_S(y_k)$. По определению проекции,
    $$
    \|y_k - x_{k+1}\| \le \|y_k - x_k\|,
    $$

    . . .

    и вспомним, что $y_k = x_k - \tfrac{1}{L}\nabla f(x_k)$ означает
    $\|y_k - x_k\| = \tfrac{1}{L}\|\nabla f(x_k)\|$.
    Следовательно,
    $$
    \frac{L}{2}\,\|y_k - x_{k+1}\|^2 \le \frac{L}{2}\,\|y_k - x_k\|^2 = \frac{L}{2}\,\frac{1}{L^2}\,\|\nabla f(x_k)\|^2 = \frac{1}{2L}\,\|\nabla f(x_k)\|^2.
    $$

    . . .

    Подставим обратно в $(*)$:
    $$
    f(x_{k+1}) \le f(x_k) - \frac{1}{2L}\|\nabla f(x_k)\|^2 + \frac{1}{2L}\|\nabla f(x_k)\|^2 = f(x_k).
    $$

    . . .

    Таким образом,
    $$
    f(x_{k+1}) \le f(x_k)\quad\text{для каждого }k,
    $$
    то есть $\{f(x_k)\}$ — монотонно невозрастающая последовательность.


## Скорость сходимости для гладкого выпуклого случая \faGem \ \faGem \ \faGem \ \faGem \ \faGem

7. Итоговая оценка сходимости
    Из шага 5 мы уже установили
    $$
    \sum_{i=0}^{k-1}\bigl[f(x_i) - f^*\bigr] \le \frac{L}{2}\|x_0 - x^*\|_2^2.
    $$

    . . .

    Поскольку $f(x_i)$ убывает по $i$, в частности $f(x_k) \le f(x_i)$ для всех $i \le k$. Следовательно,
    $$
    k\,\bigl[f(x_k) - f^*\bigr] \le \sum_{i=0}^{k-1}\bigl[f(x_i) - f^*\bigr] \le \frac{L}{2}\|x_0 - x^*\|_2^2,
    $$

    . . .

    откуда немедленно получаем
    $$
    f(x_k) - f^* \le \frac{L\|x_0 - x^*\|_2^2}{2k}.
    $$
    Это завершает доказательство скорости сходимости $\mathcal{O}(\tfrac1k)$ для выпуклой $L$-гладкой функции $f$ в условной задаче оптимизации.

## Скорость сходимости для гладкого сильно выпуклого случая \faGem \ \faGem[regular] \faGem[regular]

:::{.callout-theorem}

Пусть $f: \mathbb{R}^n \to \mathbb{R}$ — $\mu$-сильно выпуклая функция. Пусть $S \subseteq  \mathbb{R}^n$ — замкнутое выпуклое множество, и существует минимизатор $x^*$ функции $f$ на $S$; кроме того, предположим, что $f$ является гладкой на $S$ с параметром $L$. Алгоритм метода проекции градиента с шагом $\alpha \leq \frac1L$ обеспечивает следующую сходимость после $k > 0$ итераций:

$$
\|x_{k} - x^*\|_2^2 \leq \left(1 - \alpha \mu\right)^k \|x_{0} - x^*\|_2^2
$$
:::

**Доказательство**

1. Сначала докажем свойство стационарной точки: $\text{proj}_S(x^* - \alpha \nabla f(x^*)) = x^*$.

   Это следует из критерия проекции и условия оптимальности первого порядка для $x^*$.
   Пусть $y = x^* - \alpha \nabla f(x^*)$. Нужно показать, что $\langle y - x^*, x - x^* \rangle \le 0$ для всех $x \in S$.
   $$
   \langle (x^* - \alpha \nabla f(x^*)) - x^*, x - x^* \rangle = -\alpha \langle \nabla f(x^*), x - x^* \rangle \le 0
   $$
   Неравенство выполняется, так как $\alpha > 0$ и $\langle \nabla f(x^*), x-x^* \rangle \geq 0$ — условие оптимальности для $x^*$.



## Скорость сходимости для гладкого сильно выпуклого случая \faGem \ \faGem \ \faGem[regular]

1. Рассмотрим расстояние до решения, используя свойство стационарной точки:
  $$
  \begin{aligned}
  \uncover<+->{ \|x_{k+1} - x^*\|^2_2 &= \|\text{proj}_S (x_k - \alpha \nabla f (x_k)) - x^*\|^2_2 \\ }
  \uncover<+->{ {\scriptsize \text{свойство стационарной точки}}  & = \|\text{proj}_S (x_k - \alpha \nabla f (x_k)) - \text{proj}_S (x^* - \alpha \nabla f (x^*)) \|^2_2 \\ }
  \uncover<+->{ {\scriptsize \text{нерастягиваемость}}   & \leq \|x_k - \alpha \nabla f (x_k) - (x^* - \alpha \nabla f (x^*)) \|^2_2 \\ }
  \uncover<+->{ & =  \|x_k - x^*\|^2 - 2\alpha \langle \nabla f(x_k) - \nabla f(x^*), x_k - x^* \rangle + \alpha^2 \|\nabla f(x_k) - \nabla f(x^*)\|^2_2 }
  \end{aligned}
  $$

2. Теперь используем гладкость из инструментов сходимости и сильную выпуклость:
  $$
  \begin{aligned}
  \uncover<+->{ \text{гладкость} \;\; &\|\nabla f(x_k)-\nabla f (x^*)\|_2^2 \leq 2L\left(f(x_k)-f(x^*)-\langle\nabla f (x^*),x_k -x^*\rangle \right) \\ }
  \uncover<+->{ \text{сильная выпуклость} \;\; & - \langle \nabla f(x_k) -  \nabla f(x^*), x_k - x^* \rangle \leq - \left(f(x_k) - f(x^*) + \frac{\mu}{2} \| x_k - x^* \|^2_2 \right) - \langle \nabla f(x^*), x_k - x^* \rangle }
  \end{aligned}
  $$

## Скорость сходимости для гладкого сильно выпуклого случая \faGem \ \faGem \ \faGem

3. Подставим:
  $$
  \begin{aligned}
  \uncover<+->{ \|x_{k+1} - x^*\|^2_2 &\leq \|x_k - x^*\|^2 - 2\alpha \left(f(x_k) - f(x^*) + \frac{\mu}{2} \| x_k - x^* \|^2_2 \right) - 2\alpha \langle \nabla f(x^*), x_k - x^* \rangle + \\
  & + \alpha^2 2L\left(f(x_k)-f(x^*)-\langle\nabla f (x^*),x_k -x^*\rangle \right)  \\ }
  \uncover<+->{ &\leq (1 - \alpha \mu)\|x_k - x^*\|^2 + 2\alpha (\alpha L - 1) \left( f(x_k) - f(x^*) - \langle \nabla f(x^*), x_k - x^* \rangle \right)}
  \end{aligned}
  $$

4. В силу выпуклости $f$: $f(x_k) - f(x^*) - \langle \nabla f(x^*), x_k - x^* \rangle \geq 0$. Следовательно, при $\alpha \leq \frac1L$:
  $$
  \|x_{k+1} - x^*\|^2_2 \leq (1 - \alpha \mu)\|x_k - x^*\|^2,
  $$
  что означает линейную сходимость метода со скоростью $1 - \frac{\mu}{L}$.



# Метод Франк-Вульфа

---

::::{.columns}

:::{.column width="50%"}
![Маргарит Страус Франк (1927-2024)](frank.jpg){height=88%}
:::

:::{.column width="50%"}
![Филип Вульф (1927-2016)](wolfe.jpg){height=88%}
:::

::::

## Идея

![Иллюстрация алгоритма Франк-Вульфа (условного градиента)](FW1.pdf)

## Идея {.noframenumbering}

![Иллюстрация алгоритма Франк-Вульфа (условного градиента)](FW2.pdf)

## Идея {.noframenumbering}

![Иллюстрация алгоритма Франк-Вульфа (условного градиента)](FW3.pdf)

## Идея {.noframenumbering}

![Иллюстрация алгоритма Франк-Вульфа (условного градиента)](FW4.pdf)

## Идея {.noframenumbering}

![Иллюстрация алгоритма Франк-Вульфа (условного градиента)](FW5.pdf)

## Идея {.noframenumbering}

![Иллюстрация алгоритма Франк-Вульфа (условного градиента)](FW6.pdf)

## Идея {.noframenumbering}

![Иллюстрация алгоритма Франк-Вульфа (условного градиента)](FW7.pdf)

## Идея

$$
\begin{split}
y_k &= \text{arg}\min_{x \in S} f^I_{x_k}(x) = \text{arg}\min_{x \in S} \langle\nabla f(x_k), x \rangle \\
x_{k+1} &= \gamma_k x_k + (1-\gamma_k)y_k
\end{split}
$$

![Иллюстрация алгоритма Франк-Вульфа (условного градиента)](FW.pdf)

## Скорость сходимости для гладкого выпуклого случая \faGem \ \faGem[regular] \faGem[regular]

:::{.callout-theorem}
Пусть $f: \mathbb{R}^n \to \mathbb{R}$ — выпуклая и дифференцируемая функция. Пусть $S \subseteq \mathbb{R}^n$ — замкнутое выпуклое множество, и существует минимизатор $x^*$ функции $f$ на $S$; кроме того, предположим, что $f$ является гладкой на $S$ с параметром $L$. Алгоритм Франк-Вульфа с размером шага $\gamma_k = \frac{k-1}{k+1}$ обеспечивает следующую сходимость после $k > 0$ итераций:
$$
f(x_k) - f^* \leq \frac{2LR^2}{k+1}
$$
где $R = \max\limits_{x, y \in S} \|x - y\|$ — диаметр множества $S$.
:::

. . .

1. Из $L$-гладкости $f$ имеем:
    $$
    \begin{aligned}
    f\left(x_{k+1}\right) - f\left(x_k\right) &\leq \left\langle \nabla f\left(x_k\right), x_{k+1} - x_k \right\rangle + \frac{L}{2} \left\|x_{k+1} - x_k\right\|^2 \\
    &= (1 - \gamma_k) \left\langle \nabla f\left(x_k\right), y_k - x_k \right\rangle + \frac{L (1 - \gamma_k)^2}{2} \left\|y_k - x_k\right\|^2
    \end{aligned}
    $$

## Скорость сходимости для гладкого выпуклого случая \faGem \ \faGem \ \faGem[regular]

2. Из выпуклости $f$ для любого $x \in S$, включая $x^*$:
    $$
    \langle \nabla f(x_k), x - x_k \rangle \leq f(x) - f(x_k)
    $$
    В частности, для $x = x^*$:
    $$
    \langle \nabla f(x_k), x^* - x_k \rangle \leq f(x^*) - f(x_k)
    $$

3. По определению $y_k$ имеем $\langle \nabla f(x_k), y_k \rangle \leq \langle \nabla f(x_k), x^* \rangle$, поэтому:
    $$
    \langle \nabla f(x_k), y_k - x_k \rangle \leq \langle \nabla f(x_k), x^* - x_k \rangle \leq f(x^*) - f(x_k)
    $$

4. Объединяя приведённые неравенства:
    $$
    \begin{aligned}
    f\left(x_{k+1}\right) - f\left(x_k\right) &\leq (1 - \gamma_k) \left\langle \nabla f\left(x_k\right), y_k - x_k \right\rangle + \frac{L (1 - \gamma_k)^2}{2} \left\|y_k - x_k\right\|^2 \\
    &\leq (1 - \gamma_k) \left( f(x^*) - f(x_k) \right) + \frac{L (1 - \gamma_k)^2}{2} R^2
    \end{aligned}
    $$

5. Перегруппируем слагаемые:
    $$
    \begin{aligned}
    f\left(x_{k+1}\right) - f(x^*) &\leq \gamma_k \left( f(x_k) - f(x^*) \right) + (1 - \gamma_k)^2 \frac{L R^2}{2}
    \end{aligned}
    $$

## Скорость сходимости для гладкого выпуклого случая \faGem \ \faGem \ \faGem

6. Обозначив $\delta_k = \frac{f\left(x_k\right) - f\left(x^*\right)}{L R^2}$, получаем:
    $$
    \delta_{k+1} \leq \gamma_k \delta_k + \frac{(1 - \gamma_k)^2}{2} = \frac{k - 1}{k + 1} \delta_k + \frac{2}{(k + 1)^2}
    $$

7. Докажем по индукции, что $\delta_k \leq \frac{2}{k+1}$.

    * База: $\delta_2 \leq \frac{1}{2} < \frac23$
    * Предположим $\delta_k \leq \frac{2}{k+1}$
    * Тогда $\delta_{k+1} \leq \frac{k-1}{k+1} \cdot \frac{2}{k+1} + \frac{2}{(k+1)^2} = \frac{2k}{k^2 + 2k + 1} < \frac{2}{k+2}$ \faGraduationCap

    что даёт нам искомый результат:
    $$
    f(x_k) - f^* \leq \frac{2LR^2}{k+1}
    $$

## Нижняя оценка для метода Франк-Вульфа ^[[\faFilePdf The Complexity of Large-scale Convex Programming under a Linear Optimization Oracle](https://arxiv.org/abs/1309.5550)] \faGem  \faGem[regular]

:::{.callout-theorem}
Рассмотрим произвольный алгоритм, который обращается к допустимому множеству $S \subseteq \mathbb{R}^n$ только через оракул линейной минимизации (LMO). Пусть диаметр множества $S$ равен $R$. Существует $L$-гладкая сильно выпуклая функция $f : \mathbb{R}^n \to \mathbb{R}$, для которой этому алгоритму потребуется не менее
$$
\min \left( \frac{n}{2}, \frac{LR^2}{16 \varepsilon} \right)
$$
итераций (то есть вызовов LMO), чтобы построить точку $\hat{x} \in S$ с $f(\hat{x}) - \min\limits_{x \in S} f(x) \leq \varepsilon$. Нижняя оценка справедлива как для выпуклых, так и для сильно выпуклых функций.
:::

. . .

::::{.columns}

:::{.column width="50%"}

**Набросок доказательства.** Рассмотрим следующую задачу оптимизации:
$$
\begin{aligned}
\min_{x \in S} f(x) &= \min_{x \in S} \frac{1}{2} \|x\|_2^2 \\
S &= \left\{ x \in \mathbb{R}^n \mid x \geq 0,\ \sum_{i=1}^n x_i = 1 \right\}
\end{aligned}
$$
:::
:::{.column width="50%" .nonincremental}
Заметим, что:

- $f$ является $1$-гладкой;
- диаметр $S$ равен $R = 2$;
- $f$ сильно выпукла.
:::
::::

## Нижняя оценка для метода Франк-Вульфа ^[[\faFilePdf The Complexity of Large-scale Convex Programming under a Linear Optimization Oracle](https://arxiv.org/abs/1309.5550)] \faGem \ \faGem

1. Оптимальное решение:
    $$
    x^* = \frac{1}{n} \mathbf{1} = \frac{1}{n} \sum_{i=1}^n e_i, \quad \text{и} \quad f(x^*) = \frac{1}{2n},
    $$
где $e_i = (0, \dots, 0, \underset{\text{позиция } i}{1}, 0, \dots, 0)^\top$ — $i$-й стандартный базисный вектор.

1. Оракул линейной минимизации (LMO) на $S$ возвращает вершину $e_i$. После $k$ итераций метод обнаружит не более $k$ различных базисных векторов $e_{i_1}, \dots, e_{i_k}$. Наилучшая выпуклая комбинация, которую можно составить:
    $$
    \hat{x} = \frac{1}{k} \sum_{j=1}^k e_{i_j}.
    $$

1. Вычислив значение функции в точке $\hat{x}$, получаем:
    $$
    f(\hat{x}) - f(x^*) \geq \frac{1}{2} \left( \frac{1}{\min\{k, n\}} - \frac{1}{n} \right).
    $$

1. Чтобы обеспечить $f(\hat{x}) - f(x^*) \leq \varepsilon$, необходимо (полное доказательство приведено в статье):
    $$
    k \geq \min\left\{ \frac{n}{2}, \frac{1}{4\varepsilon} \right\} = \min\left\{ \frac{n}{2}, \frac{L R^2}{16\varepsilon} \right\}.
    $$

## Итоги метода Франк-Вульфа

* Метод не требует проекций, а в некоторых случаях позволяет вычислять итерации в замкнутой форме
* Глобальная скорость сходимости составляет $O\left(\frac{1}{k}\right)$ для гладких выпуклых функций. Сильная выпуклость не улучшает скорость. Это нижняя оценка для LMO
* По сравнению с методом проекции градиента скорость сходимости хуже, однако стоимость итерации может быть ниже, а решения — более разреженными
* Недавно было показано, что для сильно выпуклых множеств скорость может быть улучшена до $O\left(\frac{1}{k^2}\right)$ ([\faFilePdf \ статья](https://arxiv.org/abs/1406.1305))
* Если допустить шаги удаления, сходимость становится линейной ([\faFilePdf \ статья](https://arxiv.org/abs/1511.05932)) в сильно выпуклом случае
* В недавних работах показано обобщение на негладкий случай ([\faFilePdf \ статья](https://arxiv.org/abs/2010.01848)) со скоростью сходимости $O\left(\frac{1}{\sqrt{k}}\right)$


# Численные эксперименты

## Двумерный пример. Метод Франк-Вульфа

\includegraphics<1>[width=0.81\textwidth,center]{fw_2d_0.pdf}
\includegraphics<2>[width=0.81\textwidth,center]{fw_2d_1.pdf}
\includegraphics<3>[width=0.81\textwidth,center]{fw_2d_2.pdf}
\includegraphics<4>[width=0.81\textwidth,center]{fw_2d_3.pdf}
\includegraphics<5>[width=0.81\textwidth,center]{fw_2d_4.pdf}
\includegraphics<6>[width=0.81\textwidth,center]{fw_2d_5.pdf}
\includegraphics<7>[width=0.81\textwidth,center]{fw_2d_6.pdf}
\includegraphics<8>[width=0.81\textwidth,center]{fw_2d_7.pdf}



## Квадратичная функция. Покоординатные ограничения

::::{.columns}

:::{.column width="40%"}

$$
\begin{aligned}
\min_{\substack{x \in \mathbb{R}^n \\ -\mathbf{1} \preceq x \preceq \mathbf{1}}} \frac{1}{2} x^\top A x - b^\top x,\\
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [\mu; L].
\end{aligned}
$$

Проекция тривиальна:
$$
\pi_S(x) = \text{clip}(x, -\mathbf{1}, \mathbf{1}).
$$
или
$$
\pi_S(x) = \max\left(-\mathbf{1}, \min(\mathbf{1}, x)\right).
$$

Оракул линейной минимизации (LMO) для заданного градиента $g$ имеет вид $y = \operatorname*{argmin}\limits_{z \in S} \langle g, z \rangle$.

Поскольку допустимое множество сепарабельно по координатам, решение вычисляется покоординатно:
$$
y_i = \begin{cases}
-1, & \text{если } g_i > 0, \\
1,  & \text{если } g_i \le 0.
\end{cases}
$$
:::
:::{.column width="60%"}

![](conditional_quadratic_80_0_10.pdf)

:::
::::

## Квадратичная функция. Покоординатные ограничения

::::{.columns}

:::{.column width="40%"}

$$
\begin{aligned}
\min_{\substack{x \in \mathbb{R}^n \\ -\mathbf{1} \preceq x \preceq \mathbf{1}}} \frac{1}{2} x^\top A x - b^\top x,\\
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [\mu; L].
\end{aligned}
$$

Проекция тривиальна:
$$
\pi_S(x) = \text{clip}(x, -\mathbf{1}, \mathbf{1}).
$$
или
$$
\pi_S(x) = \max\left(-\mathbf{1}, \min(\mathbf{1}, x)\right).
$$

Оракул линейной минимизации (LMO) для заданного градиента $g$ имеет вид $y = \operatorname*{argmin}\limits_{z \in S} \langle g, z \rangle$.

Поскольку допустимое множество сепарабельно по координатам, решение вычисляется покоординатно:
$$
y_i = \begin{cases}
-1, & \text{если } g_i > 0, \\
1,  & \text{если } g_i \le 0.
\end{cases}
$$
:::
:::{.column width="60%"}

![](conditional_quadratic_80_1_10.pdf)

:::
::::

## Квадратичная функция. Симплексные ограничения (удачная задача с диагональной матрицей)

::::{.columns}
:::{.column width="50%"}

$$
\begin{aligned}
\min_{\substack{x \in \mathbb{R}^n \\ x \ge 0, \mathbf{1}^T x = 1}} \frac{1}{2} x^T A x, \\
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [0; 100].
\end{aligned}
$$

| Метод | Время шага, мс | Время LMO/проекции, мс |
|:------:|:---------------:|:---------------:|
| PGD    | 0.0069 | 0.0167 |
| FW     | 0.0070 | 0.0066 |

Проекция на единичный симплекс $\pi_S(x)$ выполняется за $\mathcal{O}(n \log n)$ или за ожидаемое $\mathcal{O}(n)$ время. ^[[\faFilePdf \ Efficient Projections onto the $\ell_1$-Ball for Learning in High Dimensions](https://stanford.edu/~jduchi/projects/DuchiShSiCh08.pdf)]

LMO для заданного градиента $g$ имеет вид $y = \operatorname*{argmin}\limits_{z \in S} \langle g, z \rangle$. Решение соответствует вершине симплекса:
$$
y = e_j \quad \text{где} \quad j = \operatorname*{argmin}_i g_i.
$$

:::
:::{.column width="50%"}
![](fw_simplex_200_mu_0_L_100.0_diag.pdf)

:::
::::

## Квадратичная функция. Симплексные ограничения

::::{.columns}
:::{.column width="50%"}

$$
\begin{aligned}
\min_{\substack{x \in \mathbb{R}^n \\ x \ge 0, \mathbf{1}^T x = 1}} \frac{1}{2} x^T A x, \\
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [0; 100].
\end{aligned}
$$

| Метод | Время шага, мс | Время LMO/проекции, мс |
|:------:|:---------------:|:---------------:|
| PGD    | 0.0069 | 0.0420 |
| FW     | 0.0069 | 0.0066 |

:::
:::{.column width="50%"}
![](fw_simplex_200_mu_0_L_100.0_rotated.pdf)

:::
::::

## Квадратичная функция. Симплексные ограничения

::::{.columns}
:::{.column width="50%"}

$$
\begin{aligned}
\min_{\substack{x \in \mathbb{R}^n \\ x \ge 0, \mathbf{1}^T x = 1}} \frac{1}{2} x^T A x, \\
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [0; 100].
\end{aligned}
$$

| Метод | Время шага, мс | Время LMO/проекции, мс |
|:------:|:---------------:|:---------------:|
| PGD    | 0.0068 | 0.0761 |
| FW     | 0.0069 | 0.0070 |

:::
:::{.column width="50%"}
![](fw_simplex_300_mu_0_L_100.0_rotated.pdf)

:::
::::

## Квадратичная функция. Симплексные ограничения

::::{.columns}
:::{.column width="50%"}

$$
\begin{aligned}
\min_{\substack{x \in \mathbb{R}^n \\ x \ge 0, \mathbf{1}^T x = 1}} \frac{1}{2} x^T A x, \\
A \in \mathbb{R}^{n \times n}, \quad \lambda\left(A\right) \in [1; 100].
\end{aligned}
$$

| Метод | Время шага, мс | Время LMO/проекции, мс |
|:------:|:---------------:|:---------------:|
| PGD    | 0.0068 | 0.0752 |
| FW     | 0.0067 | 0.0068 |

:::
:::{.column width="50%"}
![](fw_simplex_300_mu_1_L_100.0_rotated.pdf)

:::
::::

## PGD и метод Франк-Вульфа

Ключевое различие между PGD и FW состоит в том, что PGD требует проекции, тогда как FW — лишь оракул линейной минимизации (LMO).

В недавней [статье](https://arxiv.org/pdf/2211.14103) авторы представили следующую сравнительную таблицу сложностей линейной минимизации и проекций на некоторые выпуклые множества с точностью до аддитивной ошибки $\epsilon$ в евклидовой норме.

| **Множество**   | **Линейная минимизация**  | **Проекция**    |
|---------------------|-----------------|---------|
| $n$-мерный $\ell_p$-шар, $p \neq 1,2,\infty$ | $\mathcal{O}(n)$  | $\tilde{\mathcal{O}}\!\bigl(\tfrac{n}{\epsilon^2}\bigr)$|
| Шар ядерной нормы для $n\times m$ матриц | $\mathcal{O}\!\Bigl(\nu\,\ln(m + n)\,\tfrac{\sqrt{\sigma_1}}{\sqrt{\epsilon}}\Bigr)$    | $\mathcal{O}\!\bigl(m\,n\,\min\{m,n\}\bigr)$   |
| Потоковый многогранник на графе с $m$ вершинами и $n$ рёбрами (ограничение пропускной способности на рёбрах) | $\mathcal{O}\!\Bigl((n \log m)\bigl(n + m\,\log m\bigr)\Bigr)$ | $\tilde{\mathcal{O}}\!\bigl(\tfrac{n}{\epsilon^2}\bigr)\ \text{или}\ \mathcal{O}(n^4\,\log n)$    |
| Многогранник Биркгофа ($n \times n$ дважды стохастические матрицы)   | $\mathcal{O}(n^3)$| $\tilde{\mathcal{O}}\!\bigl(\tfrac{n^2}{\epsilon^2}\bigr)$   |

Когда $\epsilon$ отсутствует, аддитивной ошибки нет. Обозначение $\tilde{\mathcal{O}}$ скрывает полилогарифмические множители по размерности и полиномиальные множители в константах, связанных с расстоянием до оптимума. Для шара ядерной нормы (спектраэдра) $\nu$ обозначает число ненулевых элементов, а $\sigma_1$ — наибольшее сингулярное число проецируемой матрицы.

# Дополнительные численные эксперименты

## Траектории PGD и FW на $\ell_2$-шаре

::::{.columns}
:::{.column width="40%"}

$$
\min_{x \in \mathbb{R}^2} \frac{1}{2} x^\top A x - b^\top x, \quad \|x\|_2 \leq 1
$$

PGD: проекция $\to$ кратчайший путь к границе, затем движение вдоль неё к оптимуму.

FW: каждый шаг — выпуклая комбинация текущей точки и **антиградиентной точки на границе**. Характерный зигзаг вдоль множества.

Оба метода стартуют из $x_0 = (-0.5, -0.5)$.

:::
:::{.column width="60%"}

![](exp_2d_l2ball.pdf)

:::
::::

## Траектории PGD и FW на 2-симплексе

::::{.columns}
:::{.column width="40%"}

$$
\min_{\substack{x \in \mathbb{R}^3 \\ x \geq 0,\; \mathbf{1}^\top x = 1}} \frac{1}{2} x^\top A x
$$

FW на каждом шаге выбирает одну из **вершин** симплекса ($e_1, e_2, e_3$), движется к ней $\to$ траектория проходит через рёбра.

PGD: проекция на симплекс допускает движение в любом направлении $\to$ более прямой путь к оптимуму.

:::
:::{.column width="60%"}

![](exp_2d_simplex.pdf)

:::
::::

## $\ell_2$-шар. Проекция и LMO одинаково дёшевы

::::{.columns}
:::{.column width="40%"}

$$
\min_{\|x\|_2 \leq 2} \frac{1}{2} x^\top A x - b^\top x
$$

* $n = 200$, $\kappa = 50$
* Проекция: $\pi_S(x) = x \cdot \min\left(1, \frac{R}{\|x\|}\right)$ — $\mathcal{O}(n)$
* LMO: $y = -R \frac{g}{\|g\|}$ — $\mathcal{O}(n)$

Когда проекция и LMO стоят одинаково, PGD выигрывает за счёт **линейной скорости** при сильной выпуклости.

:::
:::{.column width="60%"}

![](exp_l2ball.pdf)

:::
::::

## Разреженность итераций FW на симплексе

::::{.columns}
:::{.column width="40%"}

$$
\min_{\substack{x \geq 0 \\ \mathbf{1}^\top x = 1}} \frac{1}{2} x^\top A x, \quad n = 500
$$

Ключевое свойство FW: итерация $x_k$ есть **выпуклая комбинация $k$ вершин** $\Rightarrow$ не более $k$ ненулевых компонент.

* PGD: после проекции число ненулевых компонент скачком достигает $\sim 400$
* FW: число ненулевых растёт **линейно** с итерациями

Это важно, когда нужно получить разреженное решение.

:::
:::{.column width="60%"}

![](exp_sparsity_simplex.pdf)

:::
::::

## Сильная выпуклость: PGD ускоряется, FW — нет

::::{.columns}
:::{.column width="40%"}

$$
\min_{\substack{x \geq 0 \\ \mathbf{1}^\top x = 1}} \frac{1}{2} x^\top A x
$$

* $n = 200$, $\mu = 5$, $L = 100$, $\kappa = 20$

PGD с шагом $\frac{1}{L}$: **линейная сходимость** $(1 - \mu/L)^k$, ведь проекция на выпуклое множество — нерастягивающий оператор.

FW: остаётся $\mathcal{O}(1/k)$ — сильная выпуклость **не помогает** со стандартным шагом $\gamma_k = \frac{2}{k+2}$.

:::
:::{.column width="60%"}

![](exp_sc_simplex.pdf)

:::
::::

## Скорость сходимости: выпуклая vs сильно выпуклая задача

::::{.columns}
:::{.column width="40%"}

$$
\min_{-\mathbf{1} \preceq x \preceq \mathbf{1}} \frac{1}{2} x^\top A x, \quad n = 100
$$

* Левая панель: $\mu \approx 0$ — оба метода $\mathcal{O}(1/k)$, но PGD быстрее по константе
* Правая панель: $\mu = 2$ — PGD получает **экспоненциальную** скорость, FW остаётся субли&shy;нейным

Главный вывод: сильная выпуклость кардинально меняет картину в пользу PGD.

:::
:::{.column width="60%"}

![](exp_rates_comparison.pdf)

:::
::::

## Стоимость итерации: проекция vs LMO на симплексе

::::{.columns}
:::{.column width="40%"}

Измеряем **только** стоимость проекции и LMO, без вычисления градиента.

* Проекция на симплекс: сортировка + O(n) — всего $\mathcal{O}(n \log n)$
* LMO (argmin): один проход — $\mathcal{O}(n)$

При $n = 10000$ проекция дороже LMO **на порядок**.

В задачах, где градиент считается быстро (разреженная матрица), разница в стоимости шага между PGD и FW определяется именно проекцией/LMO.

:::
:::{.column width="60%"}

![](exp_scaling_simplex.pdf)

:::
::::

## Ядерная норма: когда FW — единственный вариант

::::{.columns}
:::{.column width="40%"}

$$
\min_{\|X\|_* \leq R} \frac{1}{2}\|X - B\|_F^2, \quad X \in \mathbb{R}^{n \times n}
$$

* PGD: полное SVD $\mathcal{O}(n^3)$ **на каждой итерации**
* FW: rank-1 SVD (power iteration) $\mathcal{O}(n^2)$

По итерациям PGD выигрывает. Но стоимость одной итерации PGD растёт **кубически**: при $n = 800$ полное SVD в **75 раз** дороже rank-1 SVD.

Для матриц размера $10^4 \times 10^4$ и больше полное SVD вычислительно неподъёмно — FW становится единственным разумным методом.

:::
:::{.column width="60%"}

![](exp_nuclear_norm.pdf)

:::
::::

## Ядерная норма: определение и свойства

:::{.callout-note}
## Ядерная норма (trace norm)
Ядерная норма матрицы $X \in \mathbb{R}^{m \times n}$ определяется как сумма её сингулярных чисел:
$$
\|X\|_* = \sum_{i=1}^{\min(m,n)} \sigma_i(X) = \text{tr}\left(\sqrt{X^\top X}\right)
$$
:::

. . .

* Ядерная норма — выпуклая оболочка ранга матрицы на единичном спектральном шаре: $\|X\|_* = \text{conv}\left(\text{rank}(X)\right)$ при $\|X\|_{\text{op}} \leq 1$.
* Поэтому ограничение $\|X\|_* \leq R$ — стандартный выпуклый релаксант для ограничения на ранг.
* Шар ядерной нормы $\mathcal{B}_* = \{X \in \mathbb{R}^{m \times n} \mid \|X\|_* \leq R\}$ — выпуклое компактное множество.
* Крайние точки $\mathcal{B}_*$ — матрицы ранга 1 вида $R \cdot u v^\top$, где $\|u\|_2 = \|v\|_2 = 1$.

. . .

**Приложения:** matrix completion (рекомендательные системы), robust PCA, low-rank matrix recovery, сжатие нейронных сетей.

## Итерации PGD и FW на шаре ядерной нормы

**Задача:** $\min_{\|X\|_* \leq R} f(X)$, где $X \in \mathbb{R}^{m \times n}$.

. . .

:::: {.columns}

::: {.column width="50%"}

### Итерация PGD

$$
X_{k+1} = \text{proj}_{\mathcal{B}_*}\!\left(X_k - \alpha \nabla f(X_k)\right)
$$

1. Вычислить $Y = X_k - \alpha \nabla f(X_k)$
2. **Полное SVD:** $Y = U \Sigma V^\top$ — стоимость $\mathcal{O}(mn \cdot \min(m,n))$
3. Мягкое пороговое отсечение (soft thresholding) сингулярных чисел для проекции на $\mathcal{B}_*$:
$$
\text{proj}_{\mathcal{B}_*}(Y) = U \cdot \text{diag}\!\left(\max(\sigma_i - \lambda, 0)\right) \cdot V^\top
$$
где $\lambda \geq 0$ выбирается так, чтобы $\sum_i \max(\sigma_i - \lambda, 0) = R$.

:::

. . .

::: {.column width="50%"}

### Итерация FW

$$
\begin{aligned}
S_k &= \arg\min_{\|S\|_* \leq R} \langle \nabla f(X_k), S \rangle \\
X_{k+1} &= (1 - \gamma_k) X_k + \gamma_k S_k
\end{aligned}
$$

1. Вычислить $G = \nabla f(X_k)$
2. **Rank-1 SVD** (степенная итерация): найти ведущие сингулярные векторы $u_1, v_1$ матрицы $-G$ — стоимость $\mathcal{O}(\text{nnz}(G))$
3. $S_k = R \cdot u_1 v_1^\top$ — ранг-1 матрица
4. $X_{k+1} = (1-\gamma_k) X_k + \gamma_k S_k$

**Бонус:** $X_k$ имеет ранг $\leq k$ после $k$ итераций.

:::

::::

## FW побеждает: matrix completion + ядерная норма {.smaller}

::::{.columns}
:::{.column width="40%"}

**Задача:** matrix completion с ограничением $\|X\|_* \leq R$:

$$
\min_{\|X\|_* \leq R} \frac{1}{2} \sum_{(i,j) \in \Omega} (X_{ij} - B_{ij})^2
$$

**Ключ:** градиент $\nabla f$ **разрежен** (ненулевой только на $\Omega$).

* Степенная итерация для rank-1 SVD на разреженном $\nabla f$: $\mathcal{O}(|\Omega|)$
* Полное SVD плотной матрицы $Y$: $\mathcal{O}(n^3)$
* При $n = 2500$, $|\Omega| = 3\% \cdot n^2$: **отношение** $\approx 900\times$

FW компенсирует медленную сходимость $O(1/k)$ огромным числом дешёвых итераций и **побеждает PGD по времени**.

:::
:::{.column width="60%"}

![](exp_fw_wins_nuclear.pdf)

:::
::::

## Matrix completion: почему ядерная норма? {.smaller}

**Задача восстановления матрицы.** Дана матрица $B \in \mathbb{R}^{m \times n}$, наблюдаемая лишь на подмножестве индексов $\Omega \subset [m] \times [n]$. Нужно восстановить всю матрицу, предполагая, что она имеет малый ранг.

. . .

:::: {.columns}

::: {.column width="50%"}

### Некорректная (невыпуклая) постановка

$$
\min_{X} \frac{1}{2} \sum_{(i,j) \in \Omega} (X_{ij} - B_{ij})^2 \quad \text{s.t.} \quad \text{rank}(X) \leq r
$$

* Ограничение $\text{rank}(X) \leq r$ — **невыпуклое**
* NP-трудная задача в общем случае
* Нет гарантий глобальной оптимальности

:::

. . .

::: {.column width="50%"}

### Выпуклая релаксация

$$
\min_{X} \frac{1}{2} \sum_{(i,j) \in \Omega} (X_{ij} - B_{ij})^2 \quad \text{s.t.} \quad \|X\|_* \leq R
$$

* Ядерная норма — **выпуклая оболочка** ранга
* Задача выпуклая, можно применять PGD и FW
* Гарантии точного восстановления при определённых условиях ^[[\faFilePdf \ Candès, Recht. Exact Matrix Completion via Convex Optimization.](https://arxiv.org/abs/0805.4471)]

:::

::::

. . .

**Пример из практики:** Netflix Prize — предсказание оценок пользователей для фильмов. Матрица $\sim 500\text{K} \times 17\text{K}$, известно $\sim 1\%$ записей. Ядерная норма + FW позволяют эффективно решать эту задачу.

# Метод зеркального спуска

## Мотивация: зачем нужен зеркальный спуск?

Вспомним итерацию метода проекции градиента:
$$
x_{k+1} = \text{proj}_S\left(x_k - \alpha_k \nabla f(x_k)\right)
$$

. . .

Эта формула опирается на **евклидову норму** $\|\cdot\|_2$, как в шаге градиента, так и в проекции. Но что, если:

* Геометрия задачи **неевклидова**? Например, оптимизация на симплексе $\Delta_n = \{x \geq 0, \mathbf{1}^\top x = 1\}$ — естественная геометрия описывается не $\ell_2$, а $\ell_1$-нормой.
* Функция $f$ гладкая относительно **неевклидовой** нормы $\|\cdot\|$?
$$
f(y) \leq f(x) + \langle \nabla f(x), y - x \rangle + \frac{L}{2}\|y - x\|^2
$$

. . .

**Проблема:** PGD с евклидовой проекцией может давать оценку $\mathcal{O}\left(\frac{L_2 R_2^2}{k}\right)$, но $L_2$ и $R_2$ могут **зависеть от размерности** $n$, что делает оценку бессмысленной для задач большой размерности.

## Мотивация: пример на симплексе

Рассмотрим задачу на стандартном симплексе $\Delta_n = \{x \in \mathbb{R}^n \mid x \geq 0, \mathbf{1}^\top x = 1\}$:
$$
\min_{x \in \Delta_n} f(x)
$$

. . .

Пусть $f$ является $L_1$-гладкой относительно $\ell_1$-нормы: $f(y) \leq f(x) + \langle \nabla f(x), y-x\rangle + \frac{L_1}{2}\|y-x\|_1^2$.

. . .

:::: {.columns}

::: {.column width="50%"}

### PGD (евклидова геометрия)

* Гладкость: $L_2 \leq n \cdot L_1$ (из $\|x\|_2 \leq \|x\|_1$)
* Диаметр: $R_2 = \sqrt{2}$
* **Оценка:**
$$
f(x_k) - f^* \leq \frac{n \cdot L_1 \cdot 2}{2k} = \frac{n L_1}{k}
$$

:::

. . .

::: {.column width="50%"}

### Зеркальный спуск ($\ell_1$-геометрия)

* Гладкость: $L_1$ (без множителя!)
* «Диаметр»: $R_1 = \sqrt{2 \ln n}$  (в смысле дивергенции Брэгмана)
* **Оценка:**
$$
f(x_k) - f^* \leq \frac{L_1 \cdot 2\ln n}{2k} = \frac{L_1 \ln n}{k}
$$

:::

::::

. . .

**Выигрыш:** $\frac{n}{\ln n}$ — **экспоненциальный** по размерности!

## Мотивация: прямое и двойственное пространство {.smaller}

**Идея.** Градиент $\nabla f(x)$ живёт в **двойственном** пространстве $(\mathbb{R}^n)^*$ с нормой $\|\cdot\|_*$, а переменная $x$ — в **прямом** пространстве $\mathbb{R}^n$ с нормой $\|\cdot\|$.

. . .

:::: {.columns}

::: {.column width="50%"}

В PGD мы неявно отождествляем прямое и двойственное пространство (что корректно только для $\ell_2$):
$$
x_{k+1} = x_k - \alpha_k \nabla f(x_k)
$$
Вычитаем **градиент** (двойственный вектор) из **точки** (прямой вектор) напрямую.

:::

. . .

::: {.column width="50%"}

В зеркальном спуске мы **явно** работаем с двумя пространствами через **зеркальное отображение** $\nabla \omega$:
$$
\begin{aligned}
\nabla \omega(y_{k+1}) &= \nabla \omega(x_k) - \alpha_k \nabla f(x_k) \\
x_{k+1} &= \nabla \omega^*(y_{k+1})
\end{aligned}
$$
Шаг делается в двойственном пространстве, затем результат отображается обратно.

:::

::::

. . .

Функция $\omega$ называется **прокс-функцией** (distance-generating function). Она задаёт геометрию через дивергенцию Брэгмана.

## Дивергенция Брэгмана

:::{.callout-definition}
## Дивергенция Брэгмана
Пусть $\omega : \mathbb{R}^n \to \mathbb{R}$ — непрерывно дифференцируемая и строго выпуклая функция. **Дивергенция Брэгмана**, порождённая функцией $\omega$, определяется как:
$$
V_\omega(x, y) = \omega(x) - \omega(y) - \langle \nabla \omega(y), x - y \rangle
$$
:::

. . .

**Свойства:**

* $V_\omega(x, y) \geq 0$ для всех $x, y$ (из строгой выпуклости $\omega$).
* $V_\omega(x, y) = 0 \Leftrightarrow x = y$.
* **Не симметрична** в общем случае: $V_\omega(x, y) \neq V_\omega(y, x)$.
* **Не удовлетворяет** неравенству треугольника.
* Геометрически: $V_\omega(x, y)$ — разность между $\omega(x)$ и значением касательной к $\omega$ в точке $y$, вычисленным в точке $x$.

## Примеры дивергенций Брэгмана

:::: {.columns}

::: {.column width="50%"}

### Евклидова дивергенция

$\omega(x) = \frac{1}{2}\|x\|_2^2$

$$
V_\omega(x, y) = \frac{1}{2}\|x\|_2^2 - \frac{1}{2}\|y\|_2^2 - \langle y, x - y \rangle = \frac{1}{2}\|x - y\|_2^2
$$

Воспроизводит обычный PGD.

. . .

### Дивергенция Кульбака-Лейблера (KL)

$\omega(x) = \sum_{i=1}^n x_i \ln x_i$ (негативная энтропия) на $\Delta_n$

$$
V_\omega(x, y) = \sum_{i=1}^n x_i \ln \frac{x_i}{y_i}
$$

Естественна для задач на симплексе.

:::

. . .

::: {.column width="50%"}

### Дивергенция Итакура-Саито

$\omega(x) = -\sum_{i=1}^n \ln x_i$ на $\mathbb{R}^n_{++}$

$$
V_\omega(x, y) = \sum_{i=1}^n \left( \frac{x_i}{y_i} - \ln \frac{x_i}{y_i} - 1 \right)
$$

Используется в обработке сигналов.

. . .

### Дивергенция Махаланобиса

$\omega(x) = \frac{1}{2} x^\top Q x$, где $Q \succ 0$

$$
V_\omega(x, y) = \frac{1}{2}(x - y)^\top Q (x - y)
$$

Обобщённая евклидова геометрия.

:::

::::

## Сильная выпуклость относительно нормы

:::{.callout-definition}
Функция $\omega$ называется **$\sigma$-сильно выпуклой относительно нормы** $\|\cdot\|$, если для всех $x, y$ из области определения:
$$
V_\omega(x, y) \geq \frac{\sigma}{2}\|x - y\|^2
$$
:::

. . .

**Ключевые примеры:**

* $\omega(x) = \frac{1}{2}\|x\|_2^2$ является $1$-сильно выпуклой относительно $\|\cdot\|_2$.
* $\omega(x) = \sum_i x_i \ln x_i$ (негативная энтропия) является $1$-сильно выпуклой относительно $\|\cdot\|_1$ на симплексе $\Delta_n$.
    * Это утверждение известно как **неравенство Пинскера** и является нетривиальным результатом.

. . .

Именно $\sigma$-сильная выпуклость прокс-функции $\omega$ относительно нормы $\|\cdot\|$ позволяет дивергенции Брэгмана «измерять расстояния» в геометрии, задаваемой нормой $\|\cdot\|$.

## Алгоритм зеркального спуска

**Задача:** $\min_{x \in S} f(x)$, где $f$ — выпуклая функция, $S$ — замкнутое выпуклое множество.

. . .

:::{.callout-algorithm}
## Метод зеркального спуска (Mirror Descent)
**Вход:** $x_0 \in S$, шаги $\{\alpha_k\}$, прокс-функция $\omega$.

**Для** $k = 0, 1, 2, \ldots$:

1. Вычислить градиент $g_k = \nabla f(x_k)$
2. Шаг в двойственном пространстве:
$$
x_{k+1} = \arg\min_{x \in S} \left\{ \langle g_k, x \rangle + \frac{1}{\alpha_k} V_\omega(x, x_k) \right\}
$$
:::

. . .

**Интерпретация:** на каждом шаге минимизируем линейное приближение $f$ с регуляризацией дивергенцией Брэгмана вместо $\frac{1}{2\alpha}\|x - x_k\|_2^2$.

. . .

**Частный случай:** при $\omega(x) = \frac{1}{2}\|x\|_2^2$ получаем $V_\omega(x, y) = \frac{1}{2}\|x - y\|_2^2$, и итерация принимает вид:
$$
x_{k+1} = \arg\min_{x \in S} \left\{ \langle \nabla f(x_k), x \rangle + \frac{1}{2\alpha_k}\|x - x_k\|_2^2 \right\} = \text{proj}_S(x_k - \alpha_k \nabla f(x_k))
$$
то есть стандартный PGD.

## Зеркальный спуск: эквивалентные формы итерации

Итерацию зеркального спуска можно записать в нескольких эквивалентных формах:

. . .

**1. Проксимальная форма (основная):**
$$
x_{k+1} = \arg\min_{x \in S} \left\{ \alpha_k \langle \nabla f(x_k), x \rangle + V_\omega(x, x_k) \right\}
$$

. . .

**2. Двойственная форма (через зеркальное отображение):**

Для безусловной задачи ($S = \mathbb{R}^n$), используя условие оптимальности $\nabla_x V_\omega(x, x_k) = \nabla \omega(x) - \nabla \omega(x_k)$:
$$
\nabla \omega(x_{k+1}) = \nabla \omega(x_k) - \alpha_k \nabla f(x_k)
$$

. . .

**3. Проекция Брэгмана** (для условной задачи):
$$
\begin{aligned}
y_{k+1}:& \quad \nabla \omega(y_{k+1}) = \nabla \omega(x_k) - \alpha_k \nabla f(x_k) \\
x_{k+1} &= \arg\min_{x \in S} V_\omega(x, y_{k+1})
\end{aligned}
$$

. . .

В евклидовом случае ($\omega = \frac{1}{2}\|\cdot\|_2^2$) проекция Брэгмана совпадает с евклидовой проекцией.

## Зеркальный спуск на симплексе: Exponentiated Gradient

Важнейший частный случай: $S = \Delta_n$, $\omega(x) = \sum_i x_i \ln x_i$.

. . .

Итерация зеркального спуска принимает **замкнутую форму**:
$$
x_{k+1,i} = \frac{x_{k,i} \exp\left(-\alpha_k [\nabla f(x_k)]_i\right)}{\sum_{j=1}^n x_{k,j} \exp\left(-\alpha_k [\nabla f(x_k)]_j\right)}, \quad i = 1, \ldots, n
$$

. . .

Этот метод известен как **Exponentiated Gradient** (EG) или **Multiplicative Weights Update**.

. . .

**Вывод.** Из условия оптимальности: $\ln x_{k+1,i} + 1 - \ln x_{k,i} - 1 + \alpha_k g_i + \lambda = 0$, где $\lambda$ — множитель Лагранжа для ограничения $\sum_i x_i = 1$. Тогда $x_{k+1,i} \propto x_{k,i} \exp(-\alpha_k g_i)$, и нормировка даёт формулу выше.

. . .

**Стоимость:** $\mathcal{O}(n)$ — **не требуется** сортировка (в отличие от евклидовой проекции на симплекс, которая стоит $\mathcal{O}(n \log n)$).

## Скорость сходимости зеркального спуска \faGem \ \faGem[regular] \faGem[regular]

:::{.callout-theorem}
Пусть $f: S \to \mathbb{R}$ — выпуклая функция с $L$-липшицевым градиентом относительно нормы $\|\cdot\|$:
$$
\|\nabla f(x) - \nabla f(y)\|_* \leq L\|x - y\| \quad \forall x, y \in S
$$
Пусть прокс-функция $\omega$ является $\sigma$-сильно выпуклой относительно $\|\cdot\|$, и $R^2 = \max_{x \in S} V_\omega(x, x_0)$. Тогда зеркальный спуск с шагом $\alpha_k = \frac{1}{L/\sigma}$ обеспечивает:
$$
f(x_k) - f^* \leq \frac{L R^2}{\sigma k}
$$
:::

. . .

**Сравнение с PGD:**

| | PGD | Зеркальный спуск |
|:---:|:---:|:---:|
| Норма | $\|\cdot\|_2$ | $\|\cdot\|$ (произвольная) |
| Гладкость | $L_2$ | $L$ (отн. $\|\cdot\|$) |
| «Радиус» | $R_2 = \|x_0 - x^*\|_2$ | $R^2 = V_\omega(x^*, x_0)$ |
| Оценка | $\frac{L_2 R_2^2}{k}$ | $\frac{L R^2}{\sigma k}$ |

## Доказательство сходимости \faGem \ \faGem \ \faGem[regular]

1. Из $L$-гладкости $f$ относительно нормы $\|\cdot\|$ при шаге $\alpha = \sigma / L$:
    $$
    f(x_{k+1}) \leq f(x_k) + \langle \nabla f(x_k), x_{k+1} - x_k \rangle + \frac{L}{2}\|x_{k+1} - x_k\|^2
    $$

    . . .

2. Из $\sigma$-сильной выпуклости $\omega$: $\frac{L}{2}\|x_{k+1} - x_k\|^2 \leq \frac{L}{\sigma} V_\omega(x_{k+1}, x_k)$. Следовательно:
    $$
    f(x_{k+1}) \leq f(x_k) + \langle \nabla f(x_k), x_{k+1} - x_k \rangle + \frac{L}{\sigma} V_\omega(x_{k+1}, x_k)
    $$

    . . .

3. Из определения итерации (условие оптимальности) и выпуклости $f$ для любого $x \in S$:
    $$
    \alpha_k \langle \nabla f(x_k), x_{k+1} - x \rangle \leq V_\omega(x, x_k) - V_\omega(x, x_{k+1}) - V_\omega(x_{k+1}, x_k)
    $$

    . . .

    Это **лемма трёх точек** (three-point identity) для дивергенции Брэгмана:
    $$
    V_\omega(x, x_k) - V_\omega(x, x_{k+1}) = \langle \nabla\omega(x_{k+1}) - \nabla\omega(x_k), x - x_{k+1} \rangle + V_\omega(x_{k+1}, x_k)
    $$

## Доказательство сходимости (продолжение) \faGem \ \faGem \ \faGem

4. Подставляя $x = x^*$ и используя выпуклость $f(x_k) - f^* \leq \langle \nabla f(x_k), x_k - x^* \rangle$:
    $$
    \begin{aligned}
    \alpha_k(f(x_k) - f^*) &\leq \alpha_k \langle \nabla f(x_k), x_k - x^* \rangle \\
    &= \alpha_k \langle \nabla f(x_k), x_k - x_{k+1} \rangle + \alpha_k \langle \nabla f(x_k), x_{k+1} - x^* \rangle \\
    &\leq \alpha_k \langle \nabla f(x_k), x_k - x_{k+1} \rangle + V_\omega(x^*, x_k) - V_\omega(x^*, x_{k+1}) - V_\omega(x_{k+1}, x_k)
    \end{aligned}
    $$

    . . .

5. Суммируя по $k = 0, \ldots, K-1$ и используя телескопирование:
    $$
    \sum_{k=0}^{K-1} \alpha_k (f(x_k) - f^*) \leq V_\omega(x^*, x_0) + \sum_{k=0}^{K-1}\left[\alpha_k \langle \nabla f(x_k), x_k - x_{k+1} \rangle - V_\omega(x_{k+1}, x_k)\right]
    $$

    . . .

6. Используя шаг 2 для оценки каждого слагаемого и монотонность $f(x_k)$:
    $$
    K \cdot (f(x_K) - f^*) \leq \frac{L}{\sigma} V_\omega(x^*, x_0) = \frac{L R^2}{\sigma}
    $$
    откуда $f(x_K) - f^* \leq \frac{LR^2}{\sigma K}$, что и требовалось доказать.

## Пример: зеркальный спуск vs PGD на симплексе

:::: {.columns}

::: {.column width="50%"}

Рассмотрим задачу
$$
\min_{x \in \Delta_n} f(x) = \frac{1}{2}\|Ax - b\|_2^2
$$

* PGD: евклидова проекция на $\Delta_n$, $\mathcal{O}(n \log n)$ на шаг
* MD (EG): мультипликативное обновление, $\mathcal{O}(n)$ на шаг

. . .

| Параметр | PGD | MD (EG) |
|:---:|:---:|:---:|
| Гладкость | $L_2 = \|A^\top A\|_{\text{op}}$ | $L_\infty = \max_{ij}|A^\top A|_{ij}$ |
| «Радиус» | $R_2^2 \leq 2$ | $R^2 \leq \ln n$ |
| Оценка | $\frac{L_2}{k}$ | $\frac{L_\infty \ln n}{k}$ |

:::

::: {.column width="50%"}

Когда $A$ имеет слабую корреляцию между столбцами: $L_2 \approx n \cdot L_\infty$. Тогда:

* PGD: $\mathcal{O}\!\left(\frac{n L_\infty}{k}\right)$
* MD: $\mathcal{O}\!\left(\frac{L_\infty \ln n}{k}\right)$

. . .

**Выигрыш:** $\frac{n}{\ln n}$ раз!

. . .

Для $n = 1000$: выигрыш $\approx 145$ раз.

Для $n = 10^6$: выигрыш $\approx 72\,400$ раз.

:::

::::

## Негладкая задача: MD побеждает по итерациям {.smaller}

**Задача Бека**: $\min_{x \in \Delta_n} \|Ax - b\|_1$ — негладкая оптимизация на симплексе

![](exp_md_nonsmooth_l1.pdf)

* Субградиент $g = A^\top \mathrm{sign}(Ax-b)$: $\|g\|_2$ растёт с $\sqrt{n}$, а $\|g\|_\infty$ остаётся ограниченным
* **Зеркальный спуск** использует $\ell_\infty$-геометрию и побеждает в $\sim\sqrt{n / \ln n}$ раз!

## Негладкая задача: честное сравнение по времени {.smaller}

![](exp_md_nonsmooth_l1_time.pdf)

* MD побеждает **и по времени**: стоимость итерации сравнима ($O(n)$ vs $O(n \log n)$), но MD сходится существенно быстрее
* Преимущество растёт с размерностью: для $n=2000$ субградиентный метод за то же время далеко от оптимума

## Масштабирование преимущества MD {.smaller}

::::{.columns}
:::{.column width="50%"}

![](exp_md_scaling_dimension.pdf)

:::
:::{.column width="50%"}

* При росте $n$ невязка субградиентного метода **растёт**, а MD остаётся малой
* Число итераций до $\varepsilon$-точности: MD требует меньше при любом $n$

:::
::::

. . .

::::{.columns}
:::{.column width="50%"}

![](exp_md_iteration_cost.pdf){height=250}

:::
:::{.column width="50%"}

* Стоимость шага MD — $\mathcal{O}(n)$, дешевле PGD — $\mathcal{O}(n \log n)$
* MD и FW имеют одинаковую асимптотику $\mathcal{O}(n)$

:::
::::

## KL-дивергенция: относительная гладкость {.smaller}

$f(x) = \sum_{i=1}^{m} \text{KL}(A_i x \| b_i)$ на $\Delta_n$ — задача с **неограниченной** $L_2$-гладкостью, но **ограниченной** относительной гладкостью по Брэгману!

![](exp_md_kl_divergence.pdf)

* Константа $L_h = \max_k \sum_{i,j} (A_i)_{jk}$ вычислима из данных задачи; MD шаг $\eta = 1/L_h$ — теория гарантирует $O(1/k)$
* $L_2$ константа Липшица в **175 раз** больше $L_h$ → шаг PGD $\alpha = 1/L_2$ сверхконсервативен; Armijo — медленнее по времени
* Правый график: $L_h$ vs $L_2(x_k)$ — **почему** неевклидова геометрия выигрывает

## Итоги: зеркальный спуск

* **Обобщение PGD:** заменяем евклидову проекцию дивергенцией Брэгмана, адаптируя геометрию метода к задаче.
* **Скорость сходимости:** $\mathcal{O}\!\left(\frac{LR^2}{\sigma k}\right)$ — та же структура, что у PGD, но с параметрами, адаптированными к «правильной» норме.
* **Когда выигрывает:**
    * **Негладкие** задачи на симплексе: $\min \|Ax-b\|_1$ — выигрыш в $\sim\sqrt{n/\ln n}$ раз (и по итерациям, и по времени!)
    * **KL-дивергенция** и задачи с **относительной гладкостью**: $L_2$ неограничена, но MD сходится с фиксированным шагом
    * Задачи, где $\|g\|_2 \gg \|g\|_\infty$; высокоразмерные задачи, где $\ell_2$-оценки зависят от $n$
* **Ключевой результат на симплексе:** с прокс-функцией $\omega(x) = \sum_i x_i \ln x_i$ зеркальный спуск даёт оценку $\mathcal{O}\!\left(\frac{L_\infty \ln n}{k}\right)$ вместо $\mathcal{O}\!\left(\frac{n L_\infty}{k}\right)$ у PGD — экспоненциальный выигрыш по размерности.
* Выбор прокс-функции $\omega$ — ключевое решение при применении метода. Общее правило: $\omega$ должна быть сильно выпуклой относительно нормы, в которой $f$ является гладкой.