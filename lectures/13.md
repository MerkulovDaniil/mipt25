---
title: "Метод тяжёлого шарика. Ускоренный градиентный метод Нестерова"
author: Даня Меркулов
institute: Методы оптимизации. МФТИ
format: 
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back13_.jpeg}
---
# Повторение

## Результаты сходимости градиентного спуска для гладких функций

$$
\text{Градиентный спуск:} \qquad \qquad \min_{x \in \mathbb{R}^n} f(x) \qquad \qquad x_{k+1} = x_k - \alpha_k \nabla f(x_k) \qquad \lambda \left( \nabla^2 f(x)\right) \in \left[ \mu, L \right], \varkappa = \tfrac{L}{\mu}
$$

|выпуклая (негладкая) | гладкая (невыпуклая) | гладкая & выпуклая | гладкая & сильно выпуклая |
|:------:|:-------:|:------:|:--------:|
| $f(x_k) - f^* =  \mathcal{O} \left( \tfrac{1}{\sqrt{k}} \right)$ | $\min\limits_{0 \leq i \leq k}\|\nabla f(x_i)\| = \mathcal{O} \left( \tfrac{1}{\sqrt{k}} \right)$ | $f(x_k) - f^* =  \mathcal{O} \left( \tfrac{1}{k} \right)$ | $\|x_k - x^*\|^2 = \mathcal{O} \left( \left(1 - \tfrac{\mu}{L}\right)^k \right)$ |
| $k_\varepsilon =  \mathcal{O} \left( \tfrac{1}{\varepsilon^2} \right)$ | $k_\varepsilon = \mathcal{O} \left( \tfrac{1}{\varepsilon^2} \right)$ | $k_\varepsilon =  \mathcal{O}  \left( \tfrac{1}{\varepsilon} \right)$ | $k_\varepsilon  = \mathcal{O} \left( \varkappa \log \tfrac{1}{\varepsilon}\right)$ |

## Нижние оценки для методов I порядка на классе гладких функций

$$
\text{Произвольный метод I порядка:} \qquad \min_{x \in \mathbb{R}^n} f(x) \qquad x_{k+1} = x_k - \sum_{i=0}^k\alpha_i \nabla f(x_i) \qquad \lambda \left( \nabla^2 f(x) \right) \in \left[ \mu, L \right], \varkappa = \tfrac{L}{\mu}
$$

| выпуклая (негладкая) | гладкая (невыпуклая)^[[Carmon, Duchi, Hinder, Sidford, 2017](https://arxiv.org/pdf/1710.11606.pdf)] | гладкая & выпуклая^[[Nemirovski, Yudin, 1979](https://fmin.xyz/assets/files/nemyud1979.pdf)] | гладкая & сильно выпуклая |
|:------:|:-------:|:------:|:--------:|
| $f(x_k) - f^* =  \Omega \left( \tfrac{1}{\sqrt{k}} \right)$ | $\min\limits_{0 \leq i \leq k}\|\nabla f(x_i)\| = \Omega \left( \tfrac{1}{\sqrt{k}} \right)$ | $f(x_k)-f^*=\Omega\!\left(\tfrac{1}{k^2}\right)$ | $f(x_k)-f^*=\Omega\!\left(\left(\tfrac{\sqrt{\varkappa}-1}{\sqrt{\varkappa}+1}\right)^{\!2k}\right)$ |
| $k_\varepsilon =  \Omega \left( \tfrac{1}{\varepsilon^2} \right)$ | $k_\varepsilon=\Omega\!\left(\tfrac{1}{\varepsilon^2}\right)$ | $k_\varepsilon=\Omega\!\left(\tfrac{1}{\sqrt{\varepsilon}}\right)$ | $k_\varepsilon=\Omega\!\big(\sqrt{\varkappa}\,\log\tfrac{1}{\varepsilon}\big)$ |


# Метод тяжёлого шарика

## Колебания и ускорение

$$
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1}).
$$

[![](GD_vs_HB_hor.pdf)](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/GD.ipynb)

## Метод тяжёлого шарика Поляка

:::: {.columns}

::: {.column width="25%"}
![](GD_HB.pdf)
:::

::: {.column width="75%"}
Рассмотрим идею моментума (импульса, тяжёлого шарика), предложенную Б.Т. Поляком в 1964 году. Обновление метода тяжёлого шарика имеет вид

$$
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1}).
$$
\pause
Давайте теперь подставим в итерацию предыдущую итерацию, т.е. $x_{k} = x_{k-1} - \alpha \nabla f(x_{k-1}) + \beta (x_{k-1} - x_{k-2})$, а так же отметим, что $x_{k} - x_{k-1} = -\alpha \nabla f(x_{k-1}) + \beta (x_{k-1} - x_{k-2})$:
\pause
$$
\begin{aligned}
x_{k+1} &= x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1}) \pause \\ 
&= x_k - \alpha \nabla f(x_k) + \beta \left( -\alpha \nabla f(x_{k-1}) + \beta (x_{k-1} - x_{k-2}) \right) \pause \\ 
&= x_k - \alpha \left[ \nabla f(x_k) + \beta \nabla f(x_{k-1})\right] + \beta^2 (x_{k-1} - x_{k-2}) \pause \\ 
&= x_k - \alpha \left[ \nabla f(x_k) + \beta \nabla f(x_{k-1}) + \beta^2 \nabla f(x_{k-2})\right] + \beta^3 (x_{k-2} - x_{k-3}) \pause \\ 
&\vdots \pause \\ 
&= x_k - \alpha \left[ \nabla f(x_k) + \beta \nabla f(x_{k-1}) + \beta^2 \nabla f(x_{k-2}) + \cdots + \beta^{k} \nabla f(x_0) \right]
\end{aligned}
$$
\pause
Таким образом, метод тяжёлого шарика учитывает все предудущие градиенты с тем меньшим весом, чем старше итерация ($0 \leq \beta < 1$).

:::
::::

## Метод тяжёлого шарика Поляка для сильно выпуклой квадратичной функции

Мы уже до этого показывали, как для произвольной сильно выпуклой квадратичной функции можно сделать замену координат так, чтобы в новых координатах матрица квадратичной формы имела диагональный вид. Поэтому, можно рассмотреть функцию $f(x) = \frac{1}{2} x^T \Lambda x$ с диагональной матрицей $\Lambda$ и собственными значениями $\lambda(\Lambda) \in \left[\mu, L\right]$. Тогда
$$
x_{k+1} = x_k - \alpha \Lambda x_k + \beta (x_k - x_{k-1}) = (I - \alpha \Lambda + \beta I) x_k - \beta x_{k-1}
$$

\pause

Это можно переписать как

$$
\begin{split}
&\hat{x}_{k+1} = (I - \alpha \Lambda + \beta I) \hat{x}_k - \beta \hat{x}_{k-1}, \\
&\hat{x}_{k} = \hat{x}_k.
\end{split}
$$

\pause

Давайте введем следующее обозначение: $\hat{z}_k = \begin{bmatrix}
\hat{x}_{k+1} \\
\hat{x}_{k}
\end{bmatrix}$. Следовательно, $\hat{z}_{k+1} = M \hat{z}_k$, где матрица итерации $M$ имеет вид:

\pause

$$
M = \begin{bmatrix}
I - \alpha \Lambda + \beta I & - \beta I \\
I & 0_{d}
\end{bmatrix}.
$$

## Сведение к скалярному случаю

Обратим внимание, что $M$ является матрицей $2d \times 2d$ с четырьмя блочно-диагональными матрицами размера $d \times d$ внутри. Это означает, что мы можем изменить порядок координат, чтобы сделать $M$ блочно-диагональной. Обратите внимание, что в уравнении ниже матрица $M$ обозначает то же самое, что и в обозначении выше, за исключением описанной перестановки строк и столбцов. Мы используем эту небольшую перегрузку обозначений для простоты. 

. . .

:::: {.columns}

::: {.column width="40%"}

![Иллюстрация перестановки матрицы $M$](Rearranging_squares.pdf)

:::
:::{.column width="60%"}
$$
\begin{aligned}
\begin{bmatrix} 
\hat{x}_{k}^{(1)} \\
\vdots \\
\hat{x}_{k}^{(d)} \\
\addlinespace 
\hat{x}_{k-1}^{(1)} \\
\vdots \\
\hat{x}_{k-1}^{(d)}
\end{bmatrix} \to 
\begin{bmatrix} 
\hat{x}_{k}^{(1)} \\
\addlinespace 
\hat{x}_{k-1}^{(1)} \\
\vdots \\
\hat{x}_{k}^{(d)} \\
\addlinespace 
\hat{x}_{k-1}^{(d)}
\end{bmatrix} \quad M = \begin{bmatrix}
M_1\\
&M_2\\
&&\ldots\\
&&&M_d
\end{bmatrix}
\end{aligned}
$$
:::
::::
где $\hat{x}_{k}^{(i)}$ является $i$-й координатой вектора $\hat{x}_{k} \in \mathbb{R}^d$ и $M_i$ обозначает матрицу размера $2 \times 2$. Переупорядочение позволяет нам исследовать динамику метода независимо от размерности. Асимптотическая скорость сходимости последовательности векторов $\hat{z}_k$ размерности $2d$ определяется наихудшей скоростью сходимости среди его блока координат. Следовательно, достаточно исследовать оптимизацию в одномерном случае.

## Сведение к скалярному случаю

Для $i$-й координаты, где $\lambda_i$ — $i$-е собственное значение матрицы $A$, имеем: 

$$
M_i = \begin{bmatrix} 
1 - \alpha \lambda_i + \beta & -\beta \\
1 & 0
\end{bmatrix}.
$$

. . .

Метод будет сходиться, если $\rho(M) < 1$, и оптимальные параметры могут быть вычислены путем оптимизации спектрального радиуса
$$
\alpha^*, \beta^* = \arg \min_{\alpha, \beta} \max_{i} \rho(M_i), \quad \alpha^* = \dfrac{4}{(\sqrt{L} + \sqrt{\mu})^2}, \quad \beta^* = \left(\dfrac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}\right)^2.
$$

. . .

Можно показать, что для таких параметров матрица $M$ имеет комплексные собственные значения, которые образуют комплексно-сопряжённую пару, поэтому расстояние до оптимума (в этом случае $\| z_k \|$) обычно не убывает монотонно. 

## Характеристическое уравнение

Собственные значения матрицы $M_i$ определяются из характеристического уравнения $\det(M_i - zI) = 0$:
\pause
$$
\det \begin{pmatrix} 1 - \alpha \lambda_i + \beta - z & -\beta \\ 1 & -z \end{pmatrix} = -z(1 - \alpha \lambda_i + \beta - z) + \beta = z^2 - (1 - \alpha \lambda_i + \beta)z + \beta = 0
$$
\pause
Пусть $z_1, z_2$ — корни этого уравнения. По теореме Виета:
$$
z_1 z_2 = \beta, \qquad z_1 + z_2 = 1 - \alpha \lambda_i + \beta
$$
\pause
Спектральный радиус $\rho(M_i) = \max(|z_1|, |z_2|)$. Для сходимости необходимо $\rho(M_i) < 1$, что подразумевает $\beta < 1$ (так как $z_1 z_2 = \beta$).

## Анализ дискриминанта: Вещественные корни

Дискриминант квадратного уравнения $z^2 - (1 - \alpha \lambda_i + \beta)z + \beta = 0$:
$$
D = (1 - \alpha \lambda_i + \beta)^2 - 4\beta
$$
Рассмотрим случай **вещественных корней** ($D \ge 0$).
Корни вещественны и $z_1, z_2 = \frac{1 - \alpha \lambda_i + \beta \pm \sqrt{D}}{2}$.
Так как $z_1 z_2 = \beta$, то если корни различны, один из них по модулю должен быть больше $\sqrt{\beta}$ (если только они не равны $\pm \sqrt{\beta}$).
Более того, если $D > 0$, то $\max(|z_1|, |z_2|) > \sqrt{\beta}$.
Это означает, что скорость сходимости будет хуже, чем $\sqrt{\beta}$.

## Анализ дискриминанта: Комплексные корни

Рассмотрим случай **комплексных корней** ($D < 0$).
Корни комплексно-сопряженные:
$$
z_{1,2} = \frac{1 - \alpha \lambda_i + \beta \pm i\sqrt{4\beta - (1 - \alpha \lambda_i + \beta)^2}}{2}
$$
Вычислим квадрат модуля корней:
$$
|z_{1,2}|^2 = \left(\frac{1 - \alpha \lambda_i + \beta}{2}\right)^2 + \left(\frac{\sqrt{4\beta - (1 - \alpha \lambda_i + \beta)^2}}{2}\right)^2
$$
$$
= \frac{(1 - \alpha \lambda_i + \beta)^2 + 4\beta - (1 - \alpha \lambda_i + \beta)^2}{4} = \frac{4\beta}{4} = \beta
$$
Следовательно, $|z_1| = |z_2| = \sqrt{\beta}$.

## Вывод по дискриминанту

\pause
*   В случае **комплексных корней** спектральный радиус $\rho(M_i) = \sqrt{\beta}$ и **не зависит от $\lambda_i$**.
*   В случае **вещественных корней** спектральный радиус $\rho(M_i) \ge \sqrt{\beta}$ и зависит от $\lambda_i$.

\pause
**Стратегия:** Мы хотим минимизировать худший спектральный радиус по всем $\lambda_i$.
Наилучшая ситуация достигается, когда для всех $\lambda_i$ корни комплексные (или на границе $D=0$), и мы минимизируем $\sqrt{\beta}$.
Поэтому мы требуем выполнения условия $D \le 0$ для всех $\lambda_i \in [\mu, L]$.

## Постановка задачи оптимизации

Мы ищем $\alpha > 0, \beta \ge 0$, минимизирующие спектральный радиус $\rho(\alpha, \beta) = \max_{\lambda \in [\mu, L]} \max(|z_1(\lambda)|, |z_2(\lambda)|)$.
Радиус корней для фиксированного $\lambda$:
$$
r(\lambda) = \begin{cases} \frac{1}{2} \Bigl( |1+\beta-\alpha \lambda| + \sqrt{(1+\beta - \alpha \lambda)^2 - 4\beta} \Bigr), & \text{если } D > 0 \\ \sqrt{\beta}, & \text{если } D \le 0 \end{cases}
$$
\pause
Обозначим $\alpha_{opt} = \left( \frac{2}{\sqrt{L}+\sqrt{\mu}} \right)^2$.
Заметим, что $D \le 0 \iff \beta \ge (1-\sqrt{\alpha \lambda})^2$.
Также $|1-\sqrt{\alpha \mu}| < |1-\sqrt{\alpha L}| \iff \alpha > \alpha_{opt}$.

## Анализ случаев

Рассмотрим 4 случая в зависимости от $\alpha$ и $\beta$:

1.  $0 < \alpha \le \alpha_{opt}$ и $\beta \ge (1-\sqrt{\alpha \mu})^2$.
    Тогда $\rho = \sqrt{\beta} \ge 1-\sqrt{\alpha \mu} \ge \frac{\sqrt{L}-\sqrt{\mu}}{\sqrt{L}+\sqrt{\mu}}$.
    \pause
2.  $0 < \alpha \le \alpha_{opt}$ и $\beta < (1-\sqrt{\alpha \mu})^2$.
    Тогда $\rho \ge r(\mu) > 1-\sqrt{\alpha \mu} \ge \frac{\sqrt{L}-\sqrt{\mu}}{\sqrt{L}+\sqrt{\mu}}$.
    (Здесь $r(\mu)$ убывает по $\beta$).

## Анализ случаев (продолжение)

3.  $\alpha > \alpha_{opt}$ и $\beta \ge (\sqrt{\alpha L}-1)^2$.
    Тогда $\rho = \sqrt{\beta} \ge \sqrt{\alpha L}-1 > \frac{\sqrt{L}-\sqrt{\mu}}{\sqrt{L}+\sqrt{\mu}}$.
    \pause
4.  $\alpha > \alpha_{opt}$ и $\beta < (\sqrt{\alpha L}-1)^2$.
    Тогда $\rho \ge r(L) > \sqrt{\alpha L}-1 > \frac{\sqrt{L}-\sqrt{\mu}}{\sqrt{L}+\sqrt{\mu}}$.
    (Здесь $r(L)$ убывает по $\beta$).

## Оптимальные параметры

Во всех случаях $\rho(\alpha, \beta) \ge \frac{\sqrt{L}-\sqrt{\mu}}{\sqrt{L}+\sqrt{\mu}}$.
Равенство достигается только в первом случае на границе:
$$
\alpha^* = \left( \frac{2}{\sqrt{L}+\sqrt{\mu}} \right)^2, \quad \beta^* = \left( \frac{\sqrt{L}-\sqrt{\mu}}{\sqrt{L}+\sqrt{\mu}} \right)^2
$$
\pause
При этом оптимальная скорость сходимости:
$$
\rho_{opt} = \frac{\sqrt{L}-\sqrt{\mu}}{\sqrt{L}+\sqrt{\mu}} = \frac{\sqrt{\varkappa}-1}{\sqrt{\varkappa}+1}
$$
Это соответствует сложности $O(\sqrt{\varkappa} \log \frac{1}{\varepsilon})$.









## Сходимость метода тяжёлого шарика для квадратичной функции

:::{.callout-theorem}
Предположим, что $f$ является $\mu$-сильно выпуклой и $L$-гладкой квадратичной функцией. Тогда метод тяжёлого шарика с параметрами
$$
\alpha = \dfrac{4}{(\sqrt{L} + \sqrt{\mu})^2}, \beta = \left(\dfrac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}\right)^2
$$

сходится линейно:

$$
\|x_k - x^*\|_2 \leq \left( \dfrac{\sqrt{\varkappa} - 1}{\sqrt{\varkappa} + 1} \right)^k \|x_0 - x^*\|
$$

:::

## Глобальная сходимость метода тяжёлого шарика ^[[Глобальная сходимость метода тяжёлого шарика для выпуклой оптимизации, Euhanna Ghadimi et al.](https://arxiv.org/abs/1412.7457)]

:::{.callout-theorem}
Предположим, что $f$ является гладкой и выпуклой и что

$$
\beta\in[0,1),\quad \alpha\in\biggl(0,\dfrac{2(1-\beta)}{L}\biggr).
$$

Тогда последовательность $\{x_k\}$, генерируемая итерациями тяжёлого шарика, удовлетворяет

$$
f(\overline{x}_T)-f^{\star} \leq  \left\{
\begin{array}[l]{ll}
\frac{\Vert x_{0}-x^\star\Vert^2}{2(T+1)}\biggl(\frac{L\beta}{1-\beta}+\frac{1-\beta}{\alpha}\biggr),\;\;\textup{if}\;\;
\alpha\in\bigl(0,\dfrac{1-\beta}{L}\bigr],\\
\frac{\Vert x_{0}-x^\star\Vert^2}{2(T+1)(2(1-\beta)-\alpha L)}\biggl({L\beta}+\frac{(1-\beta)^2}{\alpha}\biggr),\;\;\textup{if}\;\;
\alpha\in\bigl[\dfrac{1-\beta}{L},\dfrac{2(1-\beta)}{L}\bigr),
\end{array}
\right.
$$

где $\overline{x}_T$ среднее Чезаро последовательности итераций, т.е. 

$$
\overline{x}_T = \frac{1}{T+1}\sum_{k=0}^T x_k.
$$
:::


## Глобальная сходимость метода тяжёлого шарика ^[[Глобальная сходимость метода тяжёлого шарика для выпуклой оптимизации, Euhanna Ghadimi et al.](https://arxiv.org/abs/1412.7457)]

:::{.callout-theorem}
Предположим, что $f$ является гладкой и сильно выпуклой и что

$$
\alpha\in\biggl(0,\dfrac{2}{L}\biggr),\quad 0\leq  \beta<\dfrac{1}{2}\biggl( \dfrac{\mu \alpha}{2}+\sqrt{\dfrac{\mu^2\alpha^2}{4}+4(1-\frac{\alpha L}{2})} \biggr) .
$$

Тогда последовательность $\{x_k\}$, генерируемая итерациями методатяжёлого шарика, сходится линейно к единственному оптимальному решению $x^\star$. В частности,

$$
f(x_{k})-f^\star \leq q^k (f(x_0)-f^\star),
$$

где $q\in[0,1)$.
:::

## Итоги по методу тяжёлого шарика

* Обеспечивает ускоренную сходимость для сильно выпуклых квадратичных задач.
* Локально ускоренная сходимость была доказана в оригинальной статье.
* Недавно ^[[Provable non-accelerations of the heavy-ball method](https://arxiv.org/pdf/2307.11291)] было доказано, что глобального ускорения сходимости для метода не существует.
* Метод не был чрезвычайно популярен до ML-бума.
* Сейчас он фактически является стандартом для практического ускорения методов градиентного спуска, в том числе для невыпуклых задач (обучение нейронных сетей).

# Ускоренный градиентный метод Нестерова

## Концепция ускоренного градиентного метода Нестерова

:::: {.columns}

::: {.column width="27%"}
$$
x_{k+1} = x_k - \alpha \nabla f(x_k)
$$
:::
::: {.column width="34%"}
$$
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1})
$$
:::
::: {.column width="39%"}
$$
\begin{cases}y_{k+1} = x_k + \beta (x_k - x_{k-1}) \\ x_{k+1} = y_{k+1} - \alpha \nabla f(y_{k+1}) \end{cases}
$$
:::

::::

. . .

:::: {.columns}
::: {.column width="67%"}

Давайте определим следующие обозначения

$$
\begin{aligned}
x^+ &= x - \alpha \nabla f(x) \qquad &\text{Градиентный шаг} \\
d_k &= \beta_k (x_k - x_{k-1}) \qquad &\text{Импульс}
\end{aligned}
$$

Тогда мы можем записать:


$$
\begin{aligned}
x_{k+1} &= x_k^+ \qquad &\text{Градиентный спуск} \\
x_{k+1} &= x_k^+ + d_k \qquad &\text{Метод тяжёлого шарика} \\
x_{k+1} &= (x_k + d_k)^+ \qquad &\text{Ускоренный градиентный метод Нестерова}
\end{aligned}
$$
:::
::: {.column width="33%"}

![](AGD.pdf)

:::
::::
## Сходимость для выпуклых функций

:::{.callout-theorem}
Предположим, что $f : \mathbb{R}^n \rightarrow \mathbb{R}$ является выпуклой и $L$-гладкой. Ускоренный градиентный метод Нестерова (NAG) предназначен для решения задачи минимизации, начиная с начальной точки $x_0 = y_0 \in \mathbb{R}^n$ и $\lambda_0 = 0$. Алгоритм выполняет следующие шаги:
$$
\begin{aligned}
&\textbf{Обновление градиента: } &x_{k+1} &= y_k - \frac{1}{L} \nabla f(y_k) \\
& \textbf{Вес экстраполяции: } &\lambda_{k+1} &= \frac{1 + \sqrt{1 + 4\lambda_k^2}}{2} \\
& \quad &\gamma_k &= \frac{\lambda_k - 1}{\lambda_{k+1}} \\
&\textbf{Экстраполяция: } &y_{k+1} &= x_{k+1} + \gamma_k\left(x_{k+1} - x_k\right)
\end{aligned}
$$
Последовательность $\{f(x_k)\}_{k\in\mathbb{N}}$, генерируемая алгоритмом, сходится к оптимальному значению $f^*$ со скоростью $\mathcal{O}\left(\frac{1}{k^2}\right)$, в частности:
$$
f(x_k) - f^* \leq \frac{2L \|x_0 - x^*\|^2}{k^2}
$$
:::

## Ускоренная сходимость для сильно выпуклых функций

:::{.callout-theorem}
Предположим, что $f : \mathbb{R}^n \rightarrow \mathbb{R}$ является $\mu$-сильно выпуклой и $L$-гладкой. Ускоренный градиентный метод Нестерова (NAG) предназначен для решения задачи минимизации, начиная с начальной точки $x_0 = y_0 \in \mathbb{R}^n$ и $\lambda_0 = 0$. Алгоритм выполняет следующие шаги:
$$
\begin{aligned}
&\textbf{Обновление градиента: } &x_{k+1} &= y_k - \frac{1}{L} \nabla f(y_k) \\
&\textbf{Экстраполяция: } &y_{k+1} &= x_{k+1} -  \gamma \left(x_{k+1} - x_k\right) \\
&\textbf{Вес экстраполяции: } &\gamma &= \frac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}
\end{aligned}
$$
Последовательность $\{f(x_k)\}_{k\in\mathbb{N}}$, генерируемая алгоритмом, сходится к оптимальному значению $f^*$ линейно:
$$
f(x_k) - f^* \leq \frac{\mu + L}{2}\|x_0 - x^*\|^2_2 \exp \left(-\frac{k}{\sqrt{\varkappa}}\right)
$$
:::

# Численные эксперименты

## Выпуклая квадратичная задача (линейная регрессия)

![](agd_random_0_10_60.pdf)

## Сильно выпуклая квадратичная задача (регуляризованная линейная регрессия)

![](agd_random_1_10_60.pdf)

## Сильно выпуклая квадратичная задача (регуляризованная линейная регрессия)

![](agd_random_1_1000_60.pdf)

## Сильно выпуклая квадратичная задача (регуляризованная линейная регрессия)

![](agd_random_1_1000_1000.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.1.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.2.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.3.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.4.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.5.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.6.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.7.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.8.pdf) 

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.9.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.95.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.99.pdf)

## Сильно выпуклая бинарная логистическая регрессия

![](agd_strongly_convex_logreg_0.25.pdf)

## Сильно выпуклая бинарная логистическая регрессия

![](agd_strongly_convex_logreg_0.5.pdf)

## Сильно выпуклая бинарная логистическая регрессия

![](agd_strongly_convex_logreg_0.7.pdf)

## Сильно выпуклая бинарная логистическая регрессия

![](agd_strongly_convex_logreg_0.9.pdf)