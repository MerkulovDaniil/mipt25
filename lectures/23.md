---
title: "Big models"
author: Daniil Merkulov
institute: Optimization methods. MIPT
format: 
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
 - \newcommand{\bgimage}{../files/back23.jpeg}
---


# Обучение больших моделей

## Потребление памяти при обучении GPT-2 ^[[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)]

![](gpt2_memory_hor.pdf)

* Размер модели 1.5 B. Веса модели в fp16 занимают всего 3 GB, однако, для наивного обучения не хватит GPU даже на 32 GB 
* Для использования Adam в режиме mixed precision необходимо хранить 3 (!) копии модели в fp32.
* Активации в наивном режиме могут занимать гораздо больше памяти: для длины последовательности 1K и размера батча 32 нужно 60 GB для хранения всех промежуточных активаций. Чекпоинтинг активаций позволяет сократить потребление до 8 GB за счёт их перевычисления (33% computational overhead)

## Large batch training ^[[Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677)]

![](time.pdf){width=90%}

## Large batch training ^[[An Empirical Model of Large-Batch Training](https://arxiv.org/abs/1812.06162)]

![](basic-scaling.png)

## Large batch training ^[[Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677)]

![](batchsize.pdf){width=85%}


## Linear and square root scaling rules

When training with large batches, the learning rate must be adjusted to maintain convergence speed and stability. The **linear scaling rule**^[[Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677)] suggests multiplying the learning rate by the same factor as the increase in batch size:
$$
\alpha_{\text{new}} = \alpha_{\text{base}} \cdot \frac{\text{Batch Size}_{\text{new}}}{\text{Batch Size}_{\text{base}}}
$$
The **square root scaling rule**^[[Learning Rates as a Function of Batch Size: A Random Matrix Theory Approach to Neural Network Training](https://arxiv.org/abs/2006.09092)] proposes scaling the learning rate with the square root of the batch size increase:
$$
\alpha_{\text{new}} = \alpha_{\text{base}} \cdot \sqrt{\frac{\text{Batch Size}_{\text{new}}}{\text{Batch Size}_{\text{base}}}}
$$
Authors claimed, that it suits for adaptive optimizers like Adam, RMSProp and etc. while linear scaling rule serves well for SGD.

## Gradual warmup ^[[Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677)]

Gradual warmup helps to avoid instability when starting with large learning rates by slowly increasing the learning rate from a small value to the target value over a few epochs. This is defined as:
$$
\alpha_t = \alpha_{\text{max}} \cdot \frac{t}{T_w}
$$
where $t$ is the current iteration and $T_w$ is the warmup duration in iterations. In the original paper, authors used first 5 epochs for gradual warmup.

:::: {.columns}
::: {.column width="36%"}
![no warmup](distr-warmup-none.pdf)
:::

::: {.column width="32%"}
![constant warmup](distr-warmup-constant.pdf)
:::

::: {.column width="32%"}
![gradual warmup](distr-warmup-gradual.pdf)
:::

::::

## Cooldown^[[Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations](https://arxiv.org/pdf/2405.18392)] ^[[Scaling Vision Transformers](https://arxiv.org/abs/2106.04560v2)]

:::: {.columns}

::: {.column width="50%"}

\vspace{12pt}

![](lr_schedule.pdf)

:::

::: {.column width="50%"}

![](scaling_360M_val_perplexity.pdf)

:::
::::



## NanoGPT speedrun

![[\faLink\ Источник](https://github.com/KellerJordan/modded-nanogpt)](nanogpt_speedrun.pdf){width=96%}

## Работают ли трюки, если увеличить размер модели?

![[\faLink\ Источник](https://github.com/KellerJordan/modded-nanogpt/blob/master/img/nanogpt_speedrun51.png)](nanogpt_speedrun_scale.png){width=75%}

## Работают ли трюки, если увеличить размер модели?

![[\faLink\ Источник](https://github.com/KellerJordan/modded-nanogpt/blob/master/img/nanogpt_speedrun52.png)](nanogpt_speedrun_tokens.png){width=65%}

## 

[![](gd_scalar_convergence.pdf)](https://fmin.xyz/docs/visualizations/sgd_3.mp4)

## 

[![](gd_scalar_convergence_to_local_minimum.pdf)](https://fmin.xyz/docs/visualizations/sgd_4.mp4)

## 

[![](sgd_escape.pdf)](https://fmin.xyz/docs/visualizations/sgd_5.mp4)


# Эта задача оптимизации даже сложнее, чем кажется

## Impact of initialization ^[[On the importance of initialization and momentum in deep learning Ilya Sutskever, James Martens, George Dahl, Geoffrey Hinton](https://proceedings.mlr.press/v28/sutskever13.html)]

:::{.callout-tip appearance="simple"}
Properly initializing a NN important. NN loss is highly nonconvex; optimizing it to attain a “good” solution hard, requires careful tuning. 
:::

* Don’t initialize all weights to be the same — why?
* Random: Initialize randomly, e.g., via the Gaussian $N(0, \sigma^2)$, where std $\sigma$ depends on the number of neurons in a given layer. *Symmetry breaking*.
* One can find more useful advices [here](https://cs231n.github.io/neural-networks-2/)

## Влияние инициализации весов нейронной сети на сходимость методов ^[[Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun](https://arxiv.org/abs/1502.01852)]

:::: {.columns}
::: {.column width="50%"}
![22-layer ReLU net: good init converges faster](converge_22layers.pdf)
:::

::: {.column width="50%"}
![30-layer ReLU net: good init is able to converge](converge_30layers.pdf)
:::

::::

## Методы уменьшения дисперсии: почему не работают на глубоких сетях? ^[[On the Ineffectiveness of Variance Reduced Optimization for Deep Learning](https://arxiv.org/abs/1812.04529)]

:::: {.columns}
::: {.column width="45%"}

\begin{figure}
  \centering
  \begin{minipage}[b]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{variance_ratios_densenet.pdf}
    \caption{DenseNet}
  \end{minipage}%
  \begin{minipage}[b]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{variance_ratios_small-resnet.pdf}
    \caption{Small ResNet}
  \end{minipage}

  \vspace{1em} % небольшой отступ между рядами

  \begin{minipage}[b]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{variance_ratios_lenet.pdf}
    \caption{LeNet‑5}
  \end{minipage}%
  \begin{minipage}[b]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{variance_ratios_resnet110.pdf}
    \caption{ResNet‑110}
  \end{minipage}
\end{figure}

:::


::: {.column width="55%"}
* **SVRG / SAG** дают убедительные выигрыши в выпуклых задачах, но на CIFAR‑10 (LeNet‑5) и ImageNet (ResNet‑18) не опережают обычный SGD.
* Измеренное отношение «дисперсия SGD / дисперсия SVRG» остаётся $\lesssim 2$ для большинства слоёв - то есть реальное снижение шума минимально.
* Возможные причины:
  * **Аугментация данных** делает опорный градиент $g_{\text{ref}}$ устаревшим уже после пары minibatch‑ей.
  * **BatchNorm** и **Dropout** добавляют внутреннюю стохастичность, которую невозможно компенсировать прошлым $g_{\text{ref}}$.
  * Дополнительный полный проход по датасету (для подсчёта $g_{\text{ref}}$) съедает потенциальную экономию итераций.
* «Стриминговые» модификации SVRG, рассчитанные на аугментацию, снижают теоретическое смещение, но также проигрывают SGD по времени и качеству.
* **Вывод**: существующие методы уменьшения дисперсии непрактичны для современных глубоких сетей; будущие решения должны учитывать стохастичность архитектуры и данных (аугментация, BatchNorm, Dropout).
:::
::::




## Adam работает хуже для CV, чем для LLM? ^[[Linear attention is (maybe) all you need (to understand transformer optimization)](https://arxiv.org/abs/2310.01082)]

:::: {.columns}
::: {.column width="40%"}
![CNNs on MNIST and CIFAR10](cnns.pdf)
:::

::: {.column width="60%"}
![Transformers on PTB, WikiText2, and SQuAD](transformers.pdf)
:::
::::

Черные линии - SGD; красные линии - Adam.

## Почему Adam работает хуже для CV, чем для LLM? ^[[Linear attention is (maybe) all you need (to understand transformer optimization)](https://arxiv.org/abs/2310.01082)]

### Потому что шум градиентов в языковых моделях имеет тяжелые хвосты?

![](histogram_full.pdf)


## Почему Adam работает хуже для CV, чем для LLM? ^[[Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models](https://arxiv.org/abs/2402.19449)]

### Нет! Метки имеют тяжелые хвосты!

:::: {.columns}
::: {.column width="50%"}
В компьютерном зрении датасеты часто сбалансированы: 1000 котиков, 1000 песелей и т.д.

В языковых датасетах почти всегда не так: слово *the* встречается часто, слово *tie* на порядки реже 
:::

::: {.column width="50%"}
![Распределение частоты токенов в PTB](PTB_classes.pdf){width=100%}
:::
::::

## Почему Adam работает хуже для CV, чем для LLM? ^[[Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models](https://arxiv.org/abs/2402.19449)]

### SGD медленно прогрессирует на редких классах

![](sgd_adam_imb.pdf){width=100% align="center"}
![](sgd_adam_imb_leg.pdf){width=100% align="center"}

SGD не добивается прогресса на низкочастотных классах, в то время как Adam добивается. Обучение GPT-2 S на WikiText-103. (a) Распределение классов, отсортированных по частоте встречаемости, разбитых на группы, соответствующие $\approx 10$ % данных. (b) Значение функции потерь при обучении. (c, d) Значение функции потерь при обучении для каждой группы при использовании SGD и Adam. 

## Визуализация с помощью проекции на прямую

* Обозначим начальную точку как $w_0$, представляющую собой веса нейронной сети при инициализации. Веса, полученные после обучения, обозначим как $\hat{w}$.

* Генерируем случайный вектор такой же размерности и нормы $w_1 \in \mathbb{R}^p$, затем вычисляем значение функции потерь вдоль этого вектора:

$$
L (\alpha) = L (w_0 + \alpha w_1), \text{ where } \alpha \in [-b, b].
$$

## Проекция функции потерь нейронной сети на прямую

![[\faPython Open in colab](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/NN_Surface_Visualization.ipynb)](../files/Line_projection_No Dropout.pdf)

## Проекция функции потерь нейронной сети на прямую {.noframenumbering}

![[\faPython Open in colab](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/NN_Surface_Visualization.ipynb)](../files/Line_projection_Dropout 0.pdf)

## Проекция функции потерь нейронной сети на плоскость

* Мы можем расширить эту идею и построить проекцию поверхности потерь на плоскость, которая задается 2 случайными векторами. 

* Два случайных гауссовых вектора в пространстве большой размерности с высокой вероятностью ортогональны. 

$$
L (\alpha, \beta) = L (w_0 + \alpha w_1 + \beta w_2), \text{ where } \alpha, \beta \in [-b, b]^2.
$$

![[\faPython Open in colab](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/NN_Surface_Visualization.ipynb)](plane_projection.jpeg){width=70%}

## Может ли быть полезно изучение таких проекций? ^[[Visualizing the Loss Landscape of Neural Nets, Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein](https://arxiv.org/abs/1712.09913)]

:::: {.columns}
::: {.column width="35%"}
![The loss surface of ResNet-56 without skip connections](noshortLog.png)
:::

::: {.column width="65%"}
![The loss surface of ResNet-56 with skip connections](shortHighResLog.png)
:::

::::

## Может ли быть полезно изучение таких проекций, если серьезно? ^[[Loss Landscape Sightseeing with Multi-Point Optimization, Ivan Skorokhodov, Mikhail Burtsev](https://arxiv.org/abs/1910.03867)]

![Examples of a loss landscape of a typical CNN model on FashionMNIST and CIFAR10 datasets found with MPO. Loss values are color-coded according to a logarithmic scale](icons-grid.png)


## Ширина локальных минимумов

![](sam_a.pdf)

## Ширина локальных минимумов{.noframenumbering}

![](sam_b.pdf)

## Ширина локальных минимумов{.noframenumbering}

![](sam_c.pdf)

## 

[![](gd_local_convergence.pdf)](https://fmin.xyz/docs/visualizations/sgd_1.mp4)

## 

[![](sgd_local_divergence.pdf)](https://fmin.xyz/docs/visualizations/sgd_2.mp4)

## Модели не сходятся к стационарным точкам, но это не страшно ^[[NN Weights Do Not Converge to Stationary Points](https://arxiv.org/pdf/2110.06256)]

:::: {.columns}
::: {.column width="33%"}
![](imagenet-baseline-val-accu.pdf)
:::

::: {.column width="33%"}
![](imagenet-baseline-train-loss-nonsmooth.pdf)
:::

::: {.column width="33%"}
![](imagenet-baseline-train-gradnorm.pdf)
:::
::::

## Grokking ^[[Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets,   Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra](https://arxiv.org/abs/2201.02177)]

:::: {.columns}
::: {.column width="50%"}
![Training transformer with 2 layers, width 128, and 4 attention heads, with a total of about $4 \cdot 10^5$ non-embedding parameters. Reproduction of experiments (\~ half an hour) is available [here](https://colab.research.google.com/drive/1r3Wg84XECq57fT2B1dvHLSJrJ2sjIDCJ?usp=sharing)](grokking.png)
:::

::: {.column width="50%"}

* Рекомендую посмотреть лекцию Дмитрия Ветрова **Удивительные свойства функции потерь в нейронной сети** (Surprising properties of loss landscape in overparameterized models). [\faYoutube \ видео](https://youtu.be/d60ShbSAu4A), [\faFile \ Презентация](https://disk.yandex.ru/i/OPtA2-8hSQRFNg)

* Автор [\faTelegram \ канала Свидетели Градиента](https://t.me/GradientWitnesses) собирает интересные наблюдения и эксперименты про гроккинг. 

* Также есть [\faYoutube \ видео](https://www.youtube.com/watch?v=pmHkDKPg0WM) с его докладом **Чем не является гроккинг**.





:::

::::

## Double Descent ^[[Reconciling modern machine learning practice and the bias-variance trade-off, Mikhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal](https://arxiv.org/abs/1812.11118)]

![](doubledescent.pdf){width=100%}

## Double Descent

[![](dd.pdf)](https://fmin.xyz/docs/visualizations/double_descent.mp4)

## Exponential learning rate

* [Exponential Learning Rate Schedules for Deep Learning](http://www.offconvex.org/2020/04/24/ExpLR1/)

# Large batch training

## Large batch training ^[[Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677)]

![](time.pdf){width=90%}

## Large batch training ^[[Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677)]

![](batchsize.pdf){width=85%}

## Large batch training ^[[Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677)]

| Effective batch size ($kn$)  | $\alpha$ | top-1 error (%)  |
|:-------:|:-----------------------:|:------------------------:|
| 256   | $0.05$                | 23.92 ± 0.10           |
| 256   | $0.10$                | 23.60 ± 0.12           |
| 256   | $0.20$                | 23.68 ± 0.09           |
| 8k    | $0.05 \cdot 32$       | 24.27 ± 0.08           |
| 8k    | $0.10 \cdot 32$       | 23.74 ± 0.09           |
| 8k    | $0.20 \cdot 32$       | 24.05 ± 0.18           |
| 8k    | $0.10$                | 41.67 ± 0.10           |
| 8k    | $0.10 \cdot \sqrt{32}$| 26.22 ± 0.03           |

Comparison of learning rate scaling rules. ResNet-50 trained on ImageNet. A reference learning rate of $\alpha=0.1$ works best for $kn=256$ (23.68% error). The linear scaling rule suggests $\alpha=0.1\cdot32$ when $kn=8$k, which again gives best performance (23.74\% error). Other ways of scaling $\alpha$ give worse results.

## Linear and square root scaling rules

When training with large batches, the learning rate must be adjusted to maintain convergence speed and stability. The **linear scaling rule**^[[Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677)] suggests multiplying the learning rate by the same factor as the increase in batch size:
$$
\alpha_{\text{new}} = \alpha_{\text{base}} \cdot \frac{\text{Batch Size}_{\text{new}}}{\text{Batch Size}_{\text{base}}}
$$
The **square root scaling rule**^[[Learning Rates as a Function of Batch Size: A Random Matrix Theory Approach to Neural Network Training](https://arxiv.org/abs/2006.09092)] proposes scaling the learning rate with the square root of the batch size increase:
$$
\alpha_{\text{new}} = \alpha_{\text{base}} \cdot \sqrt{\frac{\text{Batch Size}_{\text{new}}}{\text{Batch Size}_{\text{base}}}}
$$
Authors claimed, that it suits for adaptive optimizers like Adam, RMSProp and etc. while linear scaling rule serves well for SGD.

## Gradual warmup ^[[Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677)]

Gradual warmup helps to avoid instability when starting with large learning rates by slowly increasing the learning rate from a small value to the target value over a few epochs. This is defined as:
$$
\alpha_t = \alpha_{\text{max}} \cdot \frac{t}{T_w}
$$
where $t$ is the current iteration and $T_w$ is the warmup duration in iterations. In the original paper, authors used first 5 epochs for gradual warmup.

:::: {.columns}
::: {.column width="36%"}
![no warmup](distr-warmup-none.pdf)
:::

::: {.column width="32%"}
![constant warmup](distr-warmup-constant.pdf)
:::

::: {.column width="32%"}
![gradual warmup](distr-warmup-gradual.pdf)
:::

::::

## Gradient accumulation

Gradient accumulation allows the effective batch size to be increased without requiring larger memory by accumulating gradients over several mini-batches:

:::: {.columns}
::: {.column width="50%"}

### Without gradient accumulation

```python
for i, (inputs, targets) in enumerate(data):
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    loss.backward()

    optimizer.step()
    optimizer.zero_grad()
```
:::

. . .

::: {.column width="50%"}

### With gradient accumulation

```python
for i, (inputs, targets) in enumerate(data):
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    loss.backward()
    if (i+1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

:::

::::



# MultiGPU training

## Data Parallel training

1. Parameter server sends the full copy of the model to each device
2. Each device makes forward and backward passes
3. Parameter server gathers gradients
4. Parameter server updates the model

. . .

Per device batch size: $b$. Overall batchsize: $Db$. Data parallelism involves splitting the data across multiple GPUs, each with a copy of the model. Gradients are averaged and weights updated synchronously:

![Scheme of Data Parallel training](DP.pdf){width=80%}

## Distributed Data Parallel training

Distributed Data Parallel (DDP) ^[[Getting Started with Distributed Data Parallel](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)] extends data parallelism across multiple nodes. Each node computes gradients locally, then synchronizes with others. Below one can find differences from the PyTorch [site](https://pytorch.org/tutorials/beginner/ddp_series_theory.html). This is used by default in [ \faPython Accelerate library](https://huggingface.co/docs/transformers/accelerate).

|    DataParallel   | DistributedDataParallel  |
|:----------------:|:----------------:|
| More overhead; model is replicated and destroyed at each forward pass | Model is replicated only once                                    |
| Only supports single-node parallelism                            | Supports scaling to multiple machines                            |
| Slower; uses multithreading on a single process and runs into Global Interpreter Lock (GIL) contention | Faster (no GIL contention) because it uses multiprocessing |


## Naive model parallelism 

Model parallelism divides the model across multiple GPUs. Each GPU handles a subset of the model layers, reducing memory load per GPU. Allows to work with the models, that won’t fit in the single GPU
Poor resource utilization. 

![Model parallelism](MP.png)

## Pipeline model parallelism (GPipe) ^[[GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](https://arxiv.org/abs/1811.06965)]

GPipe splits the model into stages, each processed sequentially. Micro-batches are passed through the pipeline, allowing for overlapping computation and communication:
![](gpipe.png)


## Pipeline model parallelism (PipeDream) ^[[PipeDream: Generalized Pipeline Parallelism for DNN Training](https://arxiv.org/abs/1806.03377)]

PipeDream uses asynchronous pipeline parallelism, balancing forward and backward passes across the pipeline stages to maximize utilization and reduce idle time:
![](pipedream.png)

## ZeRO ^[[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)]

![](zero.png)

## LoRA ^[[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)]

:::: {.columns}
::: {.column width="50%"}
![](lora.pdf)
:::

::: {.column width="50%"}

LoRA reduces the number of parameters by approximating weight matrices with low-rank factorization:
$$
W_{\text{new}} = W + \Delta W
$$
where $\Delta W = A B^T$, with $A$ and $B$ being low-rank matrices. This reduces computational and memory overhead while maintaining model performance.

* $A$ is initialized as usual, while $B$ is initialized with zeroes in order to start from identity mapping
* $r$ is typically selected between 2 and 64
* Usually applied to attention modules

. . .

$$
h = W_{\text{new}}x = Wx + \Delta Wx = Wx + AB^T x
$$
:::

::::

## Исользование представления Кашина для квантизации весов LLM^[[Quantization of Large Language Models with an Overdetermined Basis](https://arxiv.org/abs/2404.09737)]

![Схема алгоритма, позволяющего квантизировать веса нейросети с помощью матричного разложения.](kquant_scheme.pdf){#fig-kquant_scheme}

# Тренды

## 

![Динамика вычислений, необходимых для обучения моделей. [Источник](https://epoch.ai/data/notable-ai-models)](compute_trends_global.pdf)

## 

![Динамика вычислений, необходимых для обучения нейросетевых моделей. [Источник](https://epoch.ai/data/notable-ai-models)](compute_trends_local.pdf){width=95%}

## 

![Динамика количества обучаемых параметров нейросетевых моделей. [Источник](https://epoch.ai/data/notable-ai-models)](num_param_trends.pdf){width=95%}


