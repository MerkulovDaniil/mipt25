---
title: Вспоминаем линейную алгебру.
author: Даниил Меркулов
institute: Методы оптимизации. МФТИ
format: 
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back1.jpeg}
---

# Вспоминаем линейную алгебру

## Векторы и матрицы

Мы будем считать, что все векторы являются столбцами по умолчанию. Пространство векторов длины $n$ обозначается $\mathbb{R}^n$, а пространство матриц размера $m \times n$ с вещественными элементами обозначается $\mathbb{R}^{m \times n}$. То есть [^1]:

[^1]: Подробный вводный курс по прикладной линейной алгебре можно найти в книге [Introduction to Applied Linear Algebra -- Vectors, Matrices, and Least Squares](https://web.stanford.edu/~boyd/vmls/) - книга от Stephen Boyd & Lieven Vandenberghe, которая указана в источнике. Также полезен материал по линейной алгебре приведенный в приложении А книги Numerical Optimization by Jorge Nocedal Stephen J. Wright.

$$
x = \begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix} \quad x^T = \begin{bmatrix}
x_1 & x_2 & \dots & x_n
\end{bmatrix} \quad x \in \mathbb{R}^n, x_i \in \mathbb{R}
$$ {#eq-vector}

. . .

Аналогично, если $A \in \mathbb{R}^{m \times n}$ мы обозначаем транспонирование как $A^T \in \mathbb{R}^{n \times m}$:
$$
A = \begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n} \\
a_{21} & a_{22} & \dots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \dots & a_{mn}
\end{bmatrix} \quad A^T = \begin{bmatrix}
a_{11} & a_{21} & \dots & a_{m1} \\
a_{12} & a_{22} & \dots & a_{m2} \\
\vdots & \vdots & \ddots & \vdots \\
a_{1n} & a_{2n} & \dots & a_{mn}
\end{bmatrix} \quad A \in \mathbb{R}^{m \times n}, a_{ij} \in \mathbb{R}
$$
Мы будем писать $x \geq 0$ и $x \neq 0$ для обозначения покомпонентных неравенств

---

![Эквивалентные представления вектора](vector.pdf){#fig-vector}

---

Матрица $A$ называется симметричной, если $A = A^T$. Обозначается как $A \in \mathbb{S}^n$ (множество квадратных симметричных матриц размерности $n$). Заметим, что только квадратная матрица может быть симметричной по определению.

. . .

Матрица $A \in \mathbb{S}^n$ называется **положительно (отрицательно) определенной**, если для всех $x \neq 0 : x^T Ax > (<) 0$. Обозначается как $A \succ (\prec) 0$. Множество таких матриц обозначается как $\mathbb{S}^n_{++} (\mathbb{S}^n_{- -})$

. . .

Матрица $A \in \mathbb{S}^n$ называется **положительно (отрицательно) полуопределенной**, если для всех $x : x^T Ax \geq (\leq) 0$. Обозначается как $A \succeq (\preceq) 0$. Множество таких матриц обозначается как $\mathbb{S}^n_{+} (\mathbb{S}^n_{-})$

:::{.callout-question}
Верно ли, что положительно определенная матрица имеет все положительные элементы?
:::

. . .

:::{.callout-question}
Верно ли, что если матрица симметрична, то она должна быть положительно определенной?
:::

. . .

:::{.callout-question}
Верно ли, что если матрица положительно определена, то она должна быть симметричной?
:::


---

## Матричное умножение (matmul)

Пусть $A$ - матрица размера $m \times n$, а $B$ - матрица размера $n \times p$, тогда их произведение $AB$ равно:
$$
C = AB
$$
Тогда $C$ - матрица размера $m \times p$, элемент $(i, j)$ которой равен:
$$
c_{ij} = \sum_{k=1}^n a_{ik}b_{kj}.
$$

Эта операция в наивной форме требует $\mathcal{O}(n^3)$ арифметических операций, где $n$ обычно считается наибольшей размерностью матриц.

. . .

:::{.callout-question}
Возможно ли умножить две матрицы быстрее, чем за $\mathcal{O}(n^3)$? Как насчет $\mathcal{O}(n^2)$, $\mathcal{O}(n)$?
:::

---

## Умножение матрицы на вектор (matvec)

Пусть $A$ - матрица размера $m \times n$, а $x$ - вектор длины $n$, тогда $i$-й элемент произведения $Ax$ равен:
$$
z = Ax
$$
равен:
$$
z_i = \sum_{k=1}^n a_{ik}x_k
$$

Эта операция в наивной форме требует $\mathcal{O}(n^2)$ арифметических операций, где $n$ обычно считается наибольшей размерностью входов.

Отметим, что:

* $C = AB \quad C^T = B^T A^T$
* $AB \neq BA$
* $e^{A} =\sum\limits_{k=0}^{\infty }{1 \over k!}A^{k}$
* $e^{A+B} \neq e^{A} e^{B}$ (но если $A$ и $B$ коммутируют, то есть $AB = BA$, то $e^{A+B} = e^{A} e^{B}$)
* $\langle x, Ay\rangle = \langle A^T x, y\rangle$

## Пример. Простая, но важная идея о матричных вычислениях.

Предположим, у вас есть следующее выражение

$$
b = A_1 A_2 A_3 x,
$$

где $A_1, A_2, A_3 \in \mathbb{R}^{3 \times 3}$ - случайные квадратные плотные матрицы, и $x \in \mathbb{R}^n$ - вектор. Вам нужно вычислить $b$.

Какой способ лучше всего использовать?

1. $A_1 A_2 A_3 x$ (слева направо)
2. $\left(A_1 \left(A_2 \left(A_3 x\right)\right)\right)$ (справа налево)
3. Не имеет значения
4. Результаты первых двух вариантов не будут одинаковыми.

Проверьте простой [\faPython\ код](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/stupid_important_idea_on_mm.ipynb) после вашего интуитивного ответа.

## Нормы

Норма - это **количественная мера малости вектора** и обычно обозначается как $\Vert x \Vert$.

Норма должна удовлетворять определенным свойствам:

1.  $\Vert \alpha x \Vert = \vert \alpha\vert \Vert x \Vert$, $\alpha \in \mathbb{R}$
2.  $\Vert x + y \Vert \leq \Vert x \Vert + \Vert y \Vert$ (неравенство треугольника)
3.  Если $\Vert x \Vert = 0$, то $x = 0$

. . .

Расстояние между двумя векторами определяется как
$$ 
d(x, y) = \Vert x - y \Vert. 
$$
Наиболее широко используемой нормой является **Евклидова норма**:
$$
\Vert x \Vert_2 = \sqrt{\sum_{i=1}^n |x_i|^2},
$$
которая соответствует расстоянию в нашей реальной жизни. Если векторы имеют комплексные элементы, мы используем их модуль. Евклидова норма, или $2$-норма, является подклассом важного класса $p$-норм:

$$
\Vert x \Vert_p = \Big(\sum_{i=1}^n |x_i|^p\Big)^{1/p}. 
$$

---

## $p$-норма вектора

Существуют два очень важных частных случая. Бесконечность-норма, или норма Чебышева, определяется как максимальное абсолютное значение элемента вектора:
$$
\Vert x \Vert_{\infty} = \max_i | x_i| 
$$

. . .

$l_1$ норма (или **манхэттенское расстояние**) определяется как сумма модулей элементов вектора $x$:

$$
\Vert x \Vert_1 = \sum_i |x_i| 
$$

. . .

$l_1$ норма играет очень важную роль: она все связана с методами **compressed sensing**, которые появились в середине 00-х как одна из популярных тем исследований. Код для изображения ниже доступен [*здесь:*](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Balls_p_norm.ipynb). Также посмотрите [*это*](https://fmin.xyz/docs/theory/balls_norm.mp4) видео.

![Шары в разных нормах на плоскости](p_balls.pdf)

## Матричные нормы

В некотором смысле между матрицами и векторами нет большой разницы (вы можете векторизовать матрицу), и здесь появляется самая простая матричная норма **Фробениуса**:
$$
\Vert A \Vert_F = \left(\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2\right)^{1/2}
$$

. . .

Спектральная норма, $\Vert A \Vert_2$ является одной из наиболее широко используемых матричных норм (наряду с нормой Фробениуса).

$$
\Vert A \Vert_2 = \sup_{x \ne 0} \frac{\Vert A x \Vert_2}{\Vert x \Vert_{2}},
$$

Она не может быть вычислена непосредственно из элементов с помощью простой формулы, как в случае нормы Фробениуса, однако, существуют эффективные алгоритмы для ее вычисления. Она напрямую связана с **сингулярным разложением** (SVD) матрицы. Для неё справедливо:

$$
\Vert A \Vert_2 = \sigma_1(A) = \sqrt{\lambda_{\max}(A^TA)}
$$

где $\sigma_1(A)$ - наибольшее сингулярное значение матрицы $A$.

## Скалярное произведение

Стандартное **скалярное произведение** между векторами $x$ и $y$ из $\mathbb{R}^n$ равно:
$$
\langle x, y \rangle = x^T y = \sum\limits_{i=1}^n x_i y_i = y^T x =  \langle y, x \rangle
$$

Здесь $x_i$ и $y_i$ - $i$-ые компоненты соответствующих векторов.

::: {.callout-example}
Докажите, что вы можете переставить матрицу внутри скалярного произведения с транспонированием: $\langle x, Ay\rangle = \langle A^Tx, y\rangle$ и $\langle x, yB\rangle = \langle xB^T, y\rangle$
:::

## Скалярное произведение матриц

Стандартное **скалярное произведение** между матрицами $X$ и $Y$ из $\mathbb{R}^{m \times n}$ равно:

$$
\langle X, Y \rangle = \text{tr}(X^T Y) = \sum\limits_{i=1}^m\sum\limits_{j=1}^n X_{ij} Y_{ij} =  \text{tr}(Y^T X) =  \langle Y, X \rangle
$$

::: {.callout-question} 
Существует ли связь между нормой Фробениуса $\Vert \cdot \Vert_F$ и скалярным произведением между матрицами $\langle \cdot, \cdot \rangle$?
:::


## Собственные вектора и собственные значения

Число $\lambda$ является собственным значением квадратной матрицы $A$ размера $n \times n$, если существует ненулевой вектор $q$ такой, что
$$ 
Aq = \lambda q. 
$$

Вектор $q$ называется собственным вектором матрицы $A$. Матрица $A$ невырожденная, если ни одно из её собственных значений не равно нулю. Собственные значения симметричных матриц являются вещественными числами, в то время как несимметричные матрицы могут иметь комплексные собственные значения. Если матрица положительно определена и симметрична, то все её собственные значения являются положительными вещественными числами.

## Собственные вектора и собственные значения

:::{.callout-theorem}
$$
A \succeq (\succ) 0 \Leftrightarrow \text{все собственные значения } A \text{ } \geq (>) 0 
$$

:::{.callout-proof collapse="true"}
1. $\rightarrow$ Предположим, что некоторое собственное значение $\lambda$ отрицательно, и пусть $x$ обозначает соответствующий собственный вектор. Тогда
$$
Ax = \lambda x \rightarrow x^T Ax = \lambda x^T x < 0
$$
что противоречит условию $A \succeq 0$.
2. $\leftarrow$ Для любой симметричной матрицы мы можем выбрать набор собственных векторов $v_1, \dots, v_n$, которые образуют ортонормированный базис в $\mathbb{R}^n$. Возьмем любой вектор $x \in \mathbb{R}^n$.
$$
\begin{split}
x^T A x &= (\alpha_1 v_1 + \ldots + \alpha_n v_n)^T A (\alpha_1 v_1 + \ldots + \alpha_n v_n)\\
&= \sum \alpha_i^2 v_i^T A v_i = \sum \alpha_i^2 \lambda_i v_i^T v_i \geq 0
\end{split}
$$
Здесь мы использовали тот факт, что $v_i^T v_j = 0$, для $i \neq j$.
:::
:::

## Спектральное разложение (eigendecomposition)

Пусть $A \in S_n$, т.е. $A$ - вещественная симметричная матрица размера $n \times n$. Тогда $A$ может быть разложена как

$$ 
A = Q\Lambda Q^T,
$$

. . .

где $Q \in \mathbb{R}^{n \times n}$ ортогональная, т.е. удовлетворяет $Q^T Q = I$, и $\Lambda = \text{diag}(\lambda_1, \ldots , \lambda_n)$. Вещественные числа $\lambda_i$ являются собственными значениями $A$ и являются корнями характеристического полинома $\text{det}(A - \lambda I)$. Столбцы $Q$ образуют ортонормированный набор собственных векторов $A$. Такое разложение называется спектральным. [^2]

[^2]: Хорошая шпаргалка с разложением матриц доступна на сайте курса по линейной алгебре [website](https://nla.skoltech.ru/_files/decompositions.pdf).

. . .

Мы обычно упорядочиваем вещественные собственные значения как $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n$. Мы используем обозначение $\lambda_i(A)$ для обозначения $i$-го наибольшего собственного значения $A \in S$. Мы обычно пишем наибольшее или максимальное собственное значение как $\lambda_1(A) = \lambda_{\text{max}}(A)$, и наименьшее или минимальное собственное значение как $\lambda_n(A) = \lambda_{\text{min}}(A)$.

## Собственные значения

Наибольшее и наименьшее вещественныесобственные значения удовлетворяют

$$
\lambda_{\text{min}} (A) = \inf_{x \neq 0} \dfrac{x^T Ax}{x^T x}, \qquad \lambda_{\text{max}} (A) = \sup_{x \neq 0} \dfrac{x^T Ax}{x^T x}
$$

. . .

и, следовательно, $\forall x \in \mathbb{R}^n$ (соотношение Рэлея):

$$
\lambda_{\text{min}} (A) x^T x \leq x^T Ax \leq \lambda_{\text{max}} (A) x^T x
$$

. . .

**Число обусловленности** невырожденной матрицы определяется как

$$
\kappa(A) = \|A\|\|A^{-1}\|
$$

. . .

Если мы используем спектральную матричную норму, мы можем получить:

$$
\kappa(A) = \dfrac{\sigma_{\text{max}}(A)}{\sigma _{\text{min}}(A)}
$$

Если, кроме того, $A \in \mathbb{S}^n_{++}$: $\kappa(A) = \dfrac{\lambda_{\text{max}}(A)}{\lambda_{\text{min}}(A)}$

## Число обусловленности

![](conditions.pdf)

## Сингулярное разложение (SVD)

Пусть $A \in \mathbb{R}^{m \times n}$ с рангом $A = r$. Тогда $A$ может быть разложена как

$$
A = U \Sigma V^T 
$$

. . .

где $U \in \mathbb{R}^{m \times r}$ удовлетворяет $U^T U = I$, $V \in \mathbb{R}^{n \times r}$ удовлетворяет $V^T V = I$, и $\Sigma$ является диагональной матрицей с $\Sigma = \text{diag}(\sigma_1, ..., \sigma_r)$, такой что

. . .

$$
\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r > 0. 
$$

. . .

Это разложение называется **сингулярным разложением (SVD)** матрицы $A$. Столбцы $U$ называются левыми сингулярными векторами $A$, столбцы $V$ называются правыми сингулярными векторами, и числа $\sigma_i$ являются сингулярными значениями. Сингулярное разложение может быть записано как

$$
A = \sum_{i=1}^{r} \sigma_i u_i v_i^T,
$$

где $u_i \in \mathbb{R}^m$ являются левыми сингулярными векторами, и $v_i \in \mathbb{R}^n$ являются правыми сингулярными векторами.

## Сингулярное разложение

::: {.callout-question}
Пусть $A \in \mathbb{S}^n_{++}$. Что мы можем сказать о связи между его собственными значениями и сингулярными значениями?
:::

. . .

::: {.callout-question}
Как сингулярные значения матрицы связаны с её собственными значениями, особенно для симметричной матрицы?
:::

## Пример. Связь между Фробениусовой нормой и сингулярными значениями.

Пусть $A \in \mathbb{R}^{m \times n}$, и пусть $q := \min\{m, n\}$. Докажите, что
$$
\|A\|_F^2 = \sum_{i=1}^{q} \sigma_i^2(A) ,
$$

где $\sigma_1(A) \geq \ldots \geq \sigma_q(A) \geq 0$ - сингулярные значения матрицы $A$. Подсказка: используйте связь между Фробениусовой нормой и скалярным произведением и SVD. 

## Ранговое разложение (Skeleton decomposition)

:::: {.columns}

::: {.column width="70%"}
Простое, но очень интересное разложение - это ранговое разложение, которое может быть записано в двух формах:

$$
A = U V^T \quad A = \hat{C}\hat{A}^{-1}\hat{R}
$$

. . .

Последнее выражение относится к забавному факту: вы можете случайным образом выбрать $r$ линейно независимых столбцов матрицы и любые $r$ линейно независимых строк матрицы и хранить только их с возможностью точно (!) восстановить всю матрицу.

. . .

Применения для рангового разложения:

* Сжатие модели, сжатие данных и ускорение вычислений в численном анализе: для матрицы ранга $r$ с $r \ll n, m$ необходимо хранить $\mathcal{O}((n + m)r) \ll nm$ элементов.
* Извлечение признаков в машинном обучении
* Все приложения, где применяется SVD, так как ранговое разложение может быть преобразовано в форму усеченного SVD.
:::

::: {.column width="30%"}
![Иллюстрация рангового разложения](skeleton.pdf){#fig-skeleton}
:::

::::

## Каноническое тензорное разложение

Можно рассмотреть обобщение рангового разложения на структуры данных более высокого порядка, такие как тензоры, что означает представление тензора в виде суммы $r$ простых тензоров.

![Иллюстрация канонического тензорного разложения](cp.pdf){width=40%}

::: {.callout-example} 
Заметьте, что существует множество тензорных разложений: каноническое, Таккера, тензорный поезд (TT), тензорное кольцо (TR) и другие. В случае тензоров мы не имеем прямого определения *ранга* для всех типов разложений. Например, для разложения Тензорного поезда ранг является не скаляром, а вектором.
:::

## Определитель и след матрицы

Определитель и след матрицы могут быть выражены через собственные значения
$$
\text{det} A = \prod\limits_{i=1}^n \lambda_i, \qquad \text{tr} A = \sum\limits_{i=1}^n \lambda_i
$$
Определитель имеет несколько интересныхсвойств. Например,  

* $\text{det} A = 0$ тогда и только тогда, когда $A$ является вырожденной; 
* $\text{det}  AB = (\text{det} A)(\text{det}  B)$; 
* $\text{det}  A^{-1} = \frac{1}{\text{det} \ A}$.

. . .

Не забывайте о циклическом свойстве следа для произвольных матриц $A, B, C, D$ (предполагая, что все размерности согласованы):

$$
\text{tr} (ABCD) = \text{tr} (DABC) = \text{tr} (CDAB) = \text{tr} (BCDA)
$$

. . .

::: {.callout-question} 
Как определитель матрицы связан с её обратимостью?
:::

## Задача. Знайте свое скалярное произведение.

Упростите следующее выражение:

$$
\sum\limits_{i=1}^n \langle S^{-1} a_i, a_i \rangle,
$$
где $S = \sum\limits_{i=1}^n a_ia_i^T, a_i \in \mathbb{R}^n, \det(S) \neq 0$

## Пример. LoRA: Low-Rank Adaptation of Large Language Models ([arXiv:2106.09685](https://arxiv.org/pdf/2106.09685))

Поскольку современные LLM слишком большие, чтобы вместиться в память среднего пользователя, мы используем некоторые трюки, чтобы сделать их потребление памяти меньше. Одним из наиболее популярных трюков является LoRA (Low-Rank Adaptation of Large Language Models).

:::: {.columns}
::: {.column width="55%"}
Предположим, у нас есть матрица $W \in \mathbb{R}^{d \times k}$ и мы хотим выполнить следующее обновление:
$$
W = W_0 + \Delta W.
$$
Основная идея LoRA состоит в том, чтобы разложить обновление $\Delta W$ на две низкоранговые матрицы:

\begin{gather*}
W = W_0 + \Delta W = W_0 + BA, \quad B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}, \\
rank(A) = rank(B) = r \ll \min\{d, k\}.
\end{gather*}

Проверьте [\faPython\ ноутбук](https://colab.research.google.com/github/MerkulovDaniil/hse25/blob/main/notebooks/s1_lora_trump.ipynb) для примера реализации LoRA.

:::

::: {.column width="45%"}
![Иллюстрация LoRA](lora.png){width=100%}
:::
::::

# Матрично-векторное дифференцирование

## Градиент

:::: {.columns}

::: {.column width="60%"}

Пусть $f(x):\mathbb{R}^n\to\mathbb{R}$, тогда вектор, который содержит все первые частные производные:

$$
\nabla f(x) = \dfrac{df}{dx} = \begin{pmatrix}
    \frac{\partial f}{\partial x_1} \\
    \frac{\partial f}{\partial x_2} \\
 \vdots \\
    \frac{\partial f}{\partial x_n}
\end{pmatrix}
$$

. . .


называется градиентом функции $f(x)$. Этот вектор указывает направление наискорейшего возрастания. Таким образом, вектор $-\nabla f(x)$ указывает направление наискорейшего убывания функции в точке. Кроме того, вектор градиента всегда ортогонален линии уровня в точке.

:::

::: {.column width="40%"}

::: {.callout-example}
Для функции $f(x, y) = x^2 + y^2$ градиент равен: 
$$
\nabla f(x, y) =
\begin{bmatrix}
2x \\
2y \\
\end{bmatrix}
$$
Он указывает направление наискорейшего возрастания функции.
:::

::: {.callout-question} 
Как связана норма градиента с крутизной функции?
:::
:::

::::


## Гессиан

:::: {.columns}

::: {.column width="60%"}

Пусть $f(x):\mathbb{R}^n\to\mathbb{R}$, тогда матрица, содержащая все вторые частные производные:

$$
f''(x) = \nabla^2 f(x) = \dfrac{\partial^2 f}{\partial x_i \partial x_j} = \begin{pmatrix}
    \frac{\partial^2 f}{\partial x_1 \partial x_1} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \dots  & \frac{\partial^2 f}{\partial x_1\partial x_n} \\
    \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2 \partial x_2} & \dots  & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
 \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \dots  & \frac{\partial^2 f}{\partial x_n \partial x_n}
\end{pmatrix}
$$

. . .


Гессиан может быть тензором: $\left(f(x): \mathbb{R}^n \to \mathbb{R}^m \right)$ Таким образом, это просто трехмерный тензор, каждый срез которого это  гессиан соответствующей скалярной функции $\left( \nabla^2f_1(x), \ldots, \nabla^2f_m(x)\right)$.

:::

::: {.column width="40%"}


::: {.callout-example} 
Для функции $f(x, y) = x^2 + y^2$ гессиан равен:

$$
H_f(x, y) = \begin{bmatrix} 2 & 0 \\
0 & 2 \\
\end{bmatrix}
$$
:::

Эта матрица содержит информацию о кривизне функции в разных направлениях.

::: {.callout-question} 
Как можно использовать гессиан для определения выпуклости или вогнутости функции?
:::
:::
::::


## Теорема Шварца

:::: {.columns}

::: {.column width="50%"}

Пусть $f: \mathbb{R}^n \rightarrow \mathbb{R}$ - функция. Если смешанные частные производные $\frac{\partial^2 f}{\partial x_i \partial x_j}$ и $\frac{\partial^2 f}{\partial x_j \partial x_i}$ непрерывны на открытом множестве, содержащем точку $a$, то они равны в точке $a$. То есть,
$$
\frac{\partial^2 f}{\partial x_i \partial x_j} (a) = \frac{\partial^2 f}{\partial x_j \partial x_i} (a)
$$

. . .


Согласно данной теореме, если смешанные частные производные непрерывны на открытом множестве, то гессиан симметричен. То есть,

$$
\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i} \quad \nabla^2 f(x)  =(\nabla^2 f(x))^T
$$

Эта симметричность упрощает вычисления и анализ, связанные с гессианом в различных приложениях, особенно в оптимизации.

:::

::: {.column width="50%"}
::: {.callout-example}

## [Контрпример Шварца](https://fmin.xyz/docs/theory/Matrix_calculus.html#hessian)

$$
f(x,y) = 
\begin{cases}
    \frac{xy\left(x^2 - y^2\right)}{x^2 + y^2} & \text{ для } (x,\, y) \ne (0,\, 0),\\
    0 & \text{ для } (x, y) = (0, 0).
\end{cases}
$$
![](schwartz.pdf)
Можно проверить, что $\frac{\partial^2 f}{ \partial x \partial y} (0, 0) \neq \frac{\partial^2 f}{ \partial y \partial x} (0, 0)$, хотя смешанные частные производные существуют, и в каждой другой точке симметричность выполняется.
:::
:::

::::

## Якобиан


:::: {.columns}

::: {.column width="50%"}

Обобщением понятия градиента на случай многомерной функции $f(x):\mathbb{R}^n\to\mathbb{R}^m$ является следующая матрица:

$$
J_f = f'(x) = \dfrac{df}{dx^T} = \begin{pmatrix}
    \frac{\partial f_1}{\partial x_1} & \frac{\partial f_2}{\partial x_1} & \dots  & \frac{\partial f_m}{\partial x_1} \\
    \frac{\partial f_1}{\partial x_2} & \frac{\partial f_2}{\partial x_2} & \dots  & \frac{\partial f_m}{\partial x_2} \\
 \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial f_1}{\partial x_n} & \frac{\partial f_2}{\partial x_n} & \dots  & \frac{\partial f_m}{\partial x_n}
\end{pmatrix}
$$


Она содержит информацию о скорости изменения функции по отношению к ее входу.

::: {.callout-question} 
Можно ли связать эти три определения выше (градиент, якобиан, и гессиан) с помощью одного утверждения?
:::

:::

::: {.column width="50%"}

::: {.callout-example}
Для функции  
$$
f(x, y) = \begin{bmatrix}
x + y \\
x - y \\
\end{bmatrix}, 
$$
Якобиан равен: 
$$
J_f(x, y) = \begin{bmatrix}
1 & 1 \\
1 & -1 \\
\end{bmatrix}
$$ 
:::

::: {.callout-question} 
Как матрица Якоби связана с градиентом для скалярных функций?
:::


:::

::::


## Итог

$$
f(x) : X \to Y; \qquad \frac{\partial f(x)}{\partial x} \in G
$$

|             X             |       Y        |             G             |                      Name                       |
|:----------------:|:----------------:|:----------------:|:-----------------:|
|       $\mathbb{R}$        |  $\mathbb{R}$  |       $\mathbb{R}$        |              $f'(x)$ (производная)               |
|      $\mathbb{R}^n$       |  $\mathbb{R}$  |      $\mathbb{R}^n$       |  $\dfrac{\partial f}{\partial x_i}$ (градиент)  |
|      $\mathbb{R}^n$       | $\mathbb{R}^m$ | $\mathbb{R}^{n \times m}$ | $\dfrac{\partial f_i}{\partial x_j}$ (якобиан) |
| $\mathbb{R}^{m \times n}$ |  $\mathbb{R}$  | $\mathbb{R}^{m \times n}$ |      $\dfrac{\partial f}{\partial x_{ij}}$      |

## Аппроксимация Тейлора первого порядка

:::: {.columns}

::: {.column width="70%"}
Аппроксимация Тейлора первого порядка, также известная как линейное приближение, строится вблизи некоторой точки $x_0$. Если $f: \mathbb{R}^n \rightarrow \mathbb{R}$ - дифференцируемая функция, то ее аппроксимация первого порядка задается следующим образом:

$$
f_{x_0}^I(x) = f(x_0) + \nabla f(x_0)^T (x - x_0)
$$

где: 

* $f(x_0)$ - значение функции в точке $x_0$.
* $\nabla f(x_0)$ - градиент функции в точке $x_0$.

. . .


Часто для упрощения теоретического анализа в некоторых методах заменяют функцию вблизи некоторой точки на её аппроксимацию
:::

::: {.column width="30%"}
![Аппроксимация Тейлора первого порядка в окрестности точки $x_0$](first_order_taylor.pdf)
:::

::::

## Аппроксимация Тейлора второго порядка

:::: {.columns}

::: {.column width="70%"}
Аппроксимация Тейлора второго порядка, также известная как квадратичное приближение, использует информацию о кривизне функции. Для дважды дифференцируемой функции $f: \mathbb{R}^n \rightarrow \mathbb{R}$, ее аппроксимация второго порядка, строящаяся вблизи некоторой точки $x_0$, задается следующим образом:

$$
f_{x_0}^{II}(x) = f(x_0) + \nabla f(x_0)^T (x - x_0) + \frac{1}{2} (x - x_0)^T \nabla^2 f(x_0) (x - x_0)
$$

Где $\nabla^2 f(x_0)$ - гессиан функции $f$ в точке $x_0$.

. . .

Когда линейного приближения функции не достаточно, можно рассмотреть замену $f(x)$ на $f_{x_0}^{II}(x)$ в окрестности точки $x_0$. В общем, приближения Тейлора дают нам способ локально аппроксимировать функции. Аппроксимация первого порядка определяется градиентом функции в точке, т.е. нормалью к касательной гиперплоскости. А аппроксимация второго порядка представляет из себя параболу. Эти приближения особенно полезны в оптимизации и численных методах, потому что они предоставляют простой способ работы со сложными функциями.
:::

::: {.column width="30%"}
![Аппроксимация Тейлора второго порядка в окрестности точки $x_0$](second_order_taylor.pdf)
:::

::::

## Дифференциалы

::: {.callout-theorem}
Пусть $x \in S$ - внутренняя точка множества $S$, и пусть $D : U \rightarrow V$ - линейный оператор. Мы говорим, что функция $f$ дифференцируема в точке $x$ с производной $D$, если для всех достаточно малых $h \in U$ выполняется следующее разложение: 
$$ 
f(x + h) = f(x) + D[h] + o(\|h\|)
$$
Если для любого линейного оператора $D : U \rightarrow V$ функция $f$ не дифференцируема в точке $x$ с производной $D$, то мы говорим, что $f$ не дифференцируема в точке $x$.
:::

## Дифференциалы

После получения дифференциальной записи $df$ мы можем получить градиент, используя следующую формулу:

$$
df(x) = \langle \nabla f(x), dx\rangle
$$

. . .

Далее, если у нас есть дифференциал в такой форме и мы хотим вычислить вторую производную матричной/векторной функции, мы рассматриваем "старый" $dx$ как константу $dx_1$, затем вычисляем $d(df) = d^2f(x)$

$$
d^2f(x) = \langle \nabla^2 f(x) dx_1, dx\rangle = \langle H_f(x) dx_1, dx\rangle
$$

## Свойства дифференциалов

Пусть $A$ и $B$ - постоянные матрицы, а $X$ и $Y$ - переменные (или матричные функции).

:::: {.columns}

::: {.column width="50%"}

- $dA = 0$
- $d(\alpha X) = \alpha (dX)$
- $d(AXB) = A(dX )B$
- $d(X+Y) = dX + dY$
- $d(X^T) = (dX)^T$
- $d(XY) = (dX)Y + X(dY)$
- $d\langle X, Y\rangle = \langle dX, Y\rangle+ \langle X, dY\rangle$

:::

::: {.column width="50%"}

- $d\left( \dfrac{X}{\phi}\right) = \dfrac{\phi dX - (d\phi) X}{\phi^2}$
- $d\left( \det X \right) = \det X \langle X^{-T}, dX \rangle$
- $d\left(\text{tr } X \right) = \langle I, dX\rangle$
- $df(g(x)) = \dfrac{df}{dg} \cdot dg(x)$
- $H = (J(\nabla f))^T$
- $d(X^{-1})=-X^{-1}(dX)X^{-1}$

:::

::::

## Матричное дифференцирование. Пример 1 {.t}

::: {.callout-example}
Найти $df, \nabla f(x)$, если $f(x) = \langle x, Ax\rangle -b^T x + c$. 
:::

## Матричное дифференцирование. Пример 2

::: {.callout-example}
Найти $df, \nabla f(x)$, если $f(x) = \ln \langle x, Ax\rangle$. 
:::

. . .

1. Заметим, что $A$ должна быть положительно определенной, потому что $\langle x, Ax\rangle$ аргумент логарифма и для любого $x$ формула должна быть положительной. Таким образом, $A \in \mathbb{S}^n_{++}$ Давайте сначала найдем дифференциал: 
$$
\begin{split}
 df &= d \left( \ln \langle x, Ax\rangle \right) = \dfrac{d \left( \langle x, Ax\rangle \right)}{ \langle x, Ax\rangle} = \dfrac{\langle dx, Ax\rangle +  \langle x, d(Ax)\rangle}{ \langle x, Ax\rangle} = \\
 &= \dfrac{\langle Ax, dx\rangle + \langle x, Adx\rangle}{ \langle x, Ax\rangle} = \dfrac{\langle Ax, dx\rangle + \langle A^T x, dx\rangle}{ \langle x, Ax\rangle} = \dfrac{\langle (A + A^T) x, dx\rangle}{ \langle x, Ax\rangle} 
\end{split}
$$
2. Наша основная цель - получить форму $df = \langle \cdot, dx\rangle$
$$
df = \left\langle  \dfrac{2 A x}{ \langle x, Ax\rangle} , dx\right\rangle
$$
Таким образом, градиент равен $\nabla f(x) = \dfrac{2 A x}{ \langle x, Ax\rangle}$

## Матричное дифференцирование. Пример 3 {.t}

::: {.callout-example}
Найти $df, \nabla f(X)$, если $f(X) = \langle S, X\rangle - \log \det X$. 
:::
