---
title: Автоматическое дифференцирование
author: Даниил Меркулов
institute: Методы оптимизации. МФТИ
format: 
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back2.jpeg}
---

# Повторим матричное дифференцирование

## Пример 1

::: {.callout-example}
Найти гессиан $\nabla^2 f(x)$, если $f(x) = \langle x, Ax\rangle -b^T x + c$. 
:::

. . .

:::: {.columns}
::: {.column width="50%"}
1. Распишем дифференциал $df$
    $$
    \begin{split}
    df &= d\left(\langle Ax, x\rangle - \langle b, x\rangle + c\right) \\
    &= \langle Ax, dx\rangle + \langle x, Adx\rangle - \langle b, dx\rangle \\
    &= \langle Ax, dx\rangle + \langle A^Tx, dx\rangle - \langle b, dx\rangle \\
    &= \langle (A+A^T)x - b, dx\rangle  \\
    \end{split}
    $$

    Что означает, что градиент $\nabla f = (A+A^T)x - b$.
:::

. . .

::: {.column width="50%"}
2. Найдем второй дифференциал $d^2f = d(df)$, полагая, что $dx=dx_1 = \text{const}$:
    $$
    \begin{split}
    d^2f &= d\left(\langle (A+A^T)x - b, dx_1\rangle\right) \\
    &= \langle (A+A^T)dx, dx_1\rangle \\
    &= \langle dx, (A+A^T)^Tdx_1\rangle \\
    &= \langle (A+A^T)dx_1, dx\rangle 
    \end{split}
    $$

    Таким образом, гессиан: $\nabla^2 f = (A+A^T)$.
:::
::::


## Пример 2

::: {.callout-example}
Найти гессиан $\nabla^2 f(x)$, если $f(x) = \ln \langle x, Ax\rangle$. 
:::

## Пример 3

::: {.callout-example} 
Найти градиент $\nabla f(x)$ и гессиан $\nabla^2f(x)$, если $f(x) = \ln \left( 1 + \exp\langle a,x\rangle\right)$ 
:::

. . .

:::: {.columns}
::: {.column width="50%"}
1. Начнем с записи дифференциала $df$. Имеем:
    $$
    f(x) = \ln \left( 1 + \exp\langle a, x\rangle\right)
    $$

    Используя правило дифференцирования сложной функции:
    $$
    df = d \left( \ln \left( 1 + \exp\langle a, x\rangle\right) \right)
    = \frac{d \left( 1 + \exp\langle a, x\rangle \right)}{1 + \exp\langle a, x\rangle}
    $$

    теперь посчитаем дифференциал экспоненты:
    $$
    d \left( \exp\langle a, x\rangle \right) = \exp\langle a, x\rangle \langle a, dx\rangle
    $$

    Подставляя в выражение выше, имеем:
    $$
    df = \frac{\exp\langle a, x\rangle \langle a, dx\rangle}{1 + \exp\langle a, x\rangle}
    $$

:::

. . .

::: {.column width="50%"}

2. Для выражения $df$ в нужной форме, запишем:
    $$
    df = \left\langle \frac{\exp\langle a, x\rangle}{1 + \exp\langle a, x\rangle} a, dx\right\rangle
    $$

    Напомним, что функция сигмоиды определяется как:
    $$
    \sigma(t) = \frac{1}{1 + \exp(-t)}
    $$
    Таким образом, мы можем переписать дифференциал:
    $$
    df = \langle \sigma(\langle a, x\rangle) a, dx \rangle
    $$

    Следовательно, градиент:
    $$
    \nabla f(x) = \sigma(\langle a, x\rangle) a
    $$
:::
::::

## Пример 3 {.noframenumbering}

::: {.callout-example} 
Найти градиент $\nabla f(x)$ и гессиан $\nabla^2f(x)$, если $f(x) = \ln \left( 1 + \exp\langle a,x\rangle\right)$ 
:::

3. Теперь найдем гессиан с помозью второго дифференциала:
    $$
    d(\nabla f(x)) = d\left( \sigma(\langle a, x\rangle) a \right)
    $$
    Так как вектор $a$ константа, нам необходимо продифференцировать лишь сигмоиду:
    $$
    d\left( \sigma(\langle a, x\rangle) \right) = \sigma(\langle a, x\rangle)(1 - \sigma(\langle a, x\rangle)) \langle a, dx\rangle
    $$

    То есть:
    $$
    d(\nabla f(x)) = \sigma(\langle a, x\rangle)(1 - \sigma(\langle a, x\rangle)) \langle a, dx\rangle a
    $$

    Запишем гессиан:
    $$
    \nabla^2 f(x) = \sigma(\langle a, x\rangle)(1 - \sigma(\langle a, x\rangle)) a a^T
    $$

# Автоматическое дифференцирование

## {.plain}
![Когда понял идею](autograd_expectations.jpeg)

## {.plain}
![Это не автоград](avtograd.jpeg){width=65%}

## Задача

Пусть есть задача оптимизации:

$$
L(w) \to \min_{w \in \mathbb{R}^d}
$$

. . .

* Such problems typically arise in machine learning, when you need to find optimal hyperparameters $w$ of an ML model (i.e. train a neural network). 
* You may use a lot of algorithms to approach this problem, but given the modern size of the problem, where $d$ could be dozens of billions it is very challenging to solve this problem without information about the gradients using zero-order optimization algorithms. 
* That is why it would be beneficial to be able to calculate the gradient vector $\nabla_w L = \left( \frac{\partial L}{\partial w_1}, \ldots, \frac{\partial L}{\partial w_d}\right)^T$. 
* Typically, first-order methods perform much better in huge-scale optimization, while second-order methods require too much memory.

## Пример: задача многомерного шкалирования

Suppose, we have a pairwise distance matrix for $N$ $d$-dimensional objects $D \in \mathbb{R}^{N \times N}$. Given this matrix, our goal is to recover the initial coordinates $W_i \in \mathbb{R}^d, \; i = 1, \ldots, N$.

. . .

$$
L(W) = \sum_{i, j = 1}^N \left(\|W_i - W_j\|^2_2 - D_{i,j}\right)^2 \to \min_{W \in \mathbb{R}^{N \times d}}
$$

. . .

Link to a nice visualization [$\clubsuit$](http://www.benfrederickson.com/numerical-optimization/), where one can see, that gradient-free methods handle this problem much slower, especially in higher dimensions.

:::{.callout-question}
Is it somehow connected with PCA?
:::

## Пример: задача многомерного шкалирования

![[Ссылка на анимацию](https://getfile.dokpub.com/yandex/get/https://disk.yandex.ru/i/B5uvsro-y6UCkw)](mds.png){width=40%}

## Пример: безградиентный градиентный спуск

:::: {.columns}
::: {.column width="50%"}
Рассмотрим следующую задачу оптимизации

$$
L(w) \to \min_{w \in \mathbb{R}^d}
$$

. . .

вместе с методом градиентного спуска (GD)

$$
w_{k+1} = w_k - \alpha_k \nabla_w L(w_k)
$$

. . .

Можно ли заменить $\nabla_w L(w_k)$, используя, лишь информацию нулевого порядка о функции?

. . .

Да, но есть нюанс.

. . .

One can consider 2-point gradient estimator^[I suggest a [nice](https://scholar.harvard.edu/files/yujietang/files/slides_2019_zero-order_opt_tutorial.pdf) presentation about gradient-free methods] $G$:

$$
G = d\dfrac{L(w + \varepsilon v)- L(w - \varepsilon v)}{2 \varepsilon}v, 
$$

where $v$ is spherically symmetric.
:::

. . .

::: {.column width="50%"}
!["Illustration of two-point estimator of Gradient Descent"](zgd_2p.pdf)
:::

::::


## Пример: конечно-разностный градиентный спуск

:::: {.columns}
::: {.column width="50%"}

$$
w_{k+1} = w_k - \alpha_k G
$$

. . .
 
One can also consider the idea of finite differences:

$$
G =  \sum\limits_{i=1}^d\dfrac{L(w+\varepsilon e_i) - L(w-\varepsilon e_i)}{2\varepsilon} e_i
$$

[Open In Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Zero_order_GD.ipynb)

:::

::: {.column width="50%"}
!["Illustration of finite differences estimator of Gradient Descent"](zgd_fd.pdf)
:::

::::

## Проклятие размерности методов нулевого порядка

$$
\min_{x \in \mathbb{R}^n} f(x)
$$

. . .

$$
\text{GD: } x_{k+1} = x_k - \alpha_k \nabla f(x_k) \qquad \qquad \text{Zero order GD: } x_{k+1} = x_k - \alpha_k G,
$$

where $G$ is a 2-point or multi-point estimator of the gradient.

. . .

|  | $f(x)$ - smooth | $f(x)$ - smooth and convex | $f(x)$ - smooth and strongly convex |
|:-:|:---:|:----:|:-------:|
| GD | $\|\nabla f(x_k)\|^2 \approx \mathcal{O} \left( \dfrac{1}{k} \right)$ | $f(x_k) - f^* \approx  \mathcal{O} \left( \dfrac{1}{k} \right)$ | $\|x_k - x^*\|^2 \approx \mathcal{O} \left( \left(1 - \dfrac{\mu}{L}\right)^k \right)$ |
| Zero order GD | $\|\nabla f(x_k)\|^2 \approx \mathcal{O} \left( \dfrac{n}{k} \right)$ | $f(x_k) - f^* \approx  \mathcal{O} \left( \dfrac{n}{k} \right)$ | $\|x_k - x^*\|^2 \approx \mathcal{O} \left( \left(1 - \dfrac{\mu}{n L}\right)^k \right)$ |


## Метод конечных разностей

The naive approach to get approximate values of gradients is **Finite differences** approach. For each coordinate, one can calculate the partial derivative approximation:

$$
\dfrac{\partial L}{\partial w_k} (w) \approx \dfrac{L(w+\varepsilon e_k) - L(w)}{\varepsilon}, \quad e_k = (0, \ldots, \underset{{\tiny k}}{1}, \ldots, 0)
$$

. . .

:::{.callout-question}
If the time needed for one calculation of $L(w)$ is $T$, what is the time needed for calculating $\nabla_w L$ with this approach?

. . .

**Answer** $2dT$, which is extremely long for the huge scale optimization. Moreover, this exact scheme is unstable, which means that you will have to choose between accuracy and stability.

. . .

**Theorem**

There is an algorithm to compute $\nabla_w L$ in $\mathcal{O}(T)$ operations. ^[Linnainmaa S. The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors.  Master’s Thesis (in Finnish), Univ. Helsinki, 1970.]

:::