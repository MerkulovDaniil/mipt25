---
title: "Нижние оценки для градиентного спуска. Ускорение с помощью полиномов Чебышёва"
author: Даниил Меркулов
institute: Методы оптимизации. МФТИ
format: 
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: true
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back14.jpeg}
---
# Повторение

## Результаты сходимости градиентного спуска для гладких функций

$$
\text{Градиентный спуск:} \qquad \qquad \min_{x \in \mathbb{R}^n} f(x) \qquad \qquad x_{k+1} = x_k - \alpha_k \nabla f(x_k) \qquad \lambda \left( \nabla^2 f(x)\right) \in \left[ \mu, L \right], \varkappa = \tfrac{L}{\mu}
$$

|выпуклая (негладкая) | гладкая (невыпуклая) | гладкая & выпуклая | гладкая & сильно выпуклая |
|:------:|:-------:|:------:|:--------:|
| $f(x_k) - f^* =  \mathcal{O} \left( \tfrac{1}{\sqrt{k}} \right)$ | $\min\limits_{0 \leq i \leq k}\|\nabla f(x_i)\| = \mathcal{O} \left( \tfrac{1}{\sqrt{k}} \right)$ | $f(x_k) - f^* =  \mathcal{O} \left( \tfrac{1}{k} \right)$ | $\|x_k - x^*\|^2 = \mathcal{O} \left( \left(1 - \tfrac{\mu}{L}\right)^k \right)$ |
| $k_\varepsilon =  \mathcal{O} \left( \tfrac{1}{\varepsilon^2} \right)$ | $k_\varepsilon = \mathcal{O} \left( \tfrac{1}{\varepsilon^2} \right)$ | $k_\varepsilon =  \mathcal{O}  \left( \tfrac{1}{\varepsilon} \right)$ | $k_\varepsilon  = \mathcal{O} \left( \varkappa \log \tfrac{1}{\varepsilon}\right)$ |

. . .

:::: {.columns}

::: {.column width="50%"}
Для гладких сильно выпуклых функций имеем:
$$
f(x^{k})-f^* \leq \left(1- \dfrac{\mu}{L}\right)^k (f(x^0)-f^*).
$$
Заметим, что для любого $x$, вследствие того, что $e^{-x}$ выпукла и $1-x$  - его касательная в точке $x=0$, имеем:
$$
1 - x \leq e^{-x}
$$
:::

. . .

::: {.column width="50%"}
В конечном итоге имеем:
$$
\begin{aligned}
\varepsilon &= f(x^{k_\varepsilon})-f^* \leq  \left(1- \dfrac{\mu}{L}\right)^{k_\varepsilon} (f(x^0)-f^*) \\
&\leq \exp\left(- k_\varepsilon\dfrac{\mu}{L}\right) (f(x^0)-f^*) \\
k_\varepsilon &\geq \varkappa \log \dfrac{f(x^0)-f^*}{\varepsilon} = \mathcal{\Omega} \left( \varkappa \log \dfrac{1}{\varepsilon}\right)
\end{aligned}
$$
:::

::::

. . .

\uncover<+->{{\bf Question:} Можно ли быстрее, используя лишь информацию первого порядка (градиенты)? }\uncover<+->{{\bf Да, можно.}}

# Нижние оценки для градиентных методов

## Нижние оценки для произвольных методов I порядка на классе гладких функций

$$
\text{Произвольный метод I порядка:} \qquad \min_{x \in \mathbb{R}^n} f(x) \qquad x_{k+1} = x_k - \sum_{i=0}^k\alpha_i \nabla f(x_i) \qquad \lambda \left( \nabla^2 f(x) \right) \in \left[ \mu, L \right], \varkappa = \tfrac{L}{\mu}
$$

| выпуклая (негладкая) | гладкая (невыпуклая)^[[Carmon, Duchi, Hinder, Sidford, 2017](https://arxiv.org/pdf/1710.11606.pdf)] | гладкая & выпуклая^[[Nemirovski, Yudin, 1979](https://fmin.xyz/assets/files/nemyud1979.pdf)] | гладкая & сильно выпуклая |
|:------:|:-------:|:------:|:--------:|
| $f(x_k) - f^* =  \Omega \left( \tfrac{1}{\sqrt{k}} \right)$ | $\min\limits_{0 \leq i \leq k}\|\nabla f(x_i)\| = \Omega \left( \tfrac{1}{\sqrt{k}} \right)$ | $f(x_k)-f^*=\Omega\!\left(\tfrac{1}{k^2}\right)$ | $f(x_k)-f^*=\Omega\!\left(\left(\tfrac{\sqrt{\varkappa}-1}{\sqrt{\varkappa}+1}\right)^{\!2k}\right)$ |
| $k_\varepsilon =  \Omega \left( \tfrac{1}{\varepsilon^2} \right)$ | $k_\varepsilon=\Omega\!\left(\tfrac{1}{\varepsilon^2}\right)$ | $k_\varepsilon=\Omega\!\left(\tfrac{1}{\sqrt{\varepsilon}}\right)$ | $k_\varepsilon=\Omega\!\big(\sqrt{\varkappa}\,\log\tfrac{1}{\varepsilon}\big)$ |



## Чёрный ящик

Итерация градиентного спуска:
$$
\begin{aligned}
x_{k+1} &= x_k - \alpha_k \nabla f(x_k) \pause \\ 
&= x_{k-1} - \alpha_{k-1} \nabla f(x_{k-1}) - \alpha_k \nabla f(x_k) \pause \\ 
& \;\;\vdots \pause \\ 
&= x_0 - \sum\limits_{i=0}^k \alpha_{k-i} \nabla f(x_{k-i})
\end{aligned}
$$

. . .

Рассмотрим семейство методов первого порядка, где
$$
\begin{aligned}
x_{k+1} &\in x_0 + \text{Lin} \left\{\nabla f(x_0), \nabla f(x_1), \ldots, \nabla f(x_k)\right\} \; & f \text{ — гладкая} \\
x_{k+1} &\in x_0 + \text{Lin} \left\{g_{0}, g_{1}, \ldots, g_{k}\right\} \text{, где }
g_{i} \in \partial f(x_{i}) \; & f \text{ — негладкая}
\end{aligned}
$$ {#eq-fom}

. . .

Чтобы построить нижнюю оценку, нам нужно найти функцию $f$ из соответствующего класса, такую, что любой метод из семейства (-@eq-fom) будет работать не быстрее этой нижней оценки.


## Гладкий случай

:::{.callout-theorem}
Существует $L$-гладкая и выпуклая функция $f$, такая, что любой метод (-@eq-fom) для всех $k$, $1 \leq k \leq \frac{n-1}{2}$, удовлетворяет:
$$
f(x_k) - f^* \geq \frac{3L \|x_0 - x^*\|_2^2}{32(k+1)^2}
$$
:::

. . .

* Какой бы метод из семейства методов первого порядка вы ни использовали, найдётся функция $f$, на которой скорость сходимости не лучше $\mathcal{O}\left(\frac{1}{k^2}\right)$.
* Ключом к доказательству является явное построение специальной функции $f$.
* Обратите внимание, что эта граница $\mathcal{O}\left(\frac{1}{k^2}\right)$ не соответствует скорости градиентного спуска $\mathcal{O}\left(\frac{1}{k}\right)$. Два возможных варианта:
    a. Нижняя оценка не является точной.
    b. \textbf<7>{Метод градиентного спуска не является оптимальным для этой задачи.}


## Наихудшая функция Нестерова

:::: {.columns}

::: {.column width="50%"}
* Пусть $n=2k+1$ и $A \in \mathbb{R}^{n \times n}$.
    $$
    A = \begin{bmatrix}
        2 & -1 & 0 & 0 & \cdots & 0 \\
        -1 & 2 & -1 & 0 & \cdots & 0 \\
        0 & -1 & 2 & -1  & \cdots & 0 \\
        0 & 0 & -1 & 2  & \cdots & 0 \\
        \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & 0 & \cdots & 2  \\
    \end{bmatrix}
    $$
* Обратите внимание, что
    $$
    x^T A x = x_1^2 + x_n^2 + \sum_{i=1}^{n-1} (x_i - x_{i+1})^2,
    $$
    Следовательно, $x^T A x \geq 0$. Также легко увидеть, что $0 \preceq A \preceq 4I$.
:::

. . .

::: {.column width="50%"}
Пример, когда $n=3$:
$$
A = \begin{bmatrix}
    2 & -1 & 0 \\
    -1 & 2 & -1 \\
    0 & -1 & 2 \\
    \end{bmatrix}
$$

. . .

Нижняя оценка:
$$
\begin{aligned}
x^T A x &= 2x_1^2 + 2x_2^2 + 2x_3^2 - 2x_1x_2 - 2x_2x_3 \\
&= x_1^2 + x_1^2 - 2x_1x_2 + x_2^2 + x_2^2 - 2x_2x_3 + x_3^2 + x_3^2 \\
&= x_1^2 + (x_1 - x_2)^2 + (x_2 - x_3)^2 + x_3^2 \geq 0
\end{aligned}
$$

. . .

Верхняя оценка
$$
\begin{aligned}
x^T A x &= 2x_1^2 + 2x_2^2 + 2x_3^2 - 2x_1x_2 - 2x_2x_3 \pause\\
& \leq 4(x_1^2 + x_2^2 + x_3^2) \pause\\
0 &\leq 2x_1^2 + 2x_2^2 + 2x_3^2 + 2x_1x_2 + 2x_2x_3 \pause\\
0 &\leq x_1^2 + x_1^2 + 2x_1x_2 + x_2^2 + x_2^2 + 2x_2x_3 + x_3^2 + x_3^2 \pause\\
0 &\leq x_1^2 + (x_1 + x_2)^2 + (x_2 + x_3)^2 + x_3^2
\end{aligned}
$$

:::
::::


## Наихудшая функция Нестерова
* Определим следующую $L$-гладкую выпуклую функцию: $f(x) = \frac{L}{4}\left(\frac{1}{2} x^T A x - e_1^T x \right) = \frac{L}{8} x^T A x - \frac{L}{4} e_1^T x.$

* Оптимальное решение $x^*$ удовлетворяет $Ax^* = e_1$, и решение этой системы уравнений дает:
    $$
    \begin{bmatrix}
        2 & -1 & 0 & 0 & \cdots & 0 \\
        -1 & 2 & -1 & 0 & \cdots & 0 \\
        0 & -1 & 2 & -1  & \cdots & 0 \\
        0 & 0 & -1 & 2  & \cdots & 0 \\
        \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & 0 & \cdots & 2  \\
    \end{bmatrix} \begin{bmatrix}
        x_1^* \\
        x_2^* \\
        x_3^* \\
        \vdots \\
        x_{n}^* \\
    \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix} \quad \begin{cases} 2x_1^* - x_2^* = 1 \\ -x_{i-1}^* + 2x_{i}^* - x_{i+1}^* = 0, \; i = 2, \ldots, n-1 \\ -x_{n-1}^* + 2x_n^* = 0 \end{cases}
    $$
* Гипотеза: $x_i^* = a+bi$ (вдохновлённая физикой). Проверьте, что выполнено второе уравнение, в то время как $a$ и $b$ вычисляются из первого и последнего уравнений.
* Решение:
    $$
    x^*_i = 1 - \frac{i}{n+1},
    $$
* И значение функции равно
    $$
    f(x^*) =  \frac{L}{8} {x^*}^T A x^* - \frac{L}{4}\langle x^*, e_1 \rangle = -\frac{L}{8} \langle x^*, e_1 \rangle = -\frac{L}{8} \left(1 - \frac{1}{n+1}\right).
    $$


## Гладкий случай (доказательство)

:::: {.columns}

::: {.column width="45%"}
* Предположим, что мы начинаем с $x_0 = 0$. Запросив у оракула градиент, мы получаем $g_0 = -\tfrac{L}{4}e_1$. Тогда, $x_1$ должен лежать на линии, генерируемой $e_1$. В этой точке все компоненты $x_1$ равны нулю, кроме первой, поэтому
    $$
    x_1 = \begin{bmatrix} \bullet \\ 0 \\ \vdots \\ 0 \end{bmatrix}.
    $$
* На второй итерации оракул возвращает градиент $g_1 = \tfrac{L}{4}\left(Ax_1 - e_1\right)$. Тогда, $x_2$ должен лежать на линии, генерируемой $e_1$ и $Ax_1 - e_1$. Все компоненты $x_2$ равны нулю, кроме первых двух, поэтому
    $$
    \begin{bmatrix}
        2 & -1 & 0  & \cdots & 0 \\
        -1 & 2 & -1 & \cdots & 0 \\
        0 & -1 & 2 & \cdots & 0 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & \cdots & 2  \\
    \end{bmatrix} \begin{bmatrix} \bullet \\ 0 \\ \vdots \\ 0 \end{bmatrix} \Rightarrow x_2 = \begin{bmatrix} \bullet \\ \bullet \\ 0 \\ \vdots \\ 0 \end{bmatrix}.
    $$
:::

::: {.column width="55%"}
* Из-за структуры матрицы $A$ можно показать, что после $k$ итераций все последние $n-k$ компоненты $x_k$ равны нулю.
    $$
    x_{k} =
    \begin{bmatrix} 
        \bullet \\ 
        \bullet \\ 
        \vdots \\ 
        \bullet \\ 
        0 \\ 
        \vdots \\ 
        0 
    \end{bmatrix}
    \begin{array}{l}
        1 \\ 
        2 \\ 
        \vdots \\ 
        k \\ 
        k+1 \\ 
        \vdots \\ 
        n 
    \end{array}
    $$
* Однако, поскольку каждая итерация $x_k$, произведенная нашим методом, лежит в $S_k = \text{Lin}\{e_1, e_2, \ldots, e_{k}\}$ (т.е. имеет нули в координатах $k+1,\dots,n$), она не может "достичь" полного оптимального вектора $x^*$. Другими словами, даже если бы мы выбрали лучший возможный вектор из $S_k$, обозначаемый
$$
\tilde{x}_k=\arg\min_{x\in S_k} f(x),
$$
значение функции в нём $f(\tilde{x}_k)$ будет выше, чем $f(x^*)$.
:::
::::

## Гладкий случай (доказательство)

* Поскольку $x_k\in S_k = \text{Lin}\{e_1, e_2, \ldots, e_{k}\}$ и $\tilde{x}_k$ является лучшим возможным приближением к $x^*$ в $S_k$, мы имеем
    $$
    f(x_k)\ge f(\tilde{x}_k).
    $$
* Следовательно, 
    $$
    f(x_k)-f(x^*)\ge f(\tilde{x}_k)-f(x^*).
    $$
* Аналогично, для оптимума исходной функции, мы имеем $\tilde{x}_{k_{(i)}} = 1 - \frac{i}{k+1}$ и $f(\tilde{x}_k) = -\frac{L}{8} \left(1 - \frac{1}{k+1}\right)$.
* Теперь мы имеем:
    $$
    \begin{aligned}
    f(x_k)-f(x^*) &\ge f(\tilde{x}_k)-f(x^*) \\
    \uncover<+->{&= -\frac{L}{8} \left(1 - \frac{1}{k+1}\right) - \left(-\frac{L}{8} \left(1 - \frac{1}{n+1}\right)\right) \\}
    \uncover<+->{&= \frac{L}{8} \left(\frac{1}{k+1} - \frac{1}{n+1}\right) = \frac{L}{8} \left(\frac{n-k}{(k+1)(n+1)}\right) \\}
    \uncover<+->{&\overset{n = 2k+1}{=} \frac{L }{16(k+1)}}
    \end{aligned}
    $$ {#eq-lb1}

## Гладкий случай (доказательство)

:::: {.columns}

::: {.column width="70%"}

* Теперь мы ограничиваем $R = \|x_0 - x^*\|_2$:
    $$
    \begin{aligned}
    \|x_0 - x^*\|_2^2 &= \|0 - x^*\|_2^2 = \|x^*\|_2^2 = \sum_{i=1}^n \left( 1 - \frac{i}{n+1} \right)^2 \\
    \uncover<+->{&= n - \frac{2}{n+1} \sum_{i=1}^{n} i + \frac{1}{(n+1)^2} \sum_{i=1}^{n} i^2 \\}
    \uncover<+->{&\leq n - \frac{2}{n+1} \cdot \frac{n(n+1)}{2} + \frac{1}{(n+1)^2} \cdot \frac{(n+1)^3}{3} \\}
    \uncover<+->{&= \frac{n+1}{3} \overset{n = 2k+1}{=} \frac{2(k+1)}{3}.}
    \end{aligned}
    $$

* Следовательно, 
    $$
    k+1 \geq \frac{3}{2}\|x_0 - x^*\|_2^2 = \frac32 R^2
    $${#eq-lb2}
:::

. . .

::: {.column width="30%"}
Заметим, что
$$
\begin{aligned}
\sum_{i=1}^{n} i &= \frac{n(n+1)}{2} \\
\sum_{i=1}^{n} i^2 &= \frac{n(n+1)(2n+1)}{6} \\
&\leq \frac{(n+1)^3}{3}
\end{aligned}
$$
:::

::::

## Гладкий случай (доказательство)

Наконец, используя ([-@eq-lb1]) и ([-@eq-lb2]), мы получаем:
$$
\begin{aligned}
f(x_k) - f(x^*) &\geq \frac{L}{16(k+1)}  = \frac{L (k+1)}{16(k+1)^2} \pause \\
&\geq \frac{L}{16(k+1)^2} \frac{3}{2} R^2  \pause \\
&= \frac{3L R^2}{32 (k+1)^2}
\end{aligned}
$$

. . .

Это завершает доказательство с желаемой скоростью $\mathcal{O}\left( \frac{1}{k^2}\right)$.

## Нижние оценки для гладкого случая

:::{.callout-theorem}

### Гладкий выпуклый случай
Существует $L$-гладкая выпуклая функция $f$, такая, что любой [метод в форме @eq-fom] для всех $k$, $1 \leq k \leq \frac{n-1}{2}$, удовлетворяет:
$$
f(x_k) - f^* \geq \frac{3L \|x_0 - x^*\|_2^2}{32(k+1)^2}
$$
:::

:::{.callout-theorem}

### Гладкий сильно выпуклый случай

Для любого $x_0$ и любого $\mu > 0$, $\varkappa = \frac{L}{\mu} > 1$, существует $L$-гладкая и $\mu$-сильно выпуклая функция $f$, такая, что для любого метода [в форме @eq-fom] выполняются неравенства:

$$
\begin{aligned}
\|x_k - x^*\|_2 &\geq \left( \frac{\sqrt{\varkappa}-1}{\sqrt{\varkappa}+1}\right)^{k} \|x_0 - x^*\|_2 \\
f(x_k) - f^* &\geq \frac{\mu}{2} \left( \frac{\sqrt{\varkappa}-1}{\sqrt{\varkappa}+1}\right)^{2k} \|x_0 - x^*\|_2^2 
\end{aligned}
$$
:::


## Ускорение для квадратичных функций

## Результат сходимости для квадратичных функций

Предположим, что мы решаем задачу минимизации сильно выпуклой квадратичной функции, с помощью метода градиентного спуска:
$$
f(x) = \frac{1}{2} x^T A x - b^T x \qquad x_{k+1} = x_k - \alpha_k \nabla f(x_k).
$$

. . .

:::{.callout-theorem}
Градиентный спуск с шагом $\alpha_k = \frac{2}{\mu + L}$ сходится к оптимальному решению $x^*$ со следующей гарантией:
$$
\|x_{k+1} - x^*\|_2 \le \left( \frac{\varkappa-1}{\varkappa+1}\right)^k \|x_0 - x^*\|_2 \qquad f(x_{k+1}) - f(x^*) \le \left( \frac{\varkappa-1}{\varkappa+1}\right)^{2k} \left(f(x_0) - f(x^*)\right)
$$
где $\varkappa = \frac{L}{\mu}$ является числом обусловленности $A$.
:::

## Число обусловленности $\varkappa$

[![](condition_number_gd.pdf)](https://fmin.xyz/docs/visualizations/condition_number_gd.mp4)

## Ускорение из первых принципов

$$
f(x) = \frac{1}{2} x^T A x - b^T x \qquad x_{k+1} = x_k - \alpha_k \nabla f(x_k).
$$

Пусть $x^*$ будет единственным решением системы линейных уравнений $Ax=b$ и пусть $e_k = x_k - x^*$, где $x_{k+1}=x_k - \alpha_k (Ax_k-b)$ определяется рекурсивно, начиная с некоторого $x_0$, а $\alpha_k$ — шаг, который мы определим позже.
$$
e_{k+1} = (I-\alpha_k A)e_k.
$$

. . .

### Полиномы

:::: {.columns}
::: {.column width="50%"}

Вышеуказанный расчет дает нам $e_k = p_k(A)e_0,$

где $p_k$ является полиномом
$$
p_k(a) = \prod_{i=1}^k (1-\alpha_ia).
$$

. . .

Мы можем ограничить норму ошибки как
$$
\|e_k\|\le \|p_k(A)\|\cdot\|e_0\|\,.
$$
:::

. . .

::: {.column width="50%"}
Поскольку $A$ является симметричной матрицей с собственными значениями в $[\mu,L],$:
$$
\|p_k(A)\|\le \max_{\mu\le a\le L} \left|p_k(a)\right|\,.
$$
Это приводит к интересной постановке задачи: среди всех полиномов, удовлетворяющих $p_k(0)=1$, мы ищем полином, значение которого как можно меньше отклоняется от нуля на интервале $[\mu,L]$.
:::
::::

## Наивное полиномиальное решение

:::: {.columns}
::: {.column width="50%"}

Наивное решение состоит в том, чтобы выбрать равномерный шаг $\alpha_k=\frac{2}{\mu+L}$. Благодаря этому $|p_k(\mu)| = |p_k(L)|$.
$$
\|e_k\|\le \left(\frac{\varkappa-1}{\varkappa+1}\right)^k\|e_0\|
$$
Это точно та же скорость, которую мы доказали в предыдущей лекции для любой гладкой и сильно выпуклой функции.

Давайте посмотрим на этот полином поближе. На правом рисунке мы выбираем $\mu=1$ и $L=10$ так, что $\varkappa=10$. Следовательно, соответствующий интервал равен $[1,10]$.

Можем ли мы сделать лучше? Ответ — да.
:::

::: {.column width="50%"}
\includegraphics<1>[width=\columnwidth]{gd_polynom_2_ru.pdf}
\includegraphics<2>[width=\columnwidth]{gd_polynom_3_ru.pdf}
\includegraphics<3>[width=\columnwidth]{gd_polynom_4_ru.pdf}
\includegraphics<4>[width=\columnwidth]{gd_polynom_5_ru.pdf}
\includegraphics<5>[width=\columnwidth]{gd_polynom_6_ru.pdf}
:::
::::

## Полиномы Чебышева


:::: {.columns}
::: {.column width="50%"}

Полиномы Чебышёва дают оптимальный ответ на поставленный вопрос. При соответствующем шкалировании они минимизируют абсолютное значение на заданном интервале $[\mu,L]$  , одновременно удовлетворяя нормировочному условию $p(0)=1$.
$$
\begin{aligned}
T_0(x) &= 1\\
T_1(x) &= x\\
T_k(x) &=2xT_{k-1}(x)-T_{k-2}(x),\qquad k\ge 2.\\
\end{aligned}
$$

Давайте построим стандартные полиномы Чебышёва (без масштабирования):
:::

::: {.column width="50%"}
\includegraphics<1>[width=\columnwidth]{gd_polynom_cheb_1_ru.pdf}
\includegraphics<2>[width=\columnwidth]{gd_polynom_cheb_2_ru.pdf}
\includegraphics<3>[width=\columnwidth]{gd_polynom_cheb_3_ru.pdf}
\includegraphics<4>[width=\columnwidth]{gd_polynom_cheb_4_ru.pdf}
\includegraphics<5>[width=\columnwidth]{gd_polynom_cheb_5_ru.pdf}

:::
::::

## Отшкалированные полиномы Чебышёва

Оригинальные полиномы Чебышёва определены на интервале $[-1,1]$. Чтобы использовать их для наших целей, мы должны отшкалировать их на интервал $[\mu,L]$. 

. . .

:::: {.columns} 
::: {.column width="50%"}

Мы будем использовать следующее аффинное преобразование:
$$
x = \frac{L + \mu - 2a}{L - \mu}, \quad a \in [\mu,L], \quad x \in [-1,1]. 
$$
:::

::: {.column width="50%"}
Обратите внимание, что $x=1$ соответствует $a=\mu$, $x=-1$ соответствует $a=L$ и $x=0$ соответствует $a=\frac{\mu+L}{2}$. Это преобразование гарантирует, что поведение полинома Чебышёва на интервале $[-1,1]$ транслируется на интервал $[\mu, L]$.
:::

::::

. . .

В нашем анализе ошибок мы требуем, чтобы полином был равен 1 в 0 (т.е. $p_k(0)=1$). После применения преобразования значение $T_k$ в точке, соответствующей $a=0$, может не быть 1. Следовательно, мы умножаем на обратную величину $T_k$ в точке
$$
\frac{L+\mu}{L-\mu}, \qquad \text{что обеспечивает} \qquad P_k(0)= T_k\left(\frac{L+\mu-0}{L-\mu}\right) \cdot T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1} = 1.
$$

. . .

Построим отшкалированные полиномы Чебышёва
$$
P_k(a) = T_k\left(\frac{L+\mu-2a}{L-\mu}\right) \cdot T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1}
$$
и увидим, что они больше подходят для нашей задачи, чем наивные полиномы на интервале $[\mu,L]$.

## Отшкалированные полиномы Чебышёва

\includegraphics<1>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_1_ru.pdf}
\includegraphics<2>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_2_ru.pdf}
\includegraphics<3>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_3_ru.pdf}
\includegraphics<4>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_4_ru.pdf}
\includegraphics<5>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_5_ru.pdf}
\includegraphics<6>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_6_ru.pdf}
\includegraphics<7>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_7_ru.pdf}
\includegraphics<8>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_8_ru.pdf}
\includegraphics<9>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_9_ru.pdf}
\includegraphics<10>[width=\columnwidth,height=0.8\textheight,keepaspectratio]{gd_polynoms_10_ru.pdf}

## Верхняя оценка для полиномов Чебышёва

Мы можем видеть, что максимальное значение полинома Чебышёва на интервале $[\mu,L]$ достигается на концах отрезка в точках $a=\mu$ и $a=L$. Следовательно, мы можем использовать следующую верхнюю оценку:
$$
\|P_k(A)\|_2 \le P_k(\mu) = T_k\left(\frac{L+\mu-2\mu}{L-\mu}\right) \cdot T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1} = T_k\left(1\right) \cdot T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1} = T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1}
$$

. . .

Используя определение числа обусловленности $\varkappa = \frac{L}{\mu}$, мы получаем:
$$
\|P_k(A)\|_2 \le T_k\left(\frac{\varkappa+1}{\varkappa-1}\right)^{-1} = T_k\left(1 + \frac{2}{\varkappa-1}\right)^{-1} = T_k\left(1 + \epsilon\right)^{-1}, \quad \epsilon = \frac{2}{\varkappa-1}.
$$

. . .

Именно в этот момент явно возникнет ускорение. Мы ограничим значение $\|P_k(A)\|_2$ сверху величиной $\left(\frac{1}{1 + \sqrt{\epsilon}}\right)^k$. Для этого детально изучим величину $|T_k(1 + \epsilon)|$.

## Верхняя оценка для полиномов Чебышёва

Чтобы ограничить $|P_k|$ сверху, мы должны ограничить $|T_k(1 + \epsilon)|$ снизу.

. . .

:::: {.columns}
::: {.column width="50%"}

1. Для любого $x\ge 1$, полиномы Чебышёва первого рода могут быть записаны как
   $$
   \begin{aligned}
   T_k(x)&=\cosh\left(k\,\mathrm{arccosh}(x)\right)\\
   T_k(1+\epsilon)&=\cosh\left(k\,\mathrm{arccosh}(1+\epsilon)\right).
   \end{aligned}
   $$

2. Помните, что:
    $$
    \cosh(x)=\frac{e^x+e^{-x}}{2} \quad \mathrm{arccosh}(x) = \ln(x + \sqrt{x^2-1}).
    $$

3. Теперь, пусть $\phi=\mathrm{arccosh}(1+\epsilon)$,
    $$
    e^{\phi}=1+\epsilon + \sqrt{2\epsilon+\epsilon^2} \geq 1+\sqrt{\epsilon}.
    $$
:::

::: {.column width="50%"}

4. Следовательно,
    $$
    \begin{aligned}
    T_k(1+\epsilon)&=\cosh\left(k\,\mathrm{arccosh}(1+\epsilon)\right) \\
    &= \cosh\left(k\phi\right) \\
    &= \frac{e^{k\phi} + e^{-k\phi}}{2} \geq\frac{e^{k\phi}}{2} \\
    &= \frac{\left(1+\sqrt{\epsilon}\right)^k}{2}.
    \end{aligned}
    $$

5. Наконец, мы получаем:
    $$
    \begin{aligned}
    \|e_k\| &\leq \|P_k(A)\| \|e_0\| \leq \frac{2}{\left(1 + \sqrt{\epsilon}\right)^k} \|e_0\| \\ 
    &\leq 2 \left(1 + \sqrt{\frac{2}{\varkappa-1}}\right)^{-k} \|e_0\| \\
    &\leq 2 \exp\left( - \sqrt{\frac{2}{\varkappa-1}} k\right) \|e_0\|
    \end{aligned}
    $$
    
:::
::::    

## Ускоренный метод [1/2]

Из-за рекурсивного определения полиномов Чебышёва мы непосредственно получаем итерационную схему ускоренного алгоритма. Переформулируя рекурсию в терминах наших  отшкалированных полиномов Чебышёва, мы получаем:
$$
T_{k+1}(x) =2xT_{k}(x)-T_{k-1}(x)
$$

. . .

Принимая во внимание, что $x = \frac{L+\mu-2a}{L-\mu}$, и:

:::: {.columns}
::: {.column width="50%"}
$$
\begin{aligned}
P_k(a) &= T_k\left(\frac{L+\mu-2a}{L-\mu}\right) T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1} \pause \\
T_k\left(\frac{L+\mu-2a}{L-\mu}\right) &= P_k(a) T_k\left(\frac{L+\mu}{L-\mu}\right) 
\end{aligned}
$$
:::

. . .

::: {.column width="50%"}
$$
\begin{aligned}
T_{k-1}\left(\frac{L+\mu-2a}{L-\mu}\right) &= P_{k-1}(a) T_{k-1}\left(\frac{L+\mu}{L-\mu}\right) \\
T_{k+1}\left(\frac{L+\mu-2a}{L-\mu}\right) &= P_{k+1}(a) T_{k+1}\left(\frac{L+\mu}{L-\mu}\right)
\end{aligned}
$$
:::
::::
\pause
$$
\begin{aligned}
P_{k+1}(a) t_{k+1} &= 2 \frac{L+\mu-2a}{L-\mu} P_{k}(a) t_{k} - P_{k-1}(a) t_{k-1} \text{, где } t_{k} = T_{k}\left(\frac{L+\mu}{L-\mu}\right) \pause\\
P_{k+1}(a) &= 2 \frac{L+\mu-2a}{L-\mu} P_{k}(a) \frac{t_{k}}{t_{k+1}} - P_{k-1}(a) \frac{t_{k-1}}{t_{k+1}}
\end{aligned}
$$

. . .

Поскольку мы имеем $P_{k+1}(0) = P_{k}(0) = P_{k-1}(0) = 1$, получаем рекуррентную формулу вида:
$$
P_{k+1}(a) = (1 - \alpha_k a) P_k(a) + \beta_k \left(P_{k}(a) - P_{k-1}(a) \right).
$$

## Ускоренный метод [2/2]

:::: {.columns}
::: {.column width="50%"}
Перегруппируя члены, мы получаем:
$$
\begin{aligned}
P_{k+1}(a) &= (1 + \beta_k) P_k(a) - \alpha_k a P_k(a) - \beta_k P_{k-1}(a), \pause \\
P_{k+1}(a) &= 2 \frac{L+\mu}{L-\mu}  \frac{t_{k}}{t_{k+1}} P_{k}(a) - \frac{4a}{L-\mu}  \frac{t_{k}}{t_{k+1}}P_{k}(a) - \frac{t_{k-1}}{t_{k+1}} P_{k-1}(a)
\end{aligned}
$$
:::

. . .

::: {.column width="50%"}
$$
\begin{cases}
\beta_k = \dfrac{t_{k-1}}{t_{k+1}}, \\[6pt]
\alpha_k = \dfrac{4}{L-\mu} \dfrac{t_k}{t_{k+1}}, \\[6pt]
1 + \beta_k = 2 \dfrac{L + \mu}{L - \mu} \dfrac{t_k}{t_{k+1}}
\end{cases}
$$


:::
::::

. . .

Мы почти закончили :) Помним, что $e_{k+1} = P_{k+1}(A) e_0$. Также обратим внимание, что мы работаем с квадратичной задачей, поэтому мы можем предположить $x^* = 0$ без ограничения общности. В этом случае $e_0 = x_0$ и $e_{k+1} = x_{k+1}$. \pause
$$
\begin{aligned}
x_{k+1} &= P_{k+1}(A) x_0 \pause =  (I - \alpha_k A) P_k(A) x_0 + \beta_k \left(P_{k}(A) - P_{k-1}(A) \right) x_0 \pause \\
&= (I - \alpha_k A) x_k + \beta_k \left(x_k - x_{k-1}\right)
\end{aligned}
$$

. . .

Для квадратичной задачи мы имеем $\nabla f(x_k) = A x_k$, поэтому мы можем переписать обновление как:
$$
\boxed{
x_{k+1} = x_k - \alpha_k \nabla f(x_k) + \beta_k \left(x_k - x_{k-1}\right)
}
$$

## Ускорение из первых принципов

[![](chebyshev_gd.pdf)](https://fmin.xyz/docs/visualizations/chebyshev_gd.mp4)


# Метод тяжёлого шарика

## Колебания и ускорение

$$
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1}).
$$

[![](GD_vs_HB_hor.pdf)](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/GD.ipynb)

## Метод тяжёлого шарика Поляка

:::: {.columns}

::: {.column width="25%"}
![](GD_HB.pdf)
:::

::: {.column width="75%"}
Давайте представим идею моментума (импульса, тяжёлого шарика), предложенную Б.Т. Поляком в 1964 году. Обновление метода тяжёлого шарика имеет вид

$$
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1}).
$$

. . .

В нашем (квадратичном) случае это
$$
\hat{x}_{k+1} = \hat{x}_k - \alpha \Lambda \hat{x}_k + \beta (\hat{x}_k - \hat{x}_{k-1}) = (I - \alpha \Lambda + \beta I) \hat{x}_k - \beta \hat{x}_{k-1}
$$

. . .

Это можно переписать как

$$
\begin{split}
&\hat{x}_{k+1} = (I - \alpha \Lambda + \beta I) \hat{x}_k - \beta \hat{x}_{k-1}, \\
&\hat{x}_{k} = \hat{x}_k.
\end{split}
$$

. . .

Давайте введем следующее обозначение: $\hat{z}_k = \begin{bmatrix} 
\hat{x}_{k+1} \\
\hat{x}_{k}
\end{bmatrix}$. Следовательно, $\hat{z}_{k+1} = M \hat{z}_k$, где матрица итерации $M$ имеет вид:

. . .

$$
M = \begin{bmatrix} 
I - \alpha \Lambda + \beta I & - \beta I \\
I & 0_{d}
\end{bmatrix}.
$$

:::
::::

## Сведение к скалярному случаю

Обратим внимание, что $M$ является матрицей $2d \times 2d$ с четырьмя блочно-диагональными матрицами размера $d \times d$ внутри. Это означает, что мы можем изменить порядок координат, чтобы сделать $M$ блочно-диагональной. Обратите внимание, что в уравнении ниже матрица $M$ обозначает то же самое, что и в обозначении выше, за исключением описанной перестановки строк и столбцов. Мы используем эту небольшую перегрузку обозначений для простоты. 

. . .

:::: {.columns}

::: {.column width="40%"}

![Иллюстрация перестановки матрицы $M$](Rearranging_squares.pdf)

:::
:::{.column width="60%"}
$$
\begin{aligned}
\begin{bmatrix} 
\hat{x}_{k}^{(1)} \\
\vdots \\
\hat{x}_{k}^{(d)} \\
\addlinespace 
\hat{x}_{k-1}^{(1)} \\
\vdots \\
\hat{x}_{k-1}^{(d)}
\end{bmatrix} \to 
\begin{bmatrix} 
\hat{x}_{k}^{(1)} \\
\addlinespace 
\hat{x}_{k-1}^{(1)} \\
\vdots \\
\hat{x}_{k}^{(d)} \\
\addlinespace 
\hat{x}_{k-1}^{(d)}
\end{bmatrix} \quad M = \begin{bmatrix}
M_1\\
&M_2\\
&&\ldots\\
&&&M_d
\end{bmatrix}
\end{aligned}
$$
:::
::::
где $\hat{x}_{k}^{(i)}$ является $i$-й координатой вектора $\hat{x}_{k} \in \mathbb{R}^d$ и $M_i$ обозначает $2 \times 2$ матрицу. Переупорядочение позволяет нам исследовать динамику метода независимо от размерности. Асимптотическая скорость сходимости $2d$-мерной последовательности векторов $\hat{z}_k$ определяется наихудшей скоростью сходимости среди его блока координат. Следовательно, достаточно исследовать оптимизацию в одномерном случае.

## Сведение к скалярному случаю

Для $i$-й координаты, где $\lambda_i$ — $i$-е собственное значение матрицы $A$, имеем: 

$$
M_i = \begin{bmatrix} 
1 - \alpha \lambda_i + \beta & -\beta \\
1 & 0
\end{bmatrix}.
$$

. . .

Метод будет сходиться, если $\rho(M) < 1$, и оптимальные параметры могут быть вычислены путем оптимизации спектрального радиуса
$$
\alpha^*, \beta^* = \arg \min_{\alpha, \beta} \max_{i} \rho(M_i), \quad \alpha^* = \dfrac{4}{(\sqrt{L} + \sqrt{\mu})^2}, \quad \beta^* = \left(\dfrac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}\right)^2.
$$

. . .

Можно показать, что для таких параметров матрица $M$ имеет комплексные собственные значения, которые образуют комплексно-сопряжённую пару, поэтому расстояние до оптимума (в этом случае $\| z_k \|$) обычно не убывает монотонно.

## Вывод оптимальных гиперпараметров [1/3]

Характеристический полином матрицы $M_i$ имеет вид:
$$
\det(M_i - \lambda I) = \lambda^2 - (1 - \alpha \lambda_i + \beta)\lambda + \beta = 0.
$$

. . .

Собственные значения матрицы $M_i$:
$$
\lambda^M_{1,2} = \frac{(1 + \beta - \alpha \lambda_i) \pm \sqrt{(1 + \beta - \alpha \lambda_i)^2 - 4\beta}}{2}.
$$

. . .

Спектральный радиус $\rho(M_i)$ зависит от знака дискриминанта $\Delta_i = (1 + \beta - \alpha\lambda_i)^2 - 4\beta$:

:::: {.columns}
::: {.column width="50%"}

**Случай 1: $\Delta_i \geq 0$ (вещественные с.з.)**

Собственные значения вещественны и спектральный радиус равен
$$
\rho(M_i) = \max(|\lambda^M_1|, |\lambda^M_2|).
$$
Заметим, что $\lambda^M_1 \cdot \lambda^M_2 = \beta$ (свободный член хар. полинома), поэтому оба с.з. неотрицательны при $\beta > 0$ и $\rho(M_i) \geq \sqrt{\beta}$.
:::

. . .

::: {.column width="50%"}

**Случай 2: $\Delta_i < 0$ (комплексные с.з.)**

Собственные значения комплексно-сопряжённые: $\lambda^M_1 = \overline{\lambda^M_2}$. По теореме Виета, произведение корней хар. полинома $\lambda^2 - (1+\beta-\alpha\lambda_i)\lambda + \beta = 0$ равно свободному члену:
$$
\lambda^M_1 \cdot \lambda^M_2 = |\lambda^M_1|^2 = \beta \quad \Rightarrow \quad \rho(M_i) = \sqrt{\beta}.
$$
:::
::::

. . .

Ключевое наблюдение: в комплексном режиме $\rho(M_i) = \sqrt{\beta}$ **не зависит от** $\lambda_i$. Значит, если мы подберём $\alpha, \beta$ так, чтобы все $M_i$ были в комплексном режиме, то $\rho(M) = \sqrt{\beta}$ для всех $i$ одновременно.

## Вывод оптимальных гиперпараметров [2/3]

Условие $\rho(M_i) = \sqrt{\beta}$ (комплексные с.з. или кратный вещественный корень) для всех $\lambda_i \in [\mu, L]$:
$$
(1 + \beta - \alpha \lambda_i)^2 \leq 4\beta \quad \Leftrightarrow \quad (1 - \sqrt{\beta})^2 \leq \alpha \lambda_i \leq (1 + \sqrt{\beta})^2.
$$

. . .

Заметим, что при $\Delta_i = 0$ собственное значение кратное и равно $\frac{1+\beta-\alpha\lambda_i}{2}$, а его квадрат равен $\beta$ (т. Виета), так что $\rho(M_i) = \sqrt{\beta}$ и в граничном случае. Для выполнения при **всех** $\lambda_i \in [\mu, L]$ необходимо и достаточно:
$$
(1 - \sqrt{\beta})^2 \leq \alpha \mu \quad \text{и} \quad \alpha L \leq (1 + \sqrt{\beta})^2.
$$

. . .

Минимизируем $\rho(M) = \sqrt{\beta}$, т.е. минимизируем $\beta$ при условии существования $\alpha$. Из двух неравенств выше получаем необходимое условие на $\alpha$:
$$
\frac{(1 - \sqrt{\beta})^2}{\mu} \leq \alpha \leq \frac{(1 + \sqrt{\beta})^2}{L}.
$$

. . .

Такое $\alpha$ существует тогда и только тогда, когда $\frac{(1-\sqrt{\beta})^2}{\mu} \leq \frac{(1+\sqrt{\beta})^2}{L}$, т.е. $\varkappa \leq \frac{(1+\sqrt{\beta})^2}{(1-\sqrt{\beta})^2}$. Правая часть монотонно возрастает по $\beta$, поэтому минимальное $\beta$ достигается, когда неравенство обращается в равенство:
$$
\alpha \mu = (1 - \sqrt{\beta})^2, \qquad \alpha L = (1 + \sqrt{\beta})^2.
$$

. . .

Разделим второе на первое:
$$
\frac{L}{\mu} = \varkappa = \frac{(1 + \sqrt{\beta})^2}{(1 - \sqrt{\beta})^2} \quad \Rightarrow \quad \sqrt{\varkappa} = \frac{1 + \sqrt{\beta}}{1 - \sqrt{\beta}} \quad \Rightarrow \quad \boxed{\beta^* = \left(\frac{\sqrt{\varkappa} - 1}{\sqrt{\varkappa} + 1}\right)^2 = \left(\frac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}\right)^2}
$$

## Вывод оптимальных гиперпараметров [3/3]

Подставим $\beta^*$ в $\alpha L = (1 + \sqrt{\beta^*})^2$:
$$
\alpha^* = \frac{(1 + \sqrt{\beta^*})^2}{L} = \frac{\left(1 + \frac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}\right)^2}{L} = \frac{\left(\frac{2\sqrt{L}}{\sqrt{L} + \sqrt{\mu}}\right)^2}{L} = \boxed{\frac{4}{(\sqrt{L} + \sqrt{\mu})^2}}
$$

. . .

Итого, оптимальная скорость сходимости метода тяжёлого шарика для квадратичной задачи:
$$
\rho(M) = \sqrt{\beta^*} = \frac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}} = \frac{\sqrt{\varkappa} - 1}{\sqrt{\varkappa} + 1}
$$

## Сходимость метода тяжёлого шарика для квадратичной функции

Мы можем явно вычислить собственные значения $M_i$:

$$
\lambda^M_1, \lambda^M_2 = \lambda \left( \begin{bmatrix} 
1 - \alpha \lambda_i + \beta & -\beta \\
1 & 0
\end{bmatrix}\right) = \dfrac{1+\beta - \alpha \lambda_i \pm \sqrt{(1+\beta - \alpha\lambda_i)^2 - 4\beta}}{2}.
$$

. . .

Когда $\alpha$ и $\beta$ оптимальны ($\alpha^*, \beta^*$), собственные значения являются комплексно-сопряженной парой $(1+\beta - \alpha\lambda_i)^2 - 4\beta \leq 0$, т.е. $\beta \geq (1 - \sqrt{\alpha \lambda_i})^2$.

. . .

$$
\text{Re}(\lambda^M) = \dfrac{L + \mu - 2\lambda_i}{(\sqrt{L} + \sqrt{\mu})^2}, \quad \text{Im}(\lambda^M) = \dfrac{\pm 2\sqrt{(L - \lambda_i)(\lambda_i - \mu)}}{(\sqrt{L} + \sqrt{\mu})^2}, \quad \lvert \lambda^M \rvert = \dfrac{L - \mu}{(\sqrt{L} + \sqrt{\mu})^2}.
$$

. . .

И скорость сходимости не зависит от шага и равна $\sqrt{\beta^*}$.








## Сходимость метода тяжёлого шарика для квадратичной функции

:::{.callout-theorem}
Предположим, что $f$ является $\mu$-сильно выпуклой и $L$-гладкой квадратичной функцией. Тогда метод тяжёлого шарика с параметрами
$$
\alpha = \dfrac{4}{(\sqrt{L} + \sqrt{\mu})^2}, \beta = \left(\dfrac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}\right)^2
$$

сходится линейно:

$$
\|x_k - x^*\|_2 \leq \left( \dfrac{\sqrt{\varkappa} - 1}{\sqrt{\varkappa} + 1} \right)^k \|x_0 - x^*\|
$$

:::

## Глобальная сходимость метода тяжёлого шарика ^[[Глобальная сходимость метода тяжёлого шарика для выпуклой оптимизации, Euhanna Ghadimi et al.](https://arxiv.org/abs/1412.7457)]

:::{.callout-theorem}
Предположим, что $f$ является гладкой и выпуклой и что

$$
\beta\in[0,1),\quad \alpha\in\biggl(0,\dfrac{2(1-\beta)}{L}\biggr).
$$

Тогда последовательность $\{x_k\}$, генерируемая итерациями тяжёлого шарика, удовлетворяет

$$
f(\overline{x}_T)-f^{\star} \leq  \left\{
\begin{array}[l]{ll}
\frac{\Vert x_{0}-x^\star\Vert^2}{2(T+1)}\biggl(\frac{L\beta}{1-\beta}+\frac{1-\beta}{\alpha}\biggr),\;\;\textup{if}\;\;
\alpha\in\bigl(0,\dfrac{1-\beta}{L}\bigr],\\
\frac{\Vert x_{0}-x^\star\Vert^2}{2(T+1)(2(1-\beta)-\alpha L)}\biggl({L\beta}+\frac{(1-\beta)^2}{\alpha}\biggr),\;\;\textup{if}\;\;
\alpha\in\bigl[\dfrac{1-\beta}{L},\dfrac{2(1-\beta)}{L}\bigr),
\end{array}
\right.
$$

где $\overline{x}_T$ среднее Чезаро последовательности итераций, т.е. 

$$
\overline{x}_T = \frac{1}{T+1}\sum_{k=0}^T x_k.
$$
:::


## Глобальная сходимость метода тяжёлого шарика ^[[Глобальная сходимость метода тяжёлого шарика для выпуклой оптимизации, Euhanna Ghadimi et al.](https://arxiv.org/abs/1412.7457)]

:::{.callout-theorem}
Предположим, что $f$ является гладкой и сильно выпуклой и что

$$
\alpha\in\biggl(0,\dfrac{2}{L}\biggr),\quad 0\leq  \beta<\dfrac{1}{2}\biggl( \dfrac{\mu \alpha}{2}+\sqrt{\dfrac{\mu^2\alpha^2}{4}+4(1-\frac{\alpha L}{2})} \biggr) .
$$

Тогда последовательность $\{x_k\}$, генерируемая итерациями методатяжёлого шарика, сходится линейно к единственному оптимальному решению $x^\star$. В частности,

$$
f(x_{k})-f^\star \leq q^k (f(x_0)-f^\star),
$$

где $q\in[0,1)$.
:::

## Итоги по методу тяжёлого шарика

* Обеспечивает ускоренную сходимость для сильно выпуклых квадратичных задач.
* Локально ускоренная сходимость была доказана в оригинальной статье.
* Недавно ^[[Provable non-accelerations of the heavy-ball method](https://arxiv.org/pdf/2307.11291)] было доказано, что глобального ускорения сходимости для метода не существует.
* Метод не был чрезвычайно популярен до ML-бума.
* Сейчас он фактически является стандартом для практического ускорения методов градиентного спуска, в том числе для невыпуклых задач (обучение нейронных сетей).

# Ускоренный градиентный метод Нестерова

## Концепция ускоренного градиентного метода Нестерова

:::: {.columns}

::: {.column width="27%"}
$$
x_{k+1} = x_k - \alpha \nabla f(x_k)
$$
:::
::: {.column width="34%"}
$$
x_{k+1} = x_k - \alpha \nabla f(x_k) + \beta (x_k - x_{k-1})
$$
:::
::: {.column width="39%"}
$$
\begin{cases}y_{k+1} = x_k + \beta (x_k - x_{k-1}) \\ x_{k+1} = y_{k+1} - \alpha \nabla f(y_{k+1}) \end{cases}
$$
:::

::::

. . .

:::: {.columns}
::: {.column width="67%"}

Давайте определим следующие обозначения

$$
\begin{aligned}
x^+ &= x - \alpha \nabla f(x) \qquad &\text{Градиентный шаг} \\
d_k &= \beta_k (x_k - x_{k-1}) \qquad &\text{Импульс}
\end{aligned}
$$

Тогда мы можем записать:


$$
\begin{aligned}
x_{k+1} &= x_k^+ \qquad &\text{Градиентный спуск} \\
x_{k+1} &= x_k^+ + d_k \qquad &\text{Метод тяжёлого шарика} \\
x_{k+1} &= (x_k + d_k)^+ \qquad &\text{Ускоренный градиентный метод Нестерова}
\end{aligned}
$$
:::
::: {.column width="33%"}

![](AGD.pdf)

:::
::::
## Сходимость для выпуклых функций

:::{.callout-theorem}
Предположим, что $f : \mathbb{R}^n \rightarrow \mathbb{R}$ является выпуклой и $L$-гладкой. Ускоренный градиентный метод Нестерова (NAG) предназначен для решения задачи минимизации, начиная с начальной точки $x_0 = y_0 \in \mathbb{R}^n$ и $\lambda_0 = 0$. Алгоритм выполняет следующие шаги:
$$
\begin{aligned}
&\textbf{Обновление градиента: } &x_{k+1} &= y_k - \frac{1}{L} \nabla f(y_k) \\
& \textbf{Вес экстраполяции: } &\lambda_{k+1} &= \frac{1 + \sqrt{1 + 4\lambda_k^2}}{2} \\
& \quad &\gamma_k &= \frac{\lambda_k - 1}{\lambda_{k+1}} \\
&\textbf{Экстраполяция: } &y_{k+1} &= x_{k+1} + \gamma_k\left(x_{k+1} - x_k\right)
\end{aligned}
$$
Последовательность $\{f(x_k)\}_{k\in\mathbb{N}}$, генерируемая алгоритмом, сходится к оптимальному значению $f^*$ со скоростью $\mathcal{O}\left(\frac{1}{k^2}\right)$, в частности:
$$
f(x_k) - f^* \leq \frac{2L \|x_0 - x^*\|^2}{k^2}
$$
:::

## Ускоренная сходимость для сильно выпуклых функций

:::{.callout-theorem}
Предположим, что $f : \mathbb{R}^n \rightarrow \mathbb{R}$ является $\mu$-сильно выпуклой и $L$-гладкой. Ускоренный градиентный метод Нестерова (NAG) предназначен для решения задачи минимизации, начиная с начальной точки $x_0 = y_0 \in \mathbb{R}^n$ и $\lambda_0 = 0$. Алгоритм выполняет следующие шаги:
$$
\begin{aligned}
&\textbf{Обновление градиента: } &x_{k+1} &= y_k - \frac{1}{L} \nabla f(y_k) \\
&\textbf{Экстраполяция: } &y_{k+1} &= x_{k+1} -  \gamma \left(x_{k+1} - x_k\right) \\
&\textbf{Вес экстраполяции: } &\gamma &= \frac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}
\end{aligned}
$$
Последовательность $\{f(x_k)\}_{k\in\mathbb{N}}$, генерируемая алгоритмом, сходится к оптимальному значению $f^*$ линейно:
$$
f(x_k) - f^* \leq \frac{\mu + L}{2}\|x_0 - x^*\|^2_2 \exp \left(-\frac{k}{\sqrt{\varkappa}}\right)
$$
:::

# Численные эксперименты

## Выпуклая квадратичная задача (линейная регрессия)

![](agd_random_0_10_60.pdf)

## Сильно выпуклая квадратичная задача (регуляризованная линейная регрессия)

![](agd_random_1_10_60.pdf)

## Сильно выпуклая квадратичная задача (регуляризованная линейная регрессия)

![](agd_random_1_1000_60.pdf)

## Сильно выпуклая квадратичная задача (регуляризованная линейная регрессия)

![](agd_random_1_1000_1000.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.1.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.2.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.3.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.4.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.5.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.6.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.7.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.8.pdf) 

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.9.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.95.pdf)

## Выпуклая бинарная логистическая регрессия

![](agd_convex_logreg_beta_0.99.pdf)

## Сильно выпуклая бинарная логистическая регрессия

![](agd_strongly_convex_logreg_0.25.pdf)

## Сильно выпуклая бинарная логистическая регрессия

![](agd_strongly_convex_logreg_0.5.pdf)

## Сильно выпуклая бинарная логистическая регрессия

![](agd_strongly_convex_logreg_0.7.pdf)

## Сильно выпуклая бинарная логистическая регрессия

![](agd_strongly_convex_logreg_0.9.pdf)
