---
lang: ru
format:
    pdf:
        pdf-engine: xelatex
        include-in-header: ../files/docheader.tex  # Custom LaTeX commands and preamble
---

# Определения и формулировки
1.  Положительно определенная матрица.

    :::{.callout-tip appearance="simple"}
    Матрица $A\in \mathbb{S}^n$ называется положительно (отрицательно) определённой, если для всех $x \in \mathbb{R}^n$, $x \neq 0$ выполнено $x^\top A x > 0$ (соотв. $<0$). Обозначение: $A \succ 0\; (A \prec 0)$.

    Аналогично определяется полуопределённость, только там неравенства нестрогие.
    :::
1.  Евклидова норма вектора.

    :::{.callout-tip appearance="simple"}
    $$
    \|x\|_2 = \sqrt{\sum_{i=1}^{n}|x_i|^2}
    $$

    Данная норма соответствует расстоянию в реальном мире. Иначе называется 2-норма (см. $p$-норма вектора)
    :::
1.  Неравенство треугольника для нормы.

    :::{.callout-tip appearance="simple"}
    Норма должна удовлетворять следующим свойствам:

    1. $\|\alpha x\| = |\alpha| \|x\|$, $\alpha \in \mathbb{R}$

    2. $\|x\|=0 \; \iff \; x=0$

    3. $\|x+y\|\leq \|x\|+\|y\|$ -- неравенство треугольника
    :::
1.  $p$-норма вектора.

    :::{.callout-tip appearance="simple"}
    $$
    \|x\|_p = \left(\sum_{i=1}^{n}|x_i|^{p}\right)^{\frac{1}{p}}
    $$

    Важные частные случаи:

    - Норма Чебышёва:
    $\|x\|_{\infty} = \max\limits_{i} |x_i|$

    - Манхэттенское расстояние или $\ell_1$-норма:
    $\|x\|_{1} = \sum_{i=1}^{n}|x_i|$
    :::
1.  Как выглядит единичный шар в $p$-норме на плоскости для $p=1,2,\infty$?

    :::{.callout-tip appearance="simple"}
    ![Шары в разных нормах](p_balls.pdf)
    :::
1.  Норма Фробениуса для матрицы.

    :::{.callout-tip appearance="simple"}
    $$
    \|A\|_{F} = \left(\sum\limits_{i = 1}^{m}\sum\limits_{j = 1}^{n} |a_{ij}|^2\right)^{\frac{1}{2}}
    $$
    :::
1.  Спектральная норма матрицы.

    :::{.callout-tip appearance="simple"}
    $$
    \|A\|_2 = \sup_{x \neq 0} \frac{\|Ax\|_2}{\|x\|_2} = \sigma_1 (A) = \sqrt{\lambda_{max}(A^\top A)}
    $$

    Где $\sigma_1 (A)$ -- старшее сингулярное значение $A$, $\lambda_{max}(A^\top A)$ -- наибольшее собственное значение $A^\top A$.
    :::
1.  Скалярное произведение двух векторов.

    :::{.callout-tip appearance="simple"}
    Пусть $x, y \in \mathbb{R}^{n}$, тогда их скалярное произведение это

    $$
    \langle x, y \rangle = x^Ty = \sum\limits_{i=1}^{n} x_iy_i = y^Tx = \langle y, x \rangle
    $$
    :::
1.  Скалярное произведение двух матриц, согласованное с нормой Фробениуса.

    :::{.callout-tip appearance="simple"}
    Пусть $X, Y \in \mathbb{R}^{m \times n}$, тогда их скалярное произведение это

    $$
    \langle X, Y \rangle = \operatorname{tr}(X^TY) = \sum\limits_{i=1}^{m}\sum\limits_{j=1}^{n} X_{ij}Y_{ij}= \operatorname{tr}(Y^TX) = \langle Y, X \rangle
    $$

    Связь с нормой Фробениуса: $\langle X, X \rangle = \|X\|^2_F$
    :::
1.  Собственные значения матрицы. Спектр матрицы.

    :::{.callout-tip appearance="simple"}
    Скаляр $\lambda$ является собственным значением для матрицы $A$, если существует вектор $q$, такой что $Aq = \lambda q$. В таком случае ненулевой вектор $q$ называют собственным вектором.

    Спектр матрицы -- совокупность её собственных значений.
    :::
1.  Связь спектра матрицы и ее определенности.

    :::{.callout-tip appearance="simple"}
    Симметричная матрица положительно (неотрицательно) определена $\iff$ её спектр (все её собственные значения) положителен (неотрицателен).
    :::
1.  Спектральное разложение матрицы.

    :::{.callout-tip appearance="simple"}
    Спектральное разложение матрицы, или разложение матрицы на основе собственных векторов, — это представление квадратной матрицы $A$ в виде произведения трёх матриц $A=S\Lambda S^{-1}$, где $S$ — матрица, столбцы которой являются собственными векторами матрицы $A$, $\Lambda$ — диагональная матрица с соответствующими собственными значениями на главной диагонали. В таком виде могут быть представлены только матрицы, обладающие полным набором собственных векторов.

    ![Спектральное разложение матрицы](Spectral.pdf)
    :::
1.  Сингулярное разложение матрицы.

    :::{.callout-tip appearance="simple"}
    $A \in \mathbb{R}^{m \times n}$ , $\operatorname{rank}(A) = r$.
    $$
    A = U\Sigma V^T
    $$
    $U \in \mathbb{R}^{m \times r}$, $U^T U = I$, $V \in \mathbb{R}^{n \times r}$, $V^T V = I$, $\Sigma$ диагональная матрица
    $$
    \Sigma = \operatorname{diag}(\sigma_1, \ldots, \sigma_r)
    $$
    такая что
    $$
    \sigma_1 \ge \sigma_2 \ge \ldots \ge \sigma_r > 0
    $$
    Столбцы $U$, $V$ - левые и правые сингулярные векторы $A$, $\sigma_i$ - сингулярные значения.

    $$
    A = \sum_{i=1}^r \sigma_i u_i v_i^T
    $$

    ![Сингулярное разложение матрицы](SVD.pdf)
    :::
1.  Связь определителя и собственных чисел для квадратной матрицы.

    :::{.callout-tip appearance="simple"}
    Если у матрицы $A$ собственные значения $\lambda_1, \lambda_2, \ldots, \lambda_n$, то её определитель равен:

    $$
    \det(A) = \lambda_1 \cdot \lambda_2 \cdot \ldots \cdot \lambda_n
    $$
    :::
1.  Связь следа и собственных чисел для квадратной матрицы.

    :::{.callout-tip appearance="simple"}
    Если у матрицы $A$ собственные значения $\lambda_1, \lambda_2, \ldots, \lambda_n$, то её след равен:

    $$
    \operatorname{tr}(A) = \lambda_1 + \lambda_2 + \ldots + \lambda_n
    $$
    :::
1.  Градиент функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    $\nabla f(x)$, вектор частных производных функции $f$.
    :::
1.  Гессиан функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    $$
    f''(x) = \nabla^2 f(x) =\left[\frac{\partial^2 f}{\partial x_i\partial x_j}(x)\right]_{i,j=1}^n = \begin{pmatrix}
        \frac{\partial^2 f}{\partial x_1 \partial x_1} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \dots  & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
        \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2 \partial x_2} & \dots  & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \dots  & \frac{\partial^2 f}{\partial x_n \partial x_n}
    \end{pmatrix}
    $$
    :::
1.  Якобиан функции $f(x): \mathbb{R}^n \to \mathbb{R}^m$.

    :::{.callout-tip appearance="simple"}
    $$
    J_f(x) = \begin{pmatrix}
    \dfrac{\partial f_1}{\partial x_1} & \dots & \dfrac{\partial f_1}{\partial x_n} \\
    \vdots & \ddots & \vdots \\
    \dfrac{\partial f_m}{\partial x_1} & \dots & \dfrac{\partial f_m}{\partial x_n}
    \end{pmatrix}.
    $$
    :::
1.  Формула для аппроксимации Тейлора первого порядка $f^I_{x_0}(x)$ функции $f(x): \mathbb{R}^n \to \mathbb{R}$ в точке $x_0$.

    :::{.callout-tip appearance="simple"}
    Для дифференцируемой f: 
    $$f_{x_0}^I(x) = f(x_0) + \nabla f(x_0)^T (x - x_0)$$
    :::
1.  Формула для аппроксимации Тейлора второго порядка $f^{II}_{x_0}(x)$ функции $f(x): \mathbb{R}^n \to \mathbb{R}$ в точке $x_0$.

    :::{.callout-tip appearance="simple"}
    Для дважды дифференцируемой f: 
    $$
    f_{x_0}^{II}(x) = f(x_0) + \nabla f(x_0)^T (x - x_0) + \frac{1}{2} (x - x_0)^T \nabla^2 f(x_0) (x - x_0)
    $$
    :::
1.  Определение дифференцируемости функции в точке через производную как линейный оператор.

    :::{.callout-tip appearance="simple"}
    Пусть $x \in S$ - внутренняя точка множества $S$, и пусть $D : U \rightarrow V$ - линейный оператор. Функция $f$ называется дифференцируемой в точке $x$ с производной $D$ если для всех достаточно малых $h \in U$ верно следующее: 
    $$ 
    f(x + h) = f(x) + D[h] + o(\|h\|)
    $$
    Если для любого линейного оператора $D : U \rightarrow V$ функция $f$ не является дифференцируемой в точке $x$ с производной $D$, тогда мы говорим, что $f$ не дифференцируема в точке $x$.
    :::
1.  Связь дифференциала функции $df$ и градиента $\nabla f$ для функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    $$
    df(x) = \langle \nabla f(x), dx\rangle
    $$
    :::
1.  Связь второго дифференциала функции $d^2f$ и гессиана $\nabla^2 f$ для функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    $$ d(df) = d^2f(x) = \langle \nabla^2 f(x) dx_1, dx\rangle = \langle H_f(x) dx_1, dx\rangle
    $$
    :::
1.  Формула для приближенного вычисления производной функции $f(x): \mathbb{R}^n \to \mathbb{R}$ по $k$-ой координате с помощью метода конечных разностей.

    :::{.callout-tip appearance="simple"}
    $$
    \dfrac{\partial f}{\partial x_k} (x) \approx \dfrac{f(x+\varepsilon e_k) - f(x)}{\varepsilon}, \quad e_k = (0, \ldots, \underset{{\tiny k}}{1}, \ldots, 0)
    $$

    Время работы: $(d+1)T$, где вызов $f(x)$ занимает $T$, $x \in \mathbb{R}^d$
    :::
1.  Пусть $f = f(x_1(t), \ldots, x_n(t))$. Формула для вычисления $\frac{\partial f}{\partial t}$ через $\frac{\partial x_i}{\partial t}$ (Forward chain rule).

    :::{.callout-tip appearance="simple"}
    $$
    \dfrac{\partial f}{\partial t} = \sum_{i = 1}^{n}\dfrac{\partial f}{\partial x_i}\dfrac{\partial x_i}{\partial t}
    $$
    :::
1.  Пусть $L$ - функция, возвращающая скаляр, а $v_k$ - функция, возвращающая вектор $x \in \mathbb{R}^t$. Формула для вычисления $\frac{\partial L}{\partial v_k}$ через $\frac{\partial L}{\partial x_i}$ (Backward chain rule).

    :::{.callout-tip appearance="simple"}
    $$
    \dfrac{\partial L}{\partial v_k} = \sum_{i = 1}^{t} \dfrac{\partial L}{\partial x_i} \dfrac{\partial x_i}{\partial v_k}
    $$
    :::
1.  Афинное множество. Афинная комбинация. Афинная оболочка.

    :::{.callout-tip appearance="simple"}
    Множество $A$ называется аффинным если для любых $x_1$, $x_2$ из $A$ прямая, проходящая через $x_1$, $x_2$, тоже лежит в $A$. То есть:
    $$
    \forall \theta \in \R, \forall x_1, x_2 \in A : \theta x_1 + \left(1 - \theta \right)x_2 \in A
    $$
    Пример аффинного множества: $\R^n$

    Пусть $x_1, x_2, \ldots, x_k \in S$. Тогда точка $\theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_k x_k$ называется аффинной комбинацией, если
    $$
    \forall i \in \{1, \ldots, k\} \colon \theta_i \in \R, \quad \sum_{i=1}^k \theta_i = 1
    $$

    Аффинная оболочка -- множество всех возможных аффинных комбинаций элементов множества.
    $$
    \text{aff}(S) = \left\{ \sum_{i=1}^{k}\theta_i x_i \bigg| k > 0, x_i \in S, \theta_i \in \R, \sum_{i=1}^{k} \theta_i = 1 \right\}
    $$

    :::
1.  Выпуклое множество. Выпуклая комбинация. Выпуклая оболочка.

    :::{.callout-tip appearance="simple"}
    Множество $S$ называется выпуклым если для любых $x_1$, $x_2$ из $S$ отрезок между $x_1$, $x_2$ тоже лежит в $S$. То есть:
    $$
    \forall \theta \in [0, 1], \, \forall x_1, x_2 \in S : \; \theta x_1 + \left(1 - \theta \right)x_2 \in S
    $$

    Пусть $x_1, x_2, \ldots, x_k \in S$. Тогда точка $\theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_k x_k$ называется выпуклой комбинацией, если
    $$
    \forall i \in \{1, \ldots, k\} \colon \; \theta_i \geq 0, \quad \sum_{i=1}^k \theta_i = 1
    $$

    Выпуклая оболочка -- множество всех возможных выпуклых комбинаций элементов множества.
    $$
    \text{conv}(S) = \left\{ \sum_{i=1}^{k}\theta_i x_i \, \bigg| \, k > 0, \, x_i \in S, \, \theta_i \geq 0, \, \sum_{i=1}^{k} \theta_i = 1 \right\}
    $$
    :::
1.  Конус. Выпуклый конус. Коническая комбинация. Коническая оболочка.

    :::{.callout-tip appearance="simple"}
    Множество $S$ называется конусом если для любого $x$ из $S$ луч, проходящий из 0 через $x$, тоже лежит в $S$. То есть:
    $$
    \forall \theta \geq 0, \, \forall x \in S : \; \theta x \in S
    $$

    Множество $S$ называется выпуклым конусом если для любых $x_1, x_2 \in S$ их коническая комбинация тоже лежит в $S$. То есть:

    $$
    \forall x_1, x_2 \in S, \, \theta_1, \theta_2 \geq 0 : \theta_1 x_1 + \theta_2 x_2 \in S
    $$

    Пусть $x_1, x_2, \ldots, x_k \in S$. Тогда точка $\theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_k x_k$ называется конической комбинацией, если
    $$
    \forall i \in \{1, \ldots, k\} \colon \; \theta_i \geq 0
    $$

    Коническая оболочка - множество всех возможных конических комбинаций элементов множества.
    $$
    \text{coni}(S) = \left\{ \sum_{i=1}^{k}\theta_i x_i \, \bigg| \, k > 0, \, x_i \in S, \, \theta_i \geq 0 \right\}
    $$

    :::
1.  Внутренность множества.

    :::{.callout-tip appearance="simple"}
    Внутренность множества - совокупность всех точек множества, содержащих вместе с собой в множестве некоторую окрестность вокруг себя.
    :::
1.  Относительная внутренность множества.

    :::{.callout-tip appearance="simple"}
    Относительная внутренность множества - внутренность множества в его аффинной оболочке. Может быть полезной при работе с множествами меньшей размерности чем пространство, в котором они находятся.
    $$
    \text{relint}(S) = \left\{ x \in S \, | \, \exists \varepsilon > 0, \; N_{\varepsilon}(x) \cap \text{aff}(S) \subseteq S \right\}
    $$
    $N_{\varepsilon}(x)$ -- шар радиуса $\varepsilon$ с центром в $x$, $\text{aff}(S)$ -- аффинная оболочка $S$

    Пример: отрезок на плоскости имеет пустую внутренность, но его относительная внутренность -- тот же отрезок без концов.

    ![](relative_interior.pdf)
    :::
1.  Сумма Минковского.

    :::{.callout-tip appearance="simple"}
    Сумма Минковского двух множеств $S_1, S_2$ — это множество
    $$
    S_1 + S_2 = \{ s_1 + s_2 \mid s_1 \in S_1,\; s_2 \in S_2 \}.
    $$

    ![Сумма Минковского двух множеств](minkowski.pdf){width=40%}

    :::
1.  Любые 2 операции с множествами, сохраняющие выпуклость.

    :::{.callout-tip appearance="simple"}
    1. Линейная комбинация:
    $$
    S = \left\{s \, | \, s = c_1 x + c_2 y, \; x \in S_x, \; y \in S_y, \; c_1, c_2 \in \R \right\}
    $$

    2. Пересечение любого числа выпуклых множеств

    3. Образ множества в аффинном преобразовании:
    $$
    S \subseteq \mathbb{R}^n \; \text{выпукло} \rightarrow f(S) = \left\{ f(x) | x \in S \right\} \; \text{выпукло} \quad \left( f(x) = Ax + b \right)
    $$
    :::
1.  Выпуклая функция.

    :::{.callout-tip appearance="simple"}
    Функция $f(x)$, определённая на выпуклом множестве $S \subseteq \mathbb{R}^n$ называется выпуклой на $S$ если:
    $$
    \forall x_1, x_2 \in S, \quad \forall \lambda \in [0, 1]
    $$
    $$
    f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)
    $$

    ![Иллюстрация выпуклой функции](convex_function_ru.pdf)
    
    :::
1.  Строго выпуклая функция.

    :::{.callout-tip appearance="simple"}
    Функция $f(x)$, определённая на выпуклом множестве $S \subseteq \R^n$ называется строго выпуклой на $S$ если:
    $$
    \forall x_1, x_2 \in S: x_1 \neq x_2, \quad \forall \lambda \in (0, 1)
    $$
    $$
    f(\lambda x_1 + (1 - \lambda) x_2) < \lambda f(x_1) + (1 - \lambda) f(x_2)
    $$
    :::
1.  Надграфик функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    Для функции, определённой на $S \subseteq \mathbb{R}^n$, множество:
    $$
    \text{epi}\; f = \left\{[x, \mu] \in S \times \mathbb{R} : f(x) \leq \mu \right\}
    $$
    называется надграфиком функции $f(x)$.

    ![Иллюстрация надграфика функции](epigraph.pdf){width=40% fig-align="center"}

    :::
1.  Множество подуровней функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

    :::{.callout-tip appearance="simple"}
    Для функции, определённой на $S \subseteq \R^n$, множество:
    $$
    \mathcal{L}_{\beta} = \left\{x \in S : f(x) \leq \beta \right\}
    $$
    называется множеством подуровней или множеством Лебега функции $f(x)$

    Если функция выпукла, то множество её подуровней выпукло. Обратное - не верно ($f(x) = \sqrt{|x|}$).

    ![Множество подуровней функции для выбранного уровня $\beta$](sublevel_set.pdf){width=40%}
    :::
1.  Дифференциальный критерий выпуклости первого порядка.

    :::{.callout-tip appearance="simple"}
    Дифференцируемая функция определенная на выпуклом множестве $S \subseteq \mathbb{R}^n$ выпукла тогда и только тогда когда $\forall x, y \in S$:
    $$f(y) \geq f(x) + \nabla f(x)^T (y - x)$$
    :::
1.  Дифференциальный критерий выпуклости второго порядка.

    :::{.callout-tip appearance="simple"}
    Дважды дифференцируемая функция определенная на выпуклом множестве $S \subseteq \mathbb{R}^n$ выпукла тогда и только тогда когда для любой внутренней точки x $\forall x \in \textbf{int}(S) \neq \emptyset$:
    $$\nabla^2f(x) \succeq 0$$
    :::
1.  Связь выпуклости функции и ее надграфика.

    :::{.callout-tip appearance="simple"}
    Функция выпукла тогда и только тогда, когда её надграфик - выпуклое множество.
    :::
1.  $\mu$-сильно выпуклая функция.

    :::{.callout-tip appearance="simple"}
    Функция определенная на выпуклом множестве $S \subseteq \mathbb{R}^n$ называется сильно выпуклой если $\forall x_1, x_2 \in S$, $ 0 \leq \lambda \leq 1$ и $\mu > 0$:

    $$f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2) - \frac{\mu}{2} \lambda(1 - \lambda) \|x_1 - x_2\|^2$$
    :::
1.  Дифференциальный критерий сильной выпуклости первого порядка.

    :::{.callout-tip appearance="simple"}
    Дифференцируемая функция определенная на выпуклом множестве $S \subseteq \mathbb{R}^n$ является сильно выпуклой тогда и только тогда, когда $\forall x, y  \in S$:

    $$f(y) \geq f(x) + \nabla f(x)^T (y - x) + \frac{\mu}{2} \| y - x\|^2$$
    :::
1.  Дифференциальный критерий сильной выпуклости второго порядка.

    :::{.callout-tip appearance="simple"}
    Дважды дифференцируемая функция определенная на выпуклом множестве $S \subseteq \mathbb{R}^n$ является сильно выпуклой тогда и только тогда, когда существует $\mu > 0$

    $$\nabla^2f(x) \succeq \mu I$$
    :::
1.  Любые 2 операции с функциями, сохраняющие выпуклость.

    :::{.callout-tip appearance="simple"}

    1. Сумма выпуклых функций с не отрицательными коэффициентами является выпуклой функцией.
    1. Композиция выпуклой функции с аффинной выпукла: $g(x) = f(Ax + b)$
    1. Поточечный максимум любого числа выпуклых функций есть выпуклая функция.
    :::
1.  Является ли задача линейных наименьших квадратов для переопределенной линейной системы выпуклой/сильно выпуклой?

    :::{.callout-tip appearance="simple"}
    Рассмотрим задачу минимизации функции:
    $$
    \| A x - b \|^2 \to \min_{x \in \mathbb{R}^d},
    $$
    где матрица $A \in \mathbb{R}^{m\times n}, \ b\in \mathbb{R}^{m}$, $m > n$ (стоячая). Легко заметить, что гессиан минимизируемой функции $A^T A$ - невырожденная матрица размера $n \times n$. Она является положительно определенной. То есть задача в классической постановке является сильно выпуклой. Т.е. содержит единственный локальный минимум (единственное решение).
    :::
1.  Является ли задача линейных наименьших квадратов для недоопределенной линейной системы выпуклой/сильно выпуклой?

    :::{.callout-tip appearance="simple"}
    Рассмотрим задачу минимизации функции:
    $$
    \| A x - b \|^2 \to \min_{x \in \mathbb{R}^d},
    $$
    где матрица $A \in \mathbb{R}^{m\times n}, \ b\in \mathbb{R}^{m}$, $m < n$ (лежачая). Легко заметить, что гессиан минимизируемой функции $A^T A$ - вырожденная матрица размера $n \times n$. Однако, она является положительно полуопределенной. То есть задача в классической постановке является выпуклой, но не сильно выпуклой. Т.е. содержит бесконечное количество локальных минимумов, каждый из которых - глобальный. Стоит отметить, что добавление $\ell_2$ регуляризации к минимизируемой функции изменит задачу, однако, будет гарантировать сильную выпуклость.
    :::
1.  Сопряженное множество.

    :::{.callout-tip appearance="simple"}
    Пусть $S \subseteq \mathbb{R}^n$ - произвольное непустое множество. Тогда его сопряженное множество определяется как:

    $$
    S^* = \{y \in \mathbb{R}^n \mid \langle y, x\rangle \ge -1 \;\; \forall x \in S\}
    $$
    :::
1.  Любые 2 нетривиальных свойства сопряженного множества.

    :::{.callout-tip appearance="simple"}

    1. Сопряженное множество всегда замкнуто, выпукло и содержит ноль.
    1. Для произвольного множества $S \subseteq \mathbb{R}^n$: $S^{**} = \overline{ \mathbf{conv} (S \cup \{0\}) }$
    1. Если $S_1 \subseteq S_2$, то $S_2^* \subseteq S_1^*$.
    1. $\left( \bigcup\limits_{i=1}^m S_i \right)^* = \bigcap\limits_{i=1}^m S_i^*$.
    1. Если $S$ замкнуто, выпукло и включает $0$, то $S^{**} = S$.
    1. $S^* = \left(\overline{S}\right)^*$.
    :::
1.  Сопряженный конус.

    :::{.callout-tip appearance="simple"}
    Сопряженным конусом к конусу $K$ называется множество $K^*$ такое, что: 

    $$
    K^* = \left\{ y \mid \langle x, y\rangle \ge 0 \quad \forall x \in K\right\}
    $$
    :::
1.  Сопряженная функция.

    :::{.callout-tip appearance="simple"}
    
    Для функции $f : \mathbb{R}^n \rightarrow \mathbb{R}$, сопряженной к ней называется функция $f^*$, причем область её определения можно считать теми $y$, для которых $\max$ конечен. 
    $$
    f^*(y) = \max_x \left[ y^T x - f(x)\right]
    $$ 

    ![Сопряженная функция](conj_function.pdf){width=40%}
    :::
1.  Связь сильной выпуклости функции и гладкости сопряженной функции.

    :::{.callout-tip appearance="simple"}
    Пусть $f$ - замкнутая и выпуклая. Тогда $f$ - сильно выпуклая с константой выпуклости $\mu \Leftrightarrow \nabla f^*$ - липшицев с параметром $\frac{1}{\mu}$.
    :::
1.  Сопряженная норма. Сопряженная норма к векторной $p$-норме.

    :::{.callout-tip appearance="simple"}
    Сопряжённой нормой $\|\cdot\|_*$ к норме $\|\cdot\|$ называется норма, определённая как:
    $$
    \|y\|_* = \sup_{\|x\| \leq 1} \langle y, x \rangle,
    $$
    где $x, y \in \mathbb{R}^n$.

    Для $p$-нормы сопряжённой является $q$-норма, где $p$ и $q$ связаны соотношением:
    $$
    \frac{1}{p} + \frac{1}{q} = 1, \quad p, q \geq 1.
    $$

    Например:
    - Для $p = 1$ сопряжённой является $q = \infty$.
    - Для $p = 2$ сопряжённая норма также является $2$-нормой.
    - Для $p = \infty$ сопряжённой является $q = 1$.
    :::
1.  Субградиент. Субдифференциал.

    :::{.callout-tip appearance="simple"}
    Субградиент функции $f$ в точке $x$ — это вектор $g$, удовлетворяющий условию:
    $$
    f(y) \geqslant f(x) + g^T (y - x), \quad \forall y.
    $$
    Множество всех субградиентов в точке $x$ называется субдифференциалом и обозначается как $\partial f(x)$.

    ![Субдифференциал функции ReLU.](subgradmod.pdf){width=50%}
    :::
1.  Нормальный конус.

    :::{.callout-tip appearance="simple"}
    Для $x \in S$, $\partial I_S(x) = \mathcal{N}_S(x)$, **нормальный конус** к $S$ в точке $x$, напомним.

    $$
    \mathcal{N}_S(x) = \{ g \in \mathbb{R}^n : g^T x \geq g^T y \text{ для любого } y \in S \}
    $$

    ![Нормальные конусы к множеству $S$ в точках](normal_cone.jpg){width=50%}
    :::
1.  Теорема Моро - Рокафеллара.

    :::{.callout-tip appearance="simple"}
    Пусть $f_i(x)$ — выпуклые функции, определённые на выпуклых множествах $S_i, \; i = \overline{1, n}$. Если выполнено условие $\bigcap\limits_{i=1}^n \mathbf{ri } (S_i) \neq \emptyset$, то функция $f(x) = \sum\limits_{i=1}^n a_i f_i(x), \; a_i > 0$ имеет субдифференциал $\partial_S f(x)$ на множестве $S = \bigcap\limits_{i=1}^n S_i$, и его можно выразить следующим образом:
    $$
    \partial_S f(x) = \sum\limits_{i=1}^n a_i \partial_{S_i} f_i(x).
    $$
    Это означает, что субдифференциал линейной комбинации выпуклых функций равен взвешенной сумме их субдифференциалов, взятых на пересечении соответствующих множеств.
    :::
1.  Теорема Дубовицкого - Милютина.

    :::{.callout-tip appearance="simple"}
    Пусть $f_i(x)$ — выпуклые функции, определённые на открытом выпуклом множестве $S \subseteq \mathbb{R}^n$, и $x_0 \in S$. Пусть $f(x)$ определяется как покоординатный максимум этих функций:
    $$
    f(x) = \underset{i}{\operatorname{max}} f_i(x).
    $$
    Тогда субдифференциал $f(x_0)$ выражается следующим образом:
    $$
    \partial_S f(x_0) = \mathbf{conv}\left\{ \bigcup\limits_{i \in I(x_0)} \partial_S f_i(x_0) \right\},
    $$
    где множество $I(x)$ определяется как индексы функций, достигающих максимума:
    $$
    I(x) = \{ i \in [1:m] : f_i(x) = f(x) \}.
    $$
    Это утверждение говорит, что субдифференциал максимума выпуклых функций представляет собой выпуклую оболочку объединения субдифференциалов тех функций, которые достигают максимума в данной точке.
    :::
1.  Теорема Вейерштрасса.

    :::{.callout-tip appearance="simple"}
    Пусть $S \subset \mathbb{R}^n$ - компакт, а $f(x)$ - непрерывная функция на $S$. 
    Значит, точка глобального минимума функции $f (x)$ на $S$ существует.
    :::
1.  Теорема Тейлора.

    :::{.callout-tip appearance="simple"}
    $f: \mathbb{R}^n \to \mathbb{R}$ - непрерывная, дифференцируемая функция и $p \in \mathbb{R}^n$, тогда теорема Тейлора гласит:
    $$f(x + p) = f(x) + \nabla f(x + tp)^T p$$
    Для некоторого $t\in (0, 1)$\\

    Более того, если f - дважды дифференцируема, то:

    $$f(x + p) = f(x) + \nabla f(x)^T p + \frac{1}{2} p^T \nabla^2f(x + tp) p$$
    Для некоторого $t\in (0, 1)$
    :::
1.  Необходимые условия локального экстремума.

    :::{.callout-tip appearance="simple"}
    Если $x^*$ - локальный экстремум и f непрерывно дифференцируема в открытой окрестности $x^*$, то:
    $$ \nabla f(x^*) = 0 $$
    :::
1.  Достаточные условия локального экстремума.

    :::{.callout-tip appearance="simple"}
    Если $\nabla^2 f$ непрерывна в открытой окрестности $x^*$ и 
    $$\nabla f(x^*) = 0 $$
    $$ \nabla^2 f(x^*) \succ 0$$

    То $x^*$ - локальный минимум $f(x)$.\\
    Для локального максимума аналогично, только
    $$0\succ  \nabla^2 f(x^*) $$
    :::
1.  Принцип Ферма для минимума функции.

    :::{.callout-tip appearance="simple"}
    Пусть $f : \mathbb{R}^n \to\mathbb{R} \cup \{\infty\}$, тогда $x^*$ является глобальным минимумом $f$ тогда и только тогда, когда
    $$
    0 \in \partial f(x^*)
    $$
    :::
1.  Общая задача математического программирования. Функция Лагранжа.

    :::{.callout-tip appearance="simple"}
    $$
    \left\{
            \begin{aligned}
                & f_0(x) \to \min_{x \in \mathbb{R}^d} \\
                & f_i(x) \leq 0, \quad i = 1, \dots, m, \\
                & h_i(x) = 0, \quad i = 1, \dots, p.
            \end{aligned}
        \right.
    $$
    Функция Лагранжа:
    $$
    L(x, \lambda, \nu) = f_0(x) + \sum_{i = 1}^{m} \lambda_i f_i(x) + \sum_{i = 1}^{p} \nu_i h_i(x),
    $$
    :::
1.  Теорема Каруша - Куна - Таккера в форме необходимых условий решения задачи математического программирования.

    :::{.callout-tip appearance="simple"}
    Пусть $x_*$ - решение задачи с нулевым зазором двойственности 
    $$
    \left\{
            \begin{aligned}
                & f_0(x) \to \min_{x \in \mathbb{R}^d} \\
                & f_i(x) \leq 0, \quad i = 1, \dots, m, \\
                & h_i(x) = 0, \quad i = 1, \dots, p.
            \end{aligned}
        \right.
    $$
    Функция Лагранжа:
    $$
    L(x, \lambda, \nu) = f_0(x) + \sum_{i = 1}^{m} \lambda_i f_i(x) + \sum_{i = 1}^{p} \nu_i h_i(x),
    $$
    Тогда найдутся такие векторы~$\lambda^*$ и~$\nu^*$, что выполнены условия
    $$
    \left\{
            \begin{aligned}
                & \nabla f_0(x_*)
                    +
                    \sum_{i = 1}^{m} \lambda_i^* \nabla f_i(x_*)
                    +
                    \sum_{i = 1}^{p} \nu_i^* \nabla h_i(x_*) = 0 \\
                & f_i(x_*) \leq 0, \quad i = 1, \dots m \\
                & h_i(x_*) = 0, \quad i = 1, \dots p \\
                & \lambda_i^* \geq 0, \quad i = 1, \dots m \\
                & \lambda_i^* f_i(x_*) = 0, \quad i = 1, \dots m\\
                %& (?) \, \forall y \in C(x^{*}) : \nabla^2_{xx}L(x^{*}, \lambda^{*} )y\rangle > 0
            \end{aligned}
        \right.
    $$
    :::
1.  Условие Слейтера.

    :::{.callout-tip appearance="simple"}
    1. Если задача выпуклая (т.е., говоря о задаче минимизации, оптимизируемая функция $f_0$ и ограничения вида неравенство $f_i$ -- выпуклые, ограничения вида равенства $h_i$ -- аффинные)
    1. И существует точка $x$ такая, что $h(x) = 0$ и $f_i(x) < 0$ (ограничения вида равенства активные, а ограничения вида неравенства выполняются строго)

    То тогда задача имеет нулевой зазор двойственности и условия ККТ становятся необходимыми и достаточными.
    :::
1.  Задача выпуклого программирования.

    :::{.callout-tip appearance="simple"}
    Задача выпуклого программирования — это задача оптимизации, в которой целевая функция является выпуклой функцией и область допустимых решений выпукла. В форме ниже функции $f_0, \ldots, f_m$ - выпуклые, а функции $h_i$ - аффинные.
    $$
    \left\{
            \begin{aligned}
                & f_0(x) \to \min_{x \in \mathbb{R}^d} \\
                & f_i(x) \leq 0, \quad i = 1, \dots, m, \\
                & h_i(x) = 0, \quad i = 1, \dots, p.
            \end{aligned}
        \right.
    $$

    :::
1.  Двойственная функция в задаче математического программирования.

    :::{.callout-tip appearance="simple"}
    Предположим, что $D = \bigcap\limits_{i=0}^m \textbf{dom}\,f_i \cap \bigcap\limits_{i=0}^p \textbf{dom}\,h_i$ непустое. Определим двойственную функцию $g : \mathbb{R}^{m}\times 
    \mathbb{R}^{p}\to \mathbb{R}$ как минимум лагранжиана по $x$
    : для $\lambda \in \mathbb{R}^m, \nu \in \mathbb{R}^p$ 
    $$
    g(\lambda, \nu) = \inf_{x\in D} L(x, \lambda, \nu) = \inf_{x\in D} f_0(x) +\sum^m_{i=1} \lambda_if_i(x) +\sum^p_{i=1}\nu_ih_i(x)
    $$
    Так как двойственная функция это поточечный инфинум семейства аффинных функций от $(\lambda, \nu)$, она вогнутая, даже если изначальная задача не выпуклая.
    :::
1.  Двойственная задача для задачи математического программирования.

    :::{.callout-tip appearance="simple"}
    Пусть $p^*$ - оптимальное значение исходной задачи. Пусть $\hat{x}$ достижимая точка для исходной задачи, т.е. $f_i(\hat{x}) \leq 0$ и $h_i(\hat{x}) = 0, \lambda \geq 0$. Тогда имеем:
    $$L(\hat{x}, \lambda, \nu) = f_0(\hat{x}) + \underbrace{\lambda^Tf(\hat{x})}_{\leq{0}}
    + \underbrace{\nu^T h(\hat{x})}_{=0}
    \leq f_0(\hat{x})$$
    Тогда
    $$g(\lambda, \nu) = \inf_{x\in D} L(x, \lambda, \nu) \leq L(\hat{x}, \lambda, \nu)\leq f_0(\hat{x})
    $$ $$g(\lambda, \nu) \leq p^{*}$$
    Двойственной задачей называется 
    $$g(\lambda, \nu) \to \max_{\lambda \in \mathbb{R}^m, \nu \in \mathbb{R}^p}$$
    $$s.t. \, \lambda \geq 0$$
    :::
1.  Сильная двойственность. Зазор двойственности.

    :::{.callout-tip appearance="simple"}
    Пусть $p^{*}$ - оптимальное значение прямой задачи, $d^{*}$ - оптимальное значение двойственной задачи. Зазором двойственности называется $$p^{*} - d^{*}\geq 0$$
    Сильная двойственность возникает, если зазор равен нулю
    $$p^{*} = d^{*}$$
    :::
1.  Локальный анализ чувствительности с помощью множителей Лагранжа.

    :::{.callout-tip appearance="simple"}
    Перейдем к возмущенной версии задачи:
    $$f_0(x) \to \min_x$$
    $$f_{i}(x)\leq u_i,\quad i=1,\dots ,m$$ 
    $$h_{i}(x)=v_i,\quad i=1,\dots ,p,$$
    Обозначим $p^*(u, v)$ - оптимальное значение этой задачи. Если имеет место сильная двойственность, то выполнено:
    $$p^*(u, v) \geq p^*(0, 0)-(\lambda^*)^T u-(\nu^*)^T v$$
    Если множители Лагранжа $\lambda_i^*, \nu_i^*$ большие, то небольшое изменение ограничений приведет к существенному изменению оптимального решения. То есть соответствующие ограничения очень сильно влияют на задачу.

    Если множители Лагранжа маленькие, то соответствующие ограничения мало влияют на задачу.
    $$
    \lambda_i^* = -\dfrac{\partial p^*(0,0)}{\partial u_i} \quad \nu_i^* = -\dfrac{\partial p^*(0,0)}{\partial v_i}
    $$
    :::
1.  Задача линейного программирования.

    :::{.callout-tip appearance="simple"}
    **Задача линейного программирования (ЛП)** — это задача оптимизации с **линейной** целевой функцией и **линейными** ограничениями.

    Общий вид (один из вариантов записи):
    $$
    \min_{x\in\mathbb{R}^n}\; c^\top x
    \quad \text{при}\quad
    Ax \le b,\; A_{eq}x=b_{eq}.
    $$
    Здесь $c\in\mathbb{R}^n$, матрицы $A, A_{eq}$ и векторы $b, b_{eq}$ — заданные.

    :::
1.  Стандартная форма задачи линейного программирования.

    :::{.callout-tip appearance="simple"}
    **Стандартная форма ЛП** (классическая для симплекс‑метода):
    $$
    \min_{x\in\mathbb{R}^n}\; c^\top x
    \quad \text{при}\quad
    Ax=b,\quad x\ge 0.
    $$

    (Иногда используют эквивалентную форму максимизации; различие лишь в знаках.)  
    Любая ЛП с неравенствами и/или свободными переменными сводится к стандартной форме введением добавочных переменных и разбиением свободных переменных на разность двух неотрицательных.
    :::
1.  Возможные случаи двойственности в задаче линейного программирования.

    :::{.callout-tip appearance="simple"}
    Двойственная задача:
    $$\max_{\nu \in \mathbb{R}^m}-b^T\nu$$
    $$s.t. -A^T\nu \leq c$$
    1. Если либо у прямой, либо у двойственной задачи есть конечное решение, то и у другой тоже, и целевые переменные равны.
    1. Если либо прямая, либо двойственная задача неограничена, то вторая из них невыполнима.
    :::
1.  Идея симплекс метода.

    :::{.callout-tip appearance="simple"}
    Симплекс метод решает следующую задачу:
    $$\min_{x \in \mathbb{R}^n } c^\top x$$
    $$s.t. Ax \leq b$$

    Шаги выполнения симплекс метода:

    1. Поиск начальной базисной допустимой точки
    1. Проверка оптимальности. Если решение оптимально, то алгоритм завершается, иначе переходим к следующему шагу
    1. Замена базиса
    1. Повторяем предыдущие два шага до достижения оптимального решения или установления, что задача не имеет допустимого решения
    :::
1.  Нахождение первоначальной угловой точки (идея).

    :::{.callout-tip appearance="simple"}
    Если очевидного допустимого базиса нет, применить **двухфазный метод**:
       - *Фаза 1*: добавить искусственные переменные и решить вспомогательную задачу, минимизируя сумму искусственных переменных, чтобы найти допустимую базисную точку (если минимум $>0$, исходная задача недопустима). Для этой задачи можно легко указать начальную базисную допустимую точку.
       - *Фаза 2*: стартовать симплекс‑метод для исходной целевой функции из найденного допустимого базиса.

    Это и даёт первоначальную угловую точку для запуска симплекс‑метода.
    :::
1.  Сходимость симплекс метода.

    :::{.callout-tip appearance="simple"}
    В худшем случае симплекс метод сходится экспоненциально от размерности задачи, но на практике в среднем алгоритм работает сильно лучше. Задача, на которой симплекс метод работает экспоненциальное время, называется примером Klee Minty.
    :::
1.  Теорема о связи задач max-flow и min-cut (надо суметь описать обе задачи).

    :::{.callout-tip appearance="simple"}
    **Задача Max-Flow (максимальный поток):**
    Дано ориентированное взвешенное графовое представление сети, где узлы — это вершины графа, а ребра имеют пропускную способность (capacity). Требуется найти максимальный поток из источника (source) в сток (sink), при условии:
    1. Поток на каждом ребре не превышает его пропускную способность.
    2. Сохраняется закон сохранения потока в промежуточных узлах (входящий поток равен исходящему, за исключением источника и стока).

    ![Транспортный граф](maxflow.pdf){fig-align="center" width=40%}

    **Задача Min-Cut (минимальный разрез):**
    Для той же сети требуется найти разрез — разделение вершин графа на два множества (одно включает источник, другое — сток), при котором суммарная пропускная способность ребер, пересекающих разрез, минимальна.

    ![](mincut1.pdf){width=44%} ![](mincut2.pdf){width=44%}

    **Теорема Max-Flow Min-Cut:**
    Максимальный поток из источника в сток равен минимальной пропускной способности разреза между источником и стоком.

    Формально:
    $$
    \text{MAXFLOW} = \text{MINCUT}.
    $$

    Эта теорема утверждает, что задачи поиска максимального потока и минимального разреза являются двойственными: решение одной задачи предоставляет решение другой.
    :::
1.  Линейная сходимость последовательности.

    :::{.callout-tip appearance="simple"}
    Последовательность $\{r_k\}$ называется линейно сходящейся, если существуют константы $C>0$ и $q\in(0,1)$, такие что
    $$
    r_k \le C q^k,\quad k\ge m.
    $$
    Наименьшее возможное $q$ называется константой (скоростью) линейной сходимости.
    :::
1.  Сублинейная сходимость последовательности.

    :::{.callout-tip appearance="simple"}
    Если последовательность $r_k$ сходится к нулю, но не обладает линейной сходимостью, то говорят, что она сходится сублинейно. Иногда мы можем рассматривать следующий класс сублинейной сходимости:
    $$
    r_k \leq Ck^q,
    $$

    где $q < 0$ и $0 < C < \infty$.
    :::
1.  Сверхлинейная сходимость последовательности.

    :::{.callout-tip appearance="simple"}
    Мы определяем сверхлинейную сходимость как сходимость последовательности, которая быстрее любой линейной сходимости. Иногда рассматривают более специальный класс. Тогда говорят, что последовательность $r_k > 0$ имеет сверхлинейную сходимость порядка $p$, если существуют $C > 0$ и $p > 1$ такие, что выполнено для всех достаточно больших $k$:
    $$
    r_{k+1} \leq C r_k^p
    $$
    :::
1.  Квадратичная сходимость последовательности.

    :::{.callout-tip appearance="simple"}
    Говорят, что последовательность $r_k > 0$ сходится квадратично, если существует $C > 0$ такое, что выполнено для всех достаточно больших $k$:
    $$
    r_{k+1} \leq C r_k^2
    $$
    :::
1.  Тест корней для определения скорости сходимости последовательности.

    :::{.callout-tip appearance="simple"}
    Пусть $(r_k)_{k=m}^\infty$ - последовательность неотрицательных чисел, сходящаяся к нулю, и пусть $\alpha := \limsup_{k \to \infty} r_k^{1/k}$. (Заметим, что $\alpha \ge 0$.)

    1. Если $0 \le \alpha < 1$, то $(r_k)_{k=m}^\infty$ сходится линейно с константой $\alpha$.
    1. В частности, если $\alpha = 0$, то $(r_k)_{k=m}^\infty$ сходится сверхлинейно.
    1. Если $\alpha = 1$, то $(r_k)_{k=m}^\infty$ сходится сублинейно.
    1. Случай $\alpha > 1$ невозможен.
    :::
1.  Тест отношений для определения скорости сходимости последовательности.

    :::{.callout-tip appearance="simple"}
    Пусть ${r_k}_{k=m}^\infty$ - последовательность строго положительных чисел, сходящаяся к нулю. Пусть
    $$
    q = \lim_{k \to \infty} \frac{r_{k+1}}{r_k}
    $$

    1. Если существует $q$ и $0 \le q < 1$, то ${r_k}_{k=m}^\infty$ имеет линейную сходимость с константой $q$.

    2. В частности, если $q = 0$, то ${r_k}_{k=m}^\infty$ имеет сверхлинейную сходимость.

    3. Если $q$ не существует, но $q = \lim_{k \to \infty} \sup_k \frac{r_{k+1}}{r_k} < 1$, то ${r_k}_{k=m}^\infty$ имеет линейную сходимость с константой, не превышающей $q$.

    4. Если $\lim_{k \to \infty} \inf_k \frac{r_{k+1}}{r_k} = 1$, то ${r_k}_{k=m}^\infty$ имеет сублинейную сходимость.

    5. Случай $\lim_{k \to \infty} \inf_k \frac{r_{k+1}}{r_k} > 1$ невозможен.
    :::
1.  Унимодальная функция.

    :::{.callout-tip appearance="simple"}
    Функция $f(x)$ называется унимодальной на $[a, b]$, если существует $x^* \in [a, b]$, такое, что

    1. $f(x_1) > f(x_2)$ для всех $a \le x_1 < x_2 < x^*$

    2. $f(x_1) < f(x_2)$ для всех $x^* < x_1 < x_2 \le b$
    :::
1.  Метод дихотомии.

    :::{.callout-tip appearance="simple"}
    Решаемая задача: $\min_{x \in [a, b]} f(x)$.
    В начале берётся середина отрезка и одна из четвертей (например, левая). Тогда по результатам измерения значений функции в этих двух точках решение может оказаться либо в левой половине отрезка, либо в правых трёх четвертях. Во втором случае нам необходимо провести дополнительное измерение значения функции в правой четверти исходного отрезка, тогда мы гарантируем уменьшение области поиска вдвое. Таким образом, в методе дихотомии мы гарантируем уменьшение области поиска вдвое на каждой итерации, однако, на каждой итерации (кроме самой первой) может потребоваться не более двух измерений функции. 
    :::
1.  Метод золотого сечения.

    :::{.callout-tip appearance="simple"}
    Решаемая задача: $\min_{x \in [a, b]} f(x)$. На отрезке выбираются правая и левая точки золотого сечения. По результатам измерения значений функции в этих двух точках решение может оказаться либо от начала до правой точки золотого сечения, либо от левой точки золотого сечения до конца отрезка. В любом из этих двух случаев на следующей итерации метода, одна из предыдущих измеренных точек золотого сечения станет точкой (левая на предыдущей итерации станет правой на новой или правая на предыдущей итерации станет левой на новой) золотого сечения отрезка на следующей итерации. Таким образом, нам останется доизмерить значение функции не более, чем один раз за итерацию.   
    :::
1.  Метод параболической интерполяции (без точных формул).

    :::{.callout-tip appearance="simple"}
    Идея метода: берем 3 точки, по этим 3 точкам однозначно строим параболу, находим ее минимум, и из этих 4 точек оставляем 3 так, чтобы между первой и третьей находился минимум.

    Сходится сверхлинейно, но метод довольно неустойчивый. Если $f(x)$ не похожа на параболу, нам конец. Если она обратна параболе, то мы и вовсе уйдём искать максимум.
    :::
1.  Условие достаточного убывания для неточного линейного поиска.

    :::{.callout-tip appearance="simple"}
    Неточный линейный поиск:

    $$
    x_{k+1} = x_k - \alpha \nabla f(x_k), \\
    \alpha = \arg\min_{\alpha\ge 0} f(x_k - \alpha \nabla f(x_k)).
    $$

    Хотим приближенно найти $\alpha$. Сведем задачу к поиску минимума следующей функции:

    $$
    \phi(\alpha) = f(x_k - \alpha\nabla f(x_k)), \alpha \geq 0
    $$

    Приблизим ее через первые 2 члена ряда Тейлора:

    $$
    \phi(\alpha) \approx f(x_k) - \alpha\nabla f(x_k)^\top \nabla f(x_k)
    $$

    Тогда условием достаточного убывания (Armijo condition) является:

    $$
    f(x_k - \alpha \nabla f (x_k)) \leq f(x_k) - c_1 \cdot \alpha\nabla f(x_k)^\top \nabla f(x_k), c_1 \in (0, 1)
    $$

    Иллюстрация для понимания:

    ![Иллюстрация условия достаточного убывания](sufficient%20decrease.pdf){width=40%}
    :::
1.  Условия Гольдштейна для неточного линейного поиска.

    :::{.callout-tip appearance="simple"}
    Определим $\phi_1$ и $\phi_2$ следующим образом ($0 < c_1 < c_2 < 1$)
    $$
    \begin{aligned}
    \phi_1(\alpha) &= f(x_k) - c_1 \alpha \|\nabla f(x_k)\|^2 \\
    \phi_2(\alpha) &= f(x_k) - c_2 \alpha \|\nabla f(x_k)\|^2
    \end{aligned}
    $$

    Тогда условие Гольдштейна заключается в том, что $\phi_2(\alpha)\ \le\ \phi(\alpha)\ \le\ \phi_1(\alpha)$.

    Иллюстрация для понимания:

    ![Иллюстрация условий Гольдштейна](Goldstein.pdf){width=40%}
    :::
1.  Условие ограничения на кривизну для неточного линейного поиска.

    :::{.callout-tip appearance="simple"}
    $$
    -\nabla f (x_k - \alpha \nabla f(x_k))^\top \nabla f(x_k) \geq c_2 \nabla f(x_k)^\top(- \nabla f(x_k)),
    $$
    где $c_2\in (c_1, 1)$, и $c_1$ взято из условия достаточного убывания.

    Иллюстрация для понимания:

    ![Иллюстрация условия ограничения на кривизну](Curvature.pdf){width=40%}
    :::
1.  Показать, что направление антиградиента - направление наискорейшего локального убывания функции.

    :::{.callout-tip appearance="simple"}
    Пусть $f$ дифференцируема, зададим искомое направление локального убывания - $h$ - $\| h \| = 1$.
    Тогда её аппроксимация: $f(x + \alpha h) = f(x) + \alpha \langle \nabla f(x), h \rangle + o(\alpha)$
    $$
    f(x + \alpha h) < f(x) \Rightarrow \alpha  \langle \nabla f(x), h \rangle + o(\alpha) < 0.
    $$
    При $\alpha \rightarrow +0$ получаем: $\alpha \langle \nabla f(x), h \rangle \leqslant 0$
    $$
    \| \langle \nabla f(x), h \rangle \| \leqslant \| \nabla f(x) \| \| h \| \leqslant \| \nabla f(x) \|
    $$
    $$
    \langle \nabla f(x), h \rangle \geqslant -\| \nabla f(x) \| \Rightarrow h = \frac{-\nabla f(x)}{\| \nabla f(x) \|}, \text{ ч.т.д.}
    $$
    :::
1.  Дифференциальное уравнение градиентного потока.

    :::{.callout-tip appearance="simple"}
    **Градиентный поток** (continuous-time аналог градиентного спуска) для гладкой функции $f:\mathbb{R}^n\to\mathbb{R}$ задаётся ОДУ
    $$
    \dot x(t)= -\nabla f(x(t)), \qquad x(0)=x_0.
    $$

    В евклидовой метрике это движение в направлении наискорейшего убывания.  
    По правилу цепочки вдоль траектории:
    $$
    \frac{d}{dt} f(x(t)) = \nabla f(x(t))^\top \dot x(t)= -\|\nabla f(x(t))\|_2^2 \le 0,
    $$
    т.е. значение $f$ не возрастает.
    :::
1.  Метод градиентного спуска.

    :::{.callout-tip appearance="simple"}
    Решаем задачу минимизации 
    $$
    f(x) \to \min_{x \in \mathbb{R}^d}
    $$
    Если $f$ дифференцируема, то тогда для решения этой задачи можно использовать метод градиентного спуска:
    $$
    x_{k + 1} = x_k - \alpha \nabla f(x_k)
    $$
    :::
1.  Наискорейший спуск.

    :::{.callout-tip appearance="simple"}
    Решаем задачу минимизации 
    $$
    f(x) \to \min_{x \in \mathbb{R}^d}
    $$
    Если $f$ дифференцируема, то тогда для решения этой задачи можно использовать метод наискорейшего спуска:
    $$
    x_{k + 1} = x_k - \alpha_k \nabla f(x_k)
    $$
    $$
    \alpha_k = \arg\min\limits_{\alpha \in \mathbb{R}^+} f(x_k - \alpha \nabla f(x_k)),
    $$
    т.е. выбираем наилучший шаг спуска на каждой итерации метода.
    :::
1.  Как направлены две соседние итерации метода наискорейшего спуска по отношению друг к другу?

    :::{.callout-tip appearance="simple"}
    Шаги между итерациями ортогональны друг другу.

    ![Несколько итераций наискорейшего спуска](Steepest_Descent.pdf)
    :::
1.  Липшицева парабола для гладкой функции.

    :::{.callout-tip appearance="simple"}
    Если $f: \mathbb{R}^n \rightarrow \mathbb{R}$ - непрерывно дифференцируема и градиент Липшицев с константой $L$, то $\forall x, y \in \mathbb{R}^n$:
    $$
    | f(y) - f(x) - \langle \nabla f(x), y - x \rangle | \leqslant \frac{L}{2}\| y - x \|^2
    $$
    Если зафиксируем $x_0 \in \mathbb{R}^n$, то:
    $$
    \varphi_1(x) = f(x_0) + \langle \nabla f(x_0), x - x_0 \rangle - \frac{L}{2}\| x - x_0 \|^2
    $$
    $$
    \varphi_2(x) = f(x_0) + \langle \nabla f(x_0), x - x_0 \rangle + \frac{L}{2}\| x - x_0 \|^2
    $$
    Это две параболы, и для них верно, что $\varphi_1(x) \leqslant f(x) \leqslant \varphi_2(x)$ $\forall x$

    ![Иллюстрация Липшицевых парабол, между которыми зажата гладкая функция. Чаще нас интересует мажорирующая из них.](lipschitz_parabola.pdf)
    :::
1.  Размер шага наискорейшего спуска для квадратичной функции.

    :::{.callout-tip appearance="simple"}
    Решаем задачу минимизации методом наискорейшего спуска
    $$
    f(x) = \frac{1}{2}x^TAx - b^Tx + c \to \min_{x \in \mathbb{R}^d}
    $$
    $$
    \nabla f = \frac{1}{2}(A + A^T)x - b
    $$

    Из условия $\nabla f(x_{k + 1})^T \nabla f(x_k) = 0$ получаем:
    $$
    \alpha_k = \frac{2 \nabla f(x_k)^T \nabla f(x_k)}{\nabla f(x_k)^T (A + A^T) \nabla f(x_k)} = \frac{\nabla f(x_k)^T \nabla f(x_k)}{\nabla f(x_k)^T \nabla^2 f(x_k) \nabla f(x_k)}. 
    $$
    :::
1.  Характер сходимости градиентного спуска к локальному экстремуму для гладких невыпуклых функций в терминах $\mathcal{O}$ от числа итераций метода.

    :::{.callout-note appearance="simple"}
    Пусть $f$ **$L$-гладкая** (градиент $L$-липшицев), и $f$ ограничена снизу: $f(x)\ge f_{\inf}$.  
    Рассмотрим градиентный спуск
    $$
    x_{k+1}=x_k-\alpha\,\nabla f(x_k), \qquad \alpha\in(0,1/L].
    $$

    Тогда выполняется **лемма о спуске**:
    $$
    f(x_{k+1})\le f(x_k)-\frac{\alpha}{2}\,\|\nabla f(x_k)\|_2^2 .
    $$
    Суммируя по $k=0,\dots,N-1$, получаем
    $$
    \min_{0\le k\le N-1}\|\nabla f(x_k)\|_2^2
    \le \frac{2\,(f(x_0)-f_{\inf})}{\alpha\,N}
    =\mathcal{O}\!\left(\frac{1}{N}\right).
    $$

    Следовательно, чтобы найти $\varepsilon$-стационарную точку ($\|\nabla f(x_k)\|_2\le \varepsilon$), достаточно
    $$
    N = \mathcal{O}\!\left(\frac{1}{\varepsilon^2}\right).
    $$

    В невыпуклом случае в общем виде гарантируется сходимость к **стационарной точке** (а не обязательно к локальному минимуму), при стандартных предположениях о шаге/линейном поиске и отсутствии вырождения.
    :::
1.  Характер сходимости градиентного спуска для гладких выпуклых функций в терминах $\mathcal{O}$ от числа итераций метода.

    :::{.callout-tip appearance="simple"}
    $f(x_k) - f^* \sim  \mathcal{O} \left( \frac{1}{k} \right).$
    :::
1.  Характер сходимости градиентного спуска для гладких и сильно выпуклых функций в терминах $\mathcal{O}$ от числа итераций метода.

    :::{.callout-tip appearance="simple"}
    $\|x_k - x^*\|^2 \sim \mathcal{O} \left( \left(1 - \frac{\mu}{L}\right)^k \right).$
    :::
1.  Связь спектра гессиана с константами сильной выпуклости и гладкости функции.

    :::{.callout-tip appearance="simple"}
    $\quad \mu = \min\limits_{x \in \text{dom} f}\lambda_{\min}(\nabla^2 f(x)), \quad L = \max\limits_{x \in \text{dom} f}\lambda_{\max}(\nabla^2 f(x)).$ 
    :::
1.  Условие Поляка-Лоясиевича (градиентного доминирования) для функций.

    :::{.callout-tip appearance="simple"}
    $\exists \mu > 0: \quad \| \nabla f(x) \|^2 \geqslant 2\mu(f(x) - f^*) \quad \forall x$,  где $f^*$ - минимум функции $f(x)$.
    :::
1.  Сходимость градиентного спуска для сильно выпуклых квадратичных функций. Оптимальные гиперпараметры.

    :::{.callout-tip appearance="simple"}
    Решаем задачу минимизации методом градиентного спуска. Пусть $A \in \mathbb{S}_{++}^n \Rightarrow \nabla f = Ax - b$.
    $$
    \begin{aligned}
    f(x) &= \frac{1}{2}x^TAx - b^Tx + c \to \min_{x \in \mathbb{R}^d} \\
    x_{k + 1} &= x_k - \alpha (Ax_k - b) \\
    \alpha_{opt} &= \frac{2}{\mu + L}, \text{ где } \mu = \lambda_{\min}(A), L = \lambda_{\max}(A) \\
    \kappa &= \frac{L}{\mu} \geqslant 1 \\
    \rho &= \frac{\kappa - 1}{\kappa + 1} \\
    \| x_k - x^* \| &\leqslant \rho^k \| x_0 - x^* \| \\
    \end{aligned}
    $$
    :::
1.  Связь PL-функций и сильно выпуклых функций.

    :::{.callout-tip appearance="simple"}
    Пусть $f$ $\mu$-сильно выпуклая и дифференцируемая $\Rightarrow f \in$ PL.
    
    Обратное неверно - $f(x) = x^2 + 3\sin^2x \in$ PL, но не сильно выпуклая (она вообще не выпуклая).

    ![Пример невыпуклой PL функции](pl_2d.pdf)
    :::
1.  Привести пример выпуклой, но не сильно выпуклой задачи линейных наименьших квадратов (возможно, с регуляризацией).

    :::{.callout-tip appearance="simple"}
    Рассмотрим задачу минимизации функции:
    $$
    \| A x - b \|^2 \to \min_{x \in \mathbb{R}^d},
    $$
    где матрица $A \in \mathbb{R}^{m\times n}, \ b\in \mathbb{R}^{m}$, $m < n$ (лежачая).
    :::
1.  Привести пример сильно выпуклой задачи линейных наименьших квадратов (возможно, с регуляризацией).

    :::{.callout-tip appearance="simple"}
    Рассмотрим задачу минимизации функции:
    $$
    f(x) = \|Ax - b\|_2^2,
    $$
    где $A \in \mathbb{R}^{n \times n}$ (ранг $A = n$). Эта функция сильно выпукла, так как гессиан положительно определен.
    :::
1.  Нижние оценки для гладкой выпуклой оптимизации с помощью методов первого порядка в терминах $\mathcal{O}$ от числа итераций метода.

    :::{.callout-tip appearance="simple"}

    $f(x_k) - f^* \sim  \mathcal{\Omega}\left(\frac{1}{k^2}\right)$  
    :::
1.  Нижние оценки для гладкой сильно выпуклой оптимизации с помощью методов первого порядка в терминах $\mathcal{O}$ от числа итераций метода.

    :::{.callout-tip appearance="simple"}

    $$f(x_k)-f^*=\Omega\!\left(\left(\tfrac{\sqrt{\varkappa}-1}{\sqrt{\varkappa}+1}\right)^{\!2k}\right)$$  
    :::
1.  Отличие ускоренной и неускоренной линейной сходимости для методов первого порядка.

    :::{.callout-tip appearance="simple"}
    | Функция | Неускоренная | Ускоренная |
    |:--------:|:-------:|:-------:|
    | Гладкая и сильно-выпуклая (или PL) |$\mathcal{O}\left(\varkappa \log\frac{1}{\varepsilon}\right)$| $\mathcal{O}\left(\sqrt{\varkappa} \log\frac{1}{\varepsilon}\right)$  |
    | Гладкая и выпуклая (в вопросе не требуется) |$\mathcal{O}\left(\frac{1}{\varepsilon}\right)$| $\mathcal{O}\left(\frac{1}{\sqrt{\varepsilon}}\right)$  |
    :::
1.  Метод тяжелого шарика (Поляка).

    :::{.callout-tip appearance="simple"}
    Задача: $f(x) \rightarrow \min\limits_{x \in \mathbb{R}^d}, \ f(x)$ - непрерывно дифференцируемая функция
    $$
    x_{k+1} = x_{k} - \alpha\nabla f(x_{k}) + \beta(x_k-x_{k-1}), \qquad 0 < \beta < 1.
    $$
    :::
1.  Понятие локальной и глобальной сходимости численного метода оптимизации.

    :::{.callout-tip appearance="simple"}
    Локальная сходимость означает, что последовательность итераций сходится к решению $x^*$ внутри некоторого множества. За пределами этого множества сходимости нет.
    Глобальная сходимость означает, что последовательность итераций сходится к решению $x^*$ вне зависимости от начального приближения.
    :::
1.  Ускоренный градиентный метод Нестерова для выпуклых гладких функций.

    :::{.callout-tip appearance="simple"}
    Предположим, что $f : \mathbb{R}^n \rightarrow \mathbb{R}$ является выпуклой и $L$-гладкой. Ускоренный градиентный метод Нестерова (NAG) предназначен для решения задачи минимизации, начиная с начальной точки $x_0 = y_0 \in \mathbb{R}^n$ и $\lambda_0 = 0$. Алгоритм выполняет следующие шаги:
    $$
    \begin{aligned}
    &\textbf{Обновление градиента: } &x_{k+1} &= y_k - \frac{1}{L} \nabla f(y_k) \\
    & \textbf{Вес экстраполяции: } &\lambda_{k+1} &= \frac{1 + \sqrt{1 + 4\lambda_k^2}}{2} \\
    & \quad &\gamma_k &= \frac{\lambda_k - 1}{\lambda_{k+1}} \\
    &\textbf{Экстраполяция: } &y_{k+1} &= x_{k+1} + \gamma_k\left(x_{k+1} - x_k\right)
    \end{aligned}
    $$
    Последовательность $\{f(x_k)\}_{k\in\mathbb{N}}$, генерируемая алгоритмом, сходится к оптимальному значению $f^*$ со скоростью $\mathcal{O}\left(\frac{1}{k^2}\right)$, в частности:
    $$
    f(x_k) - f^* \leq \frac{2L \|x_0 - x^*\|^2}{k^2}
    $$

    ![](AGD_horizontal.pdf)

    :::
1.  Ускоренный градиентный метод Нестерова для сильно выпуклых гладких функций.

    :::{.callout-tip appearance="simple"}
    Предположим, что $f : \mathbb{R}^n \rightarrow \mathbb{R}$ является $\mu$-сильно выпуклой и $L$-гладкой. Ускоренный градиентный метод Нестерова (NAG) предназначен для решения задачи минимизации, начиная с начальной точки $x_0 = y_0 \in \mathbb{R}^n$ и $\lambda_0 = 0$. Алгоритм выполняет следующие шаги:
    $$
    \begin{aligned}
    &\textbf{Обновление градиента: } &x_{k+1} &= y_k - \frac{1}{L} \nabla f(y_k) \\
    &\textbf{Экстраполяция: } &y_{k+1} &= x_{k+1} + \gamma \left(x_{k+1} - x_k\right) \\
    &\textbf{Вес экстраполяции: } &\gamma &= \frac{\sqrt{L} - \sqrt{\mu}}{\sqrt{L} + \sqrt{\mu}}
    \end{aligned}
    $$
    Последовательность $\{f(x_k)\}_{k\in\mathbb{N}}$, генерируемая алгоритмом, сходится к оптимальному значению $f^*$ линейно:
    $$
    f(x_k) - f^* \leq \frac{\mu + L}{2}\|x_0 - x^*\|^2_2 \exp \left(-\frac{k}{\sqrt{\varkappa}}\right)
    $$
    :::
1.  $A$-сопряженность двух векторов. $A$-ортогональность. Скалярное произведение $\langle \cdot, \cdot \rangle_A$.

    :::{.callout-tip appearance="simple"}
    $A$-ортогональность (сопряженность):
    $$
    x \perp_A y \iff x^T A y = 0.
    $$
    :::
1.  Процедура ортогонализации Грама-Шмидта.

    :::{.callout-tip appearance="simple"}
    **Классический Грам–Шмидт.** Даны линейно независимые векторы $a_1,\dots,a_m\in\mathbb{R}^n$.  
    Строим ортонормированный набор $q_1,\dots,q_m$:

    $$
    u_1=a_1,\qquad q_1=\frac{u_1}{\|u_1\|_2},
    $$
    $$
    u_k = a_k - \sum_{j=1}^{k-1} \langle q_j, a_k\rangle\, q_j, 
    \qquad 
    q_k = \frac{u_k}{\|u_k\|_2},\quad k=2,\dots,m.
    $$

    В результате $q_i^\top q_j=\delta_{ij}$ и $\operatorname{span}\{a_1,\dots,a_m\}=\operatorname{span}\{q_1,\dots,q_m\}$.  
    Для матрицы $A=[a_1\ \cdots\ a_m]$ получаем **QR-разложение**: $A=QR$, где $Q=[q_1\ \cdots\ q_m]$, а $R$ — верхнетреугольная, $R_{jk}=\langle q_j,a_k\rangle$.

    (На практике часто используют модифицированный Грам–Шмидт из‑за лучшей численной устойчивости.)
    :::
1.  Метод сопряженных направлений.

    :::{.callout-tip appearance="simple"}
    **Метод сопряжённых направлений** применяется для минимизации квадратичной функции
    $$
    f(x)=\tfrac12 x^\top A x - b^\top x,\qquad A\succ0.
    $$

    Векторы $p_0,\dots,p_{n-1}$ называются **$A$‑сопряжёнными** (conjugate), если
    $$
    p_i^\top A p_j = 0,\quad i\ne j.
    $$

    Итерации метода:
    $$
    x_{k+1}=x_k+\alpha_k p_k,
    \qquad 
    \alpha_k = -\frac{\nabla f(x_k)^\top p_k}{p_k^\top A p_k}
    =\frac{r_k^\top p_k}{p_k^\top A p_k},
    $$
    где $r_k=b-Ax_k=-\nabla f(x_k)$ (невязка).

    При точном одномерном поиске и выборе $n$ линейно независимых $A$‑сопряжённых направлений метод находит точный минимум за **не более чем $n$ шагов** (в точной арифметике), т.к. минимизация последовательно выполняется по взаимно $A$‑ортогональным направлениям.
    :::
1.  Метод сопряженных градиентов.

    :::{.callout-tip appearance="simple"}
    **Метод сопряжённых градиентов (CG)** — частный случай метода сопряжённых направлений, где направления строятся рекуррентно по градиентам.  
    Эквивалентные постановки:
    - решить СЛАУ $Ax=b$ при $A\succ0$;
    - минимизировать $f(x)=\tfrac12 x^\top A x-b^\top x$.

    Обозначим невязку $r_k=b-Ax_k=-\nabla f(x_k)$ и направление $p_k$.

    **Алгоритм (линейный CG):**
    $$
    r_0=b-Ax_0,\quad p_0=r_0;
    $$
    для $k=0,1,2,\dots$:
    $$
    \alpha_k = \frac{r_k^\top r_k}{p_k^\top A p_k},\qquad
    x_{k+1}=x_k+\alpha_k p_k,
    $$
    $$
    r_{k+1}=r_k-\alpha_k A p_k,
    $$
    $$
    \beta_k = \frac{r_{k+1}^\top r_{k+1}}{r_k^\top r_k},\qquad
    p_{k+1}=r_{k+1}+\beta_k p_k.
    $$

    Свойства (в точной арифметике): $p_k$ — $A$‑сопряжённые, $r_k$ попарно ортогональны, а $x_k$ минимизирует $f$ на аффинном подпространстве $x_0+\mathcal{K}_k(A,r_0)$ (крыловском подпространстве).
    :::
1.  Зависимость сходимости метода сопряженных градиентов от спектра матрицы.

    :::{.callout-tip appearance="simple"}
    Если матрица $A$ имеет только $r$ различных собственных чисел, тогда метод сопряжённых градиентов сходится за $r$ итераций.

    ![](cg_random_10_100_60.pdf)
    ![](cg_clustered_10_1000_60.pdf)

    :::
1.  Характер сходимости метода сопряженных градиентов в терминах $\mathcal{O}$ от числа итераций метода.

    :::{.callout-note appearance="simple"}
    Пусть $A\succ0$ и $\kappa=\dfrac{\lambda_{\max}(A)}{\lambda_{\min}(A)}$ — число обусловленности.  
    Для линейного CG выполняется оценка ошибки в $A$‑норме:
    $$
    \|x_k-x_*\|_A \le 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^k\,\|x_0-x_*\|_A,
    \qquad 
    \|v\|_A:=\sqrt{v^\top A v}.
    $$

    То есть сходимость **линейная (геометрическая)** с фактором
    $q=\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}<1$.  
    Для достижения относительной точности $\varepsilon$ достаточно
    $$
    k = \mathcal{O}\big(\sqrt{\kappa}\,\log(1/\varepsilon)\big).
    $$

    Кроме того, в точной арифметике метод завершится за $\le n$ итераций (не позднее размера задачи), т.к. строит решение в возрастающих крыловских подпространствах.
    :::
1.  Метод Флетчера-Ривза.

    :::{.callout-tip appearance="simple"}
    **Метод Флетчера–Ривза (FR)** — вариант *нелинейного* метода сопряжённых градиентов для минимизации гладкой функции $f$.

    Обозначим $g_k=\nabla f(x_k)$. Инициализация: $d_0=-g_0$.

    Итерация:
    $$
    x_{k+1}=x_k+\alpha_k d_k,
    $$
    где $\alpha_k$ выбирается одномерным поиском (обычно условия Вольфа/сильные Вольфа).

    Коэффициент:
    $$
    \beta_k^{\mathrm{FR}}=\frac{\|g_{k+1}\|_2^2}{\|g_k\|_2^2},
    $$
    направление:
    $$
    d_{k+1}=-g_{k+1}+\beta_k^{\mathrm{FR}} d_k.
    $$

    При квадратичной $f$ и точном линейном поиске FR совпадает с линейным CG.
    :::
1.  Метод Полака-Рибьера.

    :::{.callout-tip appearance="simple"}
    **Метод Полака–Рибьера (PR)** — ещё один вариант *нелинейного* сопряжённого градиента.

    Обозначим $g_k=\nabla f(x_k)$, $d_0=-g_0$, и делаем шаг
    $$
    x_{k+1}=x_k+\alpha_k d_k
    $$
    с одномерным поиском (условия Вольфа/сильные Вольфа).

    Коэффициент Полака–Рибьера:
    $$
    \beta_k^{\mathrm{PR}}=
    \frac{g_{k+1}^\top (g_{k+1}-g_k)}{\|g_k\|_2^2},
    $$
    направление:
    $$
    d_{k+1}=-g_{k+1}+\beta_k^{\mathrm{PR}} d_k.
    $$

    Часто используют модификацию **PR+**: $\beta_k=\max\{\beta_k^{\mathrm{PR}},0\}$, чтобы гарантировать спуск при стандартных условиях линейного поиска.
    :::


\newpage

# Теоремы с доказательствами
1.  Критерий положительной определенности матрицы через знаки собственных значений симметричной матрицы.

    :::{.callout-note appearance="simple"}
    $A \succeq (\succ) 0 \Longleftrightarrow$ все собственные значения симметричной матрицы $A \geq (>) 0$ 
    :::

    $\rightarrow$  Пусть некоторые собственные значения $\lambda$ отрицательны, и $x$ - соответствующий ему собственный вектор.  Тогда:

    $Ax = \lambda x, x^{T} A x \geq 0 \rightarrow x^{T} A x = \lambda x^T  x$, $x^T x \geq 0 \rightarrow \lambda \geq 0$ - противоречие

    $\leftarrow$ Помним, что положительная определённость задаётся для симметричных матриц. Для симметричной матрицы можем выбрать собственные векторы $v_i$, образующие ортогональный базис ($i\neq j: v_i^T v_j = 0$ - выкидываем часть слагаемых из суммы в доказательстве). Тогда для $x \in \mathbb{R}^{n}$

    $$x^T A x = (\alpha_1 v_1 + \dots + \alpha_{n} v_{n})^T A (\alpha_1 v_1 + \dots + \alpha_{n} v_{n}) = \sum \alpha_{i}^2 v_{i}^T A v_{i} = \sum \alpha_{i}^2 v_{i}^T \lambda_i v_{i}$$ 

    Так как $\lambda_{i} \geq 0$, то и вся сумма неотрицательна.

1.  Базовые операции, сохраняющие выпуклость множеств: пересечение бесконечного числа множеств, линейная комбинация множеств, образ афинного отображения.

    :::{.callout-note appearance="simple"}
    * Пересечение любого (!) количества выпуклых множеств — выпуклое множество.
    * Линейная комбинация выпуклых множеств выпукла.
    * Образ выпуклого множества после применения афинного отображения — выпуклое множество.
    :::

    **Пересечение бесконечного числа множеств**

    Пересечение любого (!) количества выпуклых множеств — выпуклое множество.

    Если итоговое пересечение пустое или содержит одну точку, то свойство выпуклости выполняется по определению. Иначе возьмем 2 точки и отрезок между ними. Эти точки должны лежать во всех пересекаемых множествах. Так как все пересекаемые множества выпуклы, отрезок между этими двумя точками лежит во всех множествах. А значит, отрезок лежит и в их пересечении.

    **Линейная комбинация множеств**

    Линейная комбинация выпуклых множеств выпукла.

    Пусть есть 2 выпуклых множества $S_x, S_y$, рассмотрим их линейную комбинацию
        $$
        S = \left\{s \mid s = c_1 x + c_2 y, \; x \in S_x, \; y \in S_y, \; c_1, c_2 \in \mathbb{R}\right\}
        $$
    Возьмем две точки из $S$: $s_1 = c_1 x_1 + c_2 y_1, s_2 = c_1 x_2 + c_2 y_2$ и докажем, что отрезок между ними $\theta s_1 + (1 - \theta)s_2, \theta \in [0,1]$ также принадлежит $S$
        $$
        \theta s_1 + (1 - \theta)s_2
        $$
        $$
        \theta (c_1 x_1 + c_2 y_1) + (1 - \theta)(c_1 x_2 + c_2 y_2)
        $$
        $$
        c_1 (\theta x_1 + (1 - \theta)x_2) + c_2 (\theta y_1 + (1 - \theta)y_2)
        $$
        $$
        c_1 x + c_2 y \in S
        $$

    **Образ афинного отображения**

    Образ выпуклого множества после применения афинного отображения — выпуклое множество.
        $$
        S \subseteq \mathbb{R}^n \text{ выпукло}\;\; \rightarrow \;\; f(S) = \left\{ f(x) \mid x \in S \right\} \text{ выпукло} \;\;\;\; \left(f(x) = \mathbf{A}x + \mathbf{b}\right)
        $$

    **Доказательство**

    При $\theta \in [0, 1]; x, y \in S, S$ — выпуклое. Тогда и $\theta x + (1 - \theta)y \in S$. В то же время $f(\theta x + (1 - \theta)y) = \theta A x + \theta b + (1 - \theta) A y + (1 - \theta) b = \theta A x + (1 - \theta) A y + b = \theta f(x) + (1 - \theta)f(y)$. В итоге мы доказали, что образ $f(S)$ — тоже выпуклый, так как $\forall \theta \in [0, 1], x, y \in S$ выполняется $\theta f(x) + (1 - \theta)f(y) \in f(S)$.

    Примеры афинных функций: растяжение, сжатие, проекция, транспонирование, множество решений линейного матричного неравенства $\left\{ x \mid x_1 A_1 + \ldots + x_m A_m \preceq B\right\}$. Здесь $A_i, B \in \mathbf{S}^p$ — симметричные матрицы $p \times p$. 

    Заметим также, что прообраз выпуклого множества при аффинном отображении также является выпуклым.
        $$
        S \subseteq \mathbb{R}^m \text{ выпукло}\; \rightarrow \; f^{-1}(S) = \left\{ x \in \mathbb{R}^n \mid f(x) \in S \right\} \text{ выпукло} \;\; \left(f(x) = \mathbf{A}x + \mathbf{b}\right)
        $$
1.  Неравенство Йенсена для выпуклой функции и выпуклой комбинации точек.

    :::{.callout-note appearance="simple"}
    Пусть $f(x)$ -- выпуклая функция, определённая на выпуклом множестве $S \subseteq \mathbb{R}^n$. Тогда для точек $x_1, \dots, x_m \in S$ выполнено неравенство:

    $$f \left( \sum \limits_{i=1}^m \lambda_i x_i \right) \leq \sum \limits_{i=1}^m \lambda_i f(x_i)$$

    $\lambda=[\lambda_1, \dots, \lambda_m] \in \Delta_m$.
    :::

    1. Заметим, что $\sum \limits_{i=1}^m \lambda_i x_i$ является выпуклой комбинацией элементов $S$ и лежит в $S$.

    2. Доказательство по индукции. Для $m=1$ очевидно, для $m=2$ следует из определения выпуклой функции.

    3. Пусть неравенство верно для $m=1, \dots, k$, докажем для $m=k+1$. Пусть $\lambda \in \Delta_{k+1}$, $x=\sum \limits_{i=1}^{k+1} \lambda_i x_i = \lambda_{k+1} x_{k+1} + \sum \limits_{i=1}^{k} \lambda_i x_i$. При $\lambda_i = 0$ либо $1$ выражение сводится к уже рассмотренным случаям, далее полагаем $0 < \lambda_i < 1$:
        $$
        x=\lambda_{k+1} x_{k+1} + (1-\lambda_{k+1}) \sum \limits_{i=1}^{k} \frac{\lambda_i}{1-\lambda_{k+1}} x_i =\lambda_{k+1} x_{k+1} + (1-\lambda_{k+1}) \hat{x}
        $$
    где $\hat{x} = \sum \limits_{i=1}^{k} \gamma_i x_i$ и $\gamma_i=\frac{\lambda_i}{1-\lambda_{k+1}} \geq 0,\,\, 1\leq i \leq k$.

    4. Так как $\lambda \in \Delta_{k+1}$, то $\gamma=[\gamma_1, \dots, \gamma_k] \in \Delta_k$. Значит, $\hat{x} \in S$, из выпуклости $f(x)$ и предположения индукции следует:
        $$
        f \left( \sum \limits_{i=1}^{k+1} \lambda_i x_i \right) = f(\lambda_{k+1} x_{k+1} + (1-\lambda_{k+1})\hat{x}) \leq \lambda_{k+1} f(x_{k+1})+(1-\lambda_{k+1})f(\hat{x}) \leq \sum \limits_{i=1}^{k+1} \lambda_i f(x_i)
        $$
1.  Выпуклость надграфика как критерий выпуклости функции.

    :::{.callout-note appearance="simple"}
    Чтобы функция $f(x)$, определенная на выпуклом множестве $X$, была выпуклой на $X$, необходимо и достаточно чтобы надграфик $f$ был выпуклым множеством.
    :::

    Для функции $f(x)$, определенной на $X \subseteq \mathbb{R}^n$, множество:
    $$
    \text{epi}\ f = \left\{[x, \mu] \in X \times \mathbb{R}: f(x) \le \mu \right\}
    $$
    называется **надграфиком** функции $f(x)$ (здесь $\mu \in \mathbb{R}, x \in S$).


    **Необходимость**

    Предположим, что $f(x)$ выпукла на $X$. Возьмем две произвольные точки $[x_1, \mu_1] \in \text{epi}f$ и $[x_2, \mu_2] \in \text{epi}f$. Также возьмем $0 \leq \lambda \leq 1$ и обозначим $x_{\lambda} = \lambda x_1 + (1 - \lambda) x_2, \mu_{\lambda} = \lambda \mu_1 + (1 - \lambda) \mu_2$. Тогда,
    $$
    \lambda\begin{bmatrix} x_1 \\ \mu_1 \end{bmatrix} + (1 - \lambda)\begin{bmatrix} x_2 \\ \mu_2 \end{bmatrix} = \begin{bmatrix} x_{\lambda} \\ \mu_{\lambda} \end{bmatrix}.
    $$
    Из выпуклости $X$ следует, что $x_{\lambda} \in X$. Более того, так как $f(x)$ -- выпуклая функция, то
    $$
    f(x_{\lambda}) \leq \lambda f(x_1) + (1 - \lambda) f(x_2) \leq \lambda \mu_1 + (1 - \lambda) \mu_2 = \mu_{\lambda}
    $$
    Из неравенства выше по определению надграфика следует, что $\begin{bmatrix} x_{\lambda} \\ \mu_{\lambda} \end{bmatrix} \in \text{epi}f$. Следовательно, надграфик $f$ -- выпуклое множество.

    **Достаточность**

    Предположим, что надграфик $f$, $\text{epi}f$, выпуклое множество. Тогда, исходя из того что $[x_1, \mu_1] \in \text{epi}f$ и $[x_2, \mu_2] \in \text{epi}f$, получаем
    $$
    \begin{bmatrix} x_{\lambda} \\ \mu_{\lambda} \end{bmatrix} = \lambda\begin{bmatrix} x_1 \\ \mu_1 \end{bmatrix} + (1 - \lambda)\begin{bmatrix} x_2 \\ \mu_2 \end{bmatrix} \in \text{epi}f
    $$
    для любого $0 \leq \lambda \leq 1$.

    Следовательно, из определения надграфика, подставив значение $\mu_{\lambda}$, получаем, что $f(x_{\lambda}) \leq \mu_{\lambda} = \lambda \mu_1 + (1 - \lambda) \mu_2$. 
    $$
    f(x_{\lambda}) = f(\lambda x_1 + (1 - \lambda) x_2) \leq \mu_\lambda = \lambda \mu_1 + (1 - \lambda) \mu_2
    $$

    Но это верно для всех $\mu_1 \geq f(x_1)$ и $\mu_2 \geq f(x_2)$, в том числе и при $\mu_1 = f(x_1)$ и $\mu_2 = f(x_2)$. Тогда мы получаем неравенство:
    $$
    f(x_{\lambda}) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)
    $$

    Так как $x_1 \in X$ и $x_2 \in X$ выбирались произвольно, $f(x)$ - выпуклая функция на $X$.
1.  Дифференциальный критерий сильной выпуклости первого порядка.

    :::{.callout-note appearance="simple"}
    Пусть $f(x)$ — дифференцируемая функция на выпуклом множестве $X \subseteq \mathbb{R}^n$. Тогда $f(x)$ сильно выпукла на $X$ с константой $\mu > 0$ тогда и только тогда, когда

    $$
    f(x) - f(x_0) \geq \langle \nabla f(x_0), x - x_0 \rangle + \frac{\mu}{2} \|x - x_0\|^2
    $$

    для всех $x, x_0 \in X$.
    :::

    **Необходимость**

    Пусть $0 < \lambda \leq 1$. Согласно определению сильно выпуклой функции,

    $$
    f(\lambda x + (1 - \lambda)x_0) \leq \lambda f(x) + (1 - \lambda) f(x_0) - \frac{\mu}{2} \lambda (1 - \lambda) \|x - x_0\|^2
    $$

    или эквивалентно,

    $$
    f(x) - f(x_0) - \frac{\mu}{2} (1 - \lambda) \|x - x_0\|^2 \geq \frac{1}{\lambda} \left[ f(\lambda x + (1 - \lambda)x_0) - f(x_0) \right] =
    $$

    $$
    = \frac{1}{\lambda} \left[ f(x_0 + \lambda(x - x_0)) - f(x_0) \right] = \frac{1}{\lambda} \left[ \lambda \langle \nabla f(x_0), x - x_0 \rangle + o(\lambda) \right] =
    $$

    $$
    = \langle \nabla f(x_0), x - x_0 \rangle + \frac{o(\lambda)}{\lambda}.
    $$

    Таким образом, переходя к пределу при $\lambda \to 0$, мы приходим к первоначальному утверждению.

    **Достаточность**

    Предположим, что неравенство в теореме выполнено для всех $x, x_0 \in X$. Возьмем $x_0 = \lambda x_1 + (1 - \lambda)x_2$, где $x_1, x_2 \in X$, $0 \leq \lambda \leq 1$. Согласно неравенству из условия теоремы, выполняются следующие неравенства:

    $$
    f(x_1) - f(x_0) \geq \langle \nabla f(x_0), x_1 - x_0 \rangle + \frac{\mu}{2} \|x_1 - x_0\|^2,
    $$

    $$
    f(x_2) - f(x_0) \geq \langle \nabla f(x_0), x_2 - x_0 \rangle + \frac{\mu}{2} \|x_2 - x_0\|^2.
    $$

    Умножая первое неравенство на $\lambda$ и второе на $1 - \lambda$ и складывая их, учитывая, что

    $$
    x_1 - x_0 = (1 - \lambda)(x_1 - x_2), \quad x_2 - x_0 = \lambda(x_2 - x_1),
    $$

    и что $\lambda(1 - \lambda)^2 + \lambda^2(1 - \lambda) = \lambda(1 - \lambda)$, получаем:

    $$
    \lambda f(x_1) + (1 - \lambda)f(x_2) - f(x_0) - \frac{\mu}{2} \lambda(1 - \lambda) \|x_1 - x_2\|^2 \geq
    $$

    $$
    \geq \langle \nabla f(x_0), \lambda x_1 + (1 - \lambda)x_2 - x_0 \rangle = 0.
    $$

    Таким образом, неравенство из определения сильно выпуклой функции выполнено. Важно отметить, что при $\mu = 0$ получаем случай выпуклости и соответствующий дифференциальный критерий.

1.  Дифференциальный критерий сильной выпуклости второго порядка.

    :::{.callout-note appearance="simple"}

    Пусть $X \subseteq \mathbb{R}^n$ — выпуклое множество с непустой внутренностью. Пусть также $f(x)$ — дважды непрерывно дифференцируемая функция на $X$. Тогда $f(x)$ сильно выпукла на $X$ с константой $\mu > 0$ тогда и только тогда, когда

    $$
    \langle y, \nabla^2 f(x) y \rangle \geq \mu \|y\|^2
    $$

    для всех $x \in X$ и $y \in \mathbb{R}^n$.

    Другая форма записи:
    $$ \nabla^2 f(x) \succcurlyeq \mu I $$
    :::

    Целевое неравенство тривиально, когда $y = 0_n$, поэтому предположим, что $y \neq 0_n$.

    **Необходимость**

    Пусть $x$ является внутренней точкой $X$. Тогда $x + \alpha y \in X$ для всех $y \in \mathbb{R}^n$ и достаточно малых $\alpha$. Поскольку $f(x)$ дважды дифференцируема,

    $$
    f(x + \alpha y) = f(x) + \alpha \langle \nabla f(x), y \rangle + \frac{\alpha^2}{2} \langle y, \nabla^2 f(x) y \rangle + o(\alpha^2).
    $$

    Основываясь на критерии первого порядка сильной выпуклости, имеем

    $$
    \frac{\alpha^2}{2} \langle y, \nabla^2 f(x) y \rangle + o(\alpha^2) = f(x + \alpha y) - f(x) - \alpha \langle \nabla f(x), y \rangle \geq \frac{\mu}{2} \alpha^2 \|y\|^2.
    $$

    Это неравенство сводится к целевому неравенству после деления обеих частей на $\alpha^2$ и перехода к пределу при $\alpha \to 0$.

    Если $x \in X$, но $x \notin \text{int}X$, рассмотрим последовательность $\{x_k\}$ такую, что $x_k \in \text{int}X$ и $x_k \to x$ при $k \to \infty$. Тогда мы приходим к целевому неравенству после перехода к пределу.

    **Достаточность**

    Формула Тейлора с остаточным членом Лагранжа второго порядка $\forall x, y: x, x + y \in X$ найдется $\alpha$ такая, что:

    $$
    f(x + y) = f(x) + \langle \nabla f(x), y \rangle + \frac{1}{2} \langle y, \nabla^2 f(x + \alpha y) y \rangle
    $$

    где $0 < \alpha < 1$.

    Используя формулу Тейлора с остаточным членом Лагранжа и неравенство из условия, получаем для $x + y \in X$:

    $$
    f(x + y) - f(x) - \langle \nabla f(x), y \rangle = \frac{1}{2} \langle y, \nabla^2 f(x + \alpha y) y \rangle \geq \frac{\mu}{2} \|y\|^2,
    $$

    где $0 \leq \alpha \leq 1$. Следовательно,

    $$
    f(x + y) - f(x) \geq \langle \nabla f(x), y \rangle + \frac{\mu}{2} \|y\|^2.
    $$

    Таким образом, по критерию первого порядка сильной выпуклости, функция $f(x)$ является сильно выпуклой с константой $\mu$. Важно отметить, что $\mu = 0$ соответствует случаю выпуклости и соответствующему дифференциальному критерию.
1.  Теорема о построении сопряженного множества к многогранному множеству.

    :::{.callout-note appearance="simple"}
    Пусть $x_1, \ldots, x_m \in \mathbb{R}^n$. Сопряжённое к многогранному множеству:

    $$
    S = \mathbf{conv}(x_1, \ldots, x_k) + \mathbf{cone}(x_{k+1}, \ldots, x_m) 
    $$

    будет многогранным множеством:

    $$
    S^* = \left\{ p \in \mathbb{R}^n \mid \langle p, x_i\rangle \ge -1, i = \overline{1,k} ; \langle p, x_i\rangle \ge 0, i = \overline{k+1,m} \right\}
    $$

    :::

    * Пусть $S = X, S^* = Y$. Возьмём произвольный $p \in X^*$, тогда $\langle p, x_i\rangle \ge -1, i = \overline{1,k}$. В то же время, для любого $\theta > 0, i = \overline{k+1,m}$: 
    
        $$
        \langle p, x_i\rangle \ge -1 \to \langle p, \theta x_i\rangle \ge -1
        $$

        $$
        \langle p, x_i\rangle \ge -\frac{1}{\theta} \to \langle p, x_i\rangle \geq 0. 
        $$

        Таким образом, $p \in Y \to X^* \subset Y$.

    * В обратную сторону: пусть $p \in Y$. Для любого $x \in X$:

        $$
        x = \sum\limits_{i=1}^m\theta_i x_i \;\;\;\;\;\;\; \sum\limits_{i=1}^k\theta_i = 1, \theta_i \ge 0
        $$
    
        Тогда:

        $$
        \langle p, x\rangle = \sum\limits_{i=1}^m\theta_i \langle p, x_i\rangle = \sum\limits_{i=1}^k\theta_i \langle p, x_i\rangle + \sum\limits_{i=k+1}^m\theta_i \langle p, x_i\rangle \ge \sum\limits_{i=1}^k\theta_i (-1) + \sum\limits_{i=1}^k\theta_i \cdot 0 = -1.
        $$

        Значит, $p \in X^* \to Y \subset X^*$.



1.  Субдифференциальное условие оптимальности для условных выпуклых задач.

    :::{.callout-note appearance="simple"}

    ![Условие оптимальности для условных выпуклых задач](general_first_order_local_optimality.pdf){width=75%}

    Пусть $f\colon \mathbb{R}^n \to \mathbb{R}$ — выпуклая функция, а $S \subset \mathbb{R}^n$ — некоторое (выпуклое) множество допустимых точек. Рассмотрим задачу оптимизации
    $$
    \min_{x \in S} f(x).
    $$
    Тогда точка $x^*$ является решением этой задачи тогда и только тогда, когда 
    $$
    0 \in \partial f(x^*) + \mathcal{N}_S(x^*),
    $$
    где $\partial f(x^*)$ — субдифференциал функции $f$ в точке $x^*$, а $\mathcal{N}_S(x^*)$ — нормальный конус к множеству $S$ в точке $x^*$.  

    Если же $f$ дополнительно дифференцируема, то условие оптимальности принимает вид
    $$
    \nabla f(x^*)^T \bigl(y - x^*\bigr) \geq 0 \quad \text{для всех } y \in S.
    $$
    :::

    Рассмотрим задачу
        $$
        \min_{x \in S} f(x).
        $$
    1. **Переход к неограниченной задаче:**  
    Введём *индикаторную функцию* множества $S$, то есть
        $$
        I_S(x) = \begin{cases}
        0, & x \in S, \\
        +\infty, & x \notin S.
        \end{cases}
        $$
    Тогда исходная задача эквивалентна безусловной (без явных ограничений) задаче
        $$
        \min_{x} \Bigl( f(x) + I_S(x) \Bigr).
        $$

    2. **Условие оптимальности через субградиент:**  
    Из общего субдифференциального условия оптимальности следует, что точка $x$ является решением
    $\min_x \{ f(x) + I_S(x) \}$ тогда и только тогда, когда
        $$
        0 \in \partial \bigl(f(x) + I_S(x)\bigr).
        $$

    3. **Свойство субградиента суммы:**  
    Поскольку $f$ выпуклая и $I_S$ — тоже выпуклая (это индикаторная функция выпуклого множества $S$), имеем
        $$
        \partial \bigl(f(x) + I_S(x)\bigr) 
        = \partial f(x) + \partial I_S(x).
        $$
    Но $\partial I_S(x) = \mathcal{N}_S(x)$, то есть *нормальный конус* к множеству $S$ в точке $x$. Следовательно, 
        $$
        0 \in \partial f(x) + \partial I_S(x)
        \quad \Longleftrightarrow \quad
        0 \in \partial f(x) + \mathcal{N}_S(x).
        $$

    4. **Интерпретация условия $0 \in \partial f(x) + \mathcal{N}_S(x)$:**  
    Это означает, что существует субградиент $g \in \partial f(x)$ такой, что
        $$
        -g \in \mathcal{N}_S(x).
        $$

    5. **Частный случай дифференцируемой функции $f$:**  
    Если $f$ дифференцируема, то $\partial f(x) = \{\nabla f(x)\}$. Условие  
        $$
        0 \in \{\nabla f(x)\} + \mathcal{N}_S(x)
        \quad \Longleftrightarrow \quad
        -\,\nabla f(x) \in \mathcal{N}_S(x).
        $$
        По определению нормального конуса,
        $$
        -\nabla f(x) \in \mathcal{N}_S(x)
        \quad \Longleftrightarrow \quad
        -\nabla f(x)^T x \geq -\nabla f(x)^T y \;\; \text{для всех } y \in S,
        $$
        что переписывается как
        $$
        \nabla f(x)^T (y - x) \geq 0 \;\; \text{для всех } y \in S.
        $$
        Это и есть классическое *условие оптимальности первого порядка* для дифференцируемых выпуклых задач оптимизации с ограничениями.
1.  Необходимые условия безусловного экстремума.

    :::{.callout-note appearance="simple"}
    Если в $x^*$ достигается локальный минимум и $f$ непрерывно дифференцируема в открытой окрестности $x^*$, то
    $$
    \nabla f(x^*) = 0
    $$
    :::

    Предположим обратное. Пусть $\nabla f(x^*) \neq 0$. Рассмотрим вектор $p = -\nabla f(x^*)$ и заметим, что
    $$
    p^T \nabla f(x^*) = -\| \nabla f(x^*) \|^2 < 0
    $$
    Так как $\nabla f$ непрерывна в окрестности $x^*$, то существует скаляр $T > 0$ такой, что
    $$
    p^T \nabla f(x^* + tp) < 0, \text{ для любого } t \in [0,T]
    $$
    Для любого $\bar{t} \in (0, T]$, мы можем воспользоваться теоремой Тейлора:
    $$
    f(x^* + \bar{t}p) = f(x^*) + \bar{t} p^T \nabla f(x^* + tp), \text{ для некоторого } t \in (0,\bar{t})
    $$
    Следовательно, $f(x^* + \bar{t}p) < f(x^*)$ для любого $\bar{t} \in (0, T]$. Мы нашли направление, идя вдоль которого из $x^*$ функция $f$ убывает. Тогда $x^*$ -- не точка локального минимума. Получили противоречие.
1.  Достаточные условия безусловного экстремума.

    :::{.callout-note appearance="simple"}
    Пусть $\nabla^2 f$ непрерывна в открытой окрестности $x^*$ и
    $$
    \nabla f(x^*) = 0 \quad \nabla^2 f(x^*) \succ 0.
    $$
    Тогда $x^*$ -- точка локального минимума $f$.
    :::

    Так как гессиан непрерывен и положительно определен в $x^*$, то мы можем выбрать радиус $r > 0$ такой, что $\nabla^2 f(x)$ остается положительно определенной для всех $x$ в открытом шаре $B = \{ z \mid \|z - x^*\| < r \}$. Взяв любой ненулевой вектор $p$, для которого выполняется $\|p\| < r$, мы получаем $x^* + p \in B$, а также по формуле Тейлора:
    $$ 
    f(x^* + p) = f(x^*) + p^T \nabla f(x^*) + \frac{1}{2} p^T \nabla^2 f(z) p
    $$
    $$ 
    = f(x^*) + \frac{1}{2} p^T \nabla^2 f(z) p
    $$
    где $z = x^* + tp$ для некоторого $t \in (0,1)$. Так как $z \in B$, мы получаем $p^T \nabla^2 f(z) p > 0$, и следовательно $f(x^* + p) > f(x^*)$. Таким образом $x^*$ -- точка локального минимума.
1.  Субдифференциальная форма теоремы Каруша Куна Таккера (доказательство). Необходимые условия ККТ для произвольной задачи математического программирования (только формулировка).

    :::{.callout-note appearance="simple"}
    Пусть $X$ - линейное нормированное пространство, а $f_j: X \to \mathbb{R}$, $j = 0, 1, \ldots, m$, - выпуклые собственные (никогда не принимающие значения $-\infty$, а также не тождественно равные $\infty$) функции. Рассмотрим задачу
    $$
    \begin{split}
    & f_0(x)\to \min\limits_{x \in X}\\\
    \text{s.t.} & f_j(x)\leq 0, \; j = 1,\ldots,m\\\\
    \end{split}
    $$
    Пусть $x^* \in X$ - минимум в задаче выше, а функции $f_j$, $j = 0, 1, \ldots, m$, непрерывны в точке $x^*$. Тогда существуют числа $\lambda_j \geq 0$, $j = 0, 1, \ldots, m$, такие, что
    $$
    \sum_{j=0}^{m} \lambda_j = 1,
    $$
    $$
    \lambda_j f_j(x^*) = 0, \quad j = 1, \ldots, m,
    $$
    $$
    0 \in \sum_{j=0}^{m} \lambda_j \partial f_j(x^*).
    $$
    :::

    **Доказательство**.

    1. Рассмотрим функцию
        $$
        f(x) = \max\{f_0(x) - f_0(x^*), f_1(x), \ldots, f_m(x)\}.
        $$
        Точка $x^*$ является глобальным минимумом этой функции. Действительно, если бы в некоторой точке $x_e \in X$ выполнялось неравенство $f(x_e) < 0$, то из этого следовало бы, что $f_0(x_e) < f_0(x^*)$ и $f_j(x_e) < 0$, $j = 1, \ldots, m$, что противоречит минимальности $x^*$ в задаче выше. 

    2. Тогда из теоремы Ферма в субдифференциальной форме следует, что 
        $$
        0 \in \partial f(x^*).
        $$
    
    3. По теореме Дубовицкого-Милютина имеем
        $$
        \partial f(x^*) = \text{conv } \left( \bigcup\limits_{j \in I}\partial f_j(x^*)\right),
        $$ 
    
    4. Поэтому существует $g_j \in \partial f_j(x^*)$, $j \in I$, такой, что
        $$
        \sum_{j \in I} \lambda_j g_j = 0, \quad \sum\limits_{j \in I}\lambda_j = 1, \quad \lambda_j \geq 0, \quad j \in I.
        $$
        Осталось задать $\lambda_j = 0$ для $j \notin I$.

    :::{.callout-note appearance="simple"}
    Для задачи математического программирования в общем виде
    $$
    \begin{split}
    & f_0(x) \to \min\limits_{x \in \mathbb{R}^n}\\
    \text{s.t. } & f_i(x) \leq 0, \; i = 1,\ldots,m\\
    & h_i(x) = 0, \; i = 1,\ldots, p
    \end{split}
    $$
    можно сформулировать лагранжиан:
    $$
    L(x, \lambda, \nu) = f_0(x) + \sum\limits_{i=1}^m \lambda_i f_i(x) + \sum\limits_{i=1}^p\nu_i h_i(x)
    $$
    Пусть $x^*$, $(\lambda^*, \nu^*)$ - решение задачи математического программирования с нулевым зазором двойственности (оптимальное значение для прямой задачи $p^*$ равно оптимальному значению для двойственной задачи $d^*$). Пусть также функции $f_0, f_i, h_i$ дифференцируемы.

    * $\nabla_x L(x^*, \lambda^*, \nu^*) = 0$
    * $\nabla_\nu L(x^*, \lambda^*, \nu^*) = 0$
    * $\lambda^*_i \geq 0, i = 1,\ldots,m$
    * $\lambda^*_i f_i(x^*) = 0, i = 1,\ldots,m$
    * $f_i(x^*) \leq 0, i = 1,\ldots,m$

    ## Некоторые условия регулярности
    Эти условия необходимы для того, чтобы сделать условия KKT необходимыми. Некоторые из них даже превращают необходимые условия в достаточные (например, условия Слейтера). Более того, если у вас есть регулярность, вы можете записать необходимые условия второго порядка $\langle y, \nabla^2_{xx} L(x^*, \lambda^*, \nu^*) y \rangle \geq 0$ с *полуопределенным* гессианом лагранжиана.

    * **Условие Слейтера.** Если для выпуклой задачи (т.е. в предположении минимизации, $f_0,f_{i}$ выпуклы и $h_{i}$ аффинны) существует точка $x$ такая, что $h(x)=0$ и $f_{i}(x)<0$ (существование строго внутренней точки бюджетного множества), то мы имеем нулевой зазор двойственности и условия KKT становятся необходимыми и достаточными.
    * **Условие линейности ограничений.** Если $f_{i}$ и $h_{i}$ - аффинные функции, то других условий не требуется.
    * **Условие линейной независимости ограничений.** Градиенты активных ограничений неравенств и градиенты ограничений равенств линейно независимы в точке $x^*$.  
    * Другие примеры см. в [wiki](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions#Regularity_conditions_(or_constraint_qualifications)).
    :::
1.  Формулировка симплекс метода для задачи линейного программирования в стандартной форме. Теорема о проверке оптимальности решения.

    :::{.callout-note appearance="simple"}
    Если все элементы $\lambda_B$ неположительны и базис $B$ допустимый, тогда базис $B$ оптимален.

    Здесь $\lambda_B$ это коэффициенты при разложении $c$ по базису $B$: $\lambda_B^{T}A_{B} = c^T \Rightarrow \lambda_B^T = c^TA_{B}^{-1}$.
    :::

    **Формулировка симплекс метода для задачи линейного программирования в стандартной форме. Теорема о проверке оптимальности решения**

    Задача линейного программирования:

    Пусть $c \in \mathbb{R}^n$, $b \in \mathbb{R}^m$, $A \in \mathbb{R}^{m \times n}$, тогда задача формулируется так:

    $$
    \begin{aligned}
    & \min_{x \in \mathbb{R}^n} && c^T x \\
    & \text{s.t.} && Ax \leq b
    \end{aligned}
    $$

    **Идейное описание симплекс метода:**

    1. Убедится, что точка, в которой мы находимся, является угловой
    2. Проверить оптимальность точки
    3. Если необходимо, сменить угол (то есть сменить базис)
    4. Повторять до схождения

    Шаги выполнения симплекс метода:

    1. **Поиск начальной базисной допустимой точки:**
        - Выберем начальную базисную (она является решением системы 
        $A_B x = b_B$, где $B$ - базис размера $n$ пространства, а матрица $A$ обычно имеет больше $n$ ограничений) допустимую (
            $Ax_0 \leq b$) точку 
        $x_0$ (искать ее будем через двухфазный симплексметод). Если такая точка не найдена, задача не имеет допустимого решения.

    2. **Проверка оптимальности:**
        - Разложение вектора 
        $c$ в данном базисе $B$ с коэффициентами 
        $\lambda_B$:
        $$
        \lambda_B^\top A_B = c^\top \quad \text{или} \quad \lambda_B^\top = c^\top A_B^{-1}
        $$
        - Если все компоненты 
        $\lambda_B$ неположительны, текущий базис является оптимальным. Иначе далее меняем вершину симплекса.

    3. **Определение переменной для удаления из базиса:**
        - Если в разложении 
        $\lambda_B$ есть положительные координаты, продолжаем оптимизацию. Пусть 
        $\lambda_B^k > 0$. Необходимо исключить 
        $k$ из базиса. Рассчитаем направляющий вектор 
        $d$, идя вдоль которого изменим вершину следующим образом: во-первых, для векторов всех ограничений из базиса, которые мы оставляем, направление должно быть им ортогонально, и, во-вторых, вдоль него значение, связанное с нашим ограничением, должно убывать:
        $$
        \begin{cases}
        A_{B \backslash \{k\} } d = 0 \\
        a_k^\top d < 0
        \end{cases}
        $$

    4. **Вычисление шага вдоль выбранного направления $d$:**
        - Для всех 
        $j \notin B$ считаем шаг:
        $$
        \mu_j = \frac{b_j - a_j^\top x_B}{a_j^\top d}
        $$
        - Новая вершина, которую добавим в базис:
        $$
        t = \arg\min_j \{\mu_j \mid \mu_j > 0\}
        $$

    5. **Обновление базиса:**
        - Обновляем базис и текущее решение:
        $$
        \begin{aligned}
            B' = B \backslash \{k\} \cup \{t\}, \\
            x_{B'} = x_B + \mu_t d = A_{B'}^{-1} b_{B'}
        \end{aligned}
        $$
        - Изменение базиса приводит к уменьшению значения целевой функции:
        $$
        c^\top x_{B'} = c^\top (x_B + \mu_t d) = c^\top x_B + \mu_t c^\top d
        $$

    6. **Повторение:**
        - Далее повторяем шаги 2-5 до достижения оптимального решения или установления, что задача не имеет допустимого решения.

    **Теорема о проверке оптимальности решения:**

    Если все элементы $\lambda_B$ неположительны и базис $B$ достижим, тогда базис $B$ оптимален.

    Здесь $\lambda_B$ это коэффициенты при разложении $c$ по базису $B$: $\lambda_B^{T}A_{B} = c^T \Rightarrow \lambda_B^T = c^TA_{B}^{-1}$.

    **Доказательство:**

    Предположим противное (что этот базис не оптимален), пусть $\exists x^*: \; Ax^{*}\leq b$ и при этом $c^Tx^* < c^T x_{B}$. Так как для всей матрицы $A$ и вектора $b$ неравенство верно, то и для подматрицы оно верно:

    $$
    A_{B}x^* \leq b_{B}
    $$

    Так как все элементы $\lambda_{B}$ неположительны, то домножим строки на соответствующие элементы и сложим:

    $$
    \lambda_{B}^TA_{B}x^* \geq \lambda_{B}^T b_{B}
    $$

    $$
    c^Tx^* \geq \lambda_{B}^T b_{B} = \lambda_{B}^T A_{B}x_{B} =c^Tx_B 
    $$

    Противоречие.
1.  Доказательство работы теста корней.

    
    :::{.callout-note appearance="simple"}
    Пусть $(r_k)_{k=m}^\infty$ последовательность неотрицательных чисел, сходящихся к нулю, и пусть $\alpha := \limsup_{k \to \infty} r_k^{1/k}$. (причем $\alpha \geq 0$.)

    (a) Если $0 \leq \alpha < 1$, то $(r_k)_{k=m}^\infty$ сходится линейно с константой $\alpha$.

    (b) В частности, если $\alpha = 0$, то $(r_k)_{k=m}^\infty$ сходится сверхлинейно.

    (c) Если $\alpha = 1$, то $(r_k)_{k=m}^\infty$ сходится сублинейно.

    (d) Случай $\alpha > 1$ невозможен.
    :::

    **Доказательство**. 

    1. Покажем, что если $(r_k)_{k=m}^\infty$ сходится линейно с константой $0 \leq \beta < 1$, то обязательно $\alpha \leq \beta$.
        
        Действительно, по определению константы линейной сходимости для любого $\varepsilon > 0$, удовлетворяющего условию $\beta + \varepsilon < 1$, существует $C > 0$, такое что $r_k \leq C(\beta + \varepsilon)^k$ для всех $k \geq m$.
        
        Отсюда $r_k^{1/k} \leq C^{1/k}(\beta + \varepsilon)$ для всех $k \geq m$. Переходя к пределу при $k \to \infty$ и используя $C^{1/k} \to 1$, получаем $\alpha \leq \beta + \varepsilon$. Учитывая произвольность $\varepsilon$, следует, что $\alpha \leq \beta$.
        
    2. Таким образом, в случае $\alpha = 1$ последовательность $(r_k)_{k=m}^\infty$ не может иметь линейную сходимость согласно вышеуказанному результату (доказанному от противного). Поскольку, тем не менее, $(r_k)_{k=m}^\infty$ сходится к нулю, она должна сходиться сублинейно.
    
    3. Теперь рассмотрим случай $0 \leq \alpha < 1$. Пусть $\varepsilon > 0$ — произвольное число, такое что $\alpha + \varepsilon < 1$.
    
        Согласно свойствам $\limsup$, существует $N \geq m$, такое что $r_k^{1/k} \leq \alpha + \varepsilon$ для всех $k \geq N$.
        
        Следовательно, $r_k \leq (\alpha + \varepsilon)^k$ для всех $k \geq N$. Поэтому последовательность $(r_k)_{k=m}^\infty$ сходится линейно с параметром $\alpha + \varepsilon$ (не имеет значения, что неравенство выполняется только начиная с числа $N$).
        
        Из произвольности $\varepsilon$ следует, что константа линейной сходимости последовательности $(r_k)_{k=m}^\infty$ не превышает $\alpha$.
        
        Поскольку, как показано выше, константа линейной сходимости не может быть меньше $\alpha$, это означает, что константа линейной сходимости последовательности $(r_k)_{k=m}^\infty$ равна именно $\alpha$.
    
    4. Покажем, что случай $\alpha > 1$ невозможен.
        
        Действительно, предположим, что $\alpha > 1$. Тогда из определения $\limsup$ следует, что для любого $N \geq m$ существует $k \geq N$, такое что $r_k^{1/k} \geq 1$, и, в частности, $r_k \geq 1$.
        
        Но это означает, что у $r_k$ есть подпоследовательность, ограниченная от нуля. Следовательно, последовательность $(r_k)_{k=m}^\infty$ не может сходиться к нулю, что противоречит условию.
1.  Метод дихотомии и золотого сечения для унимодальных функций. Скорость сходимости.

    :::{.callout-note appearance="simple"}
    Методы локализации решения для скалярной минимизации. Сходятся линейно.
    :::

    **Метод дихотомии**

    Решаем следующую задачу: 
    $$
    f(x) \rightarrow \min_{x \in [a, b]}
    $$
    где $f(x)$ — унимодальная функция.

    Мы хотим на каждом шаге вдвое сокращать область, в которой ищем минимум. Для этого будем пользоваться основным свойством унимодальных функций:
    $$
    \forall a \leq x_1 < x_2 \leq b:
    $$
    $$
    f(x_1) \leq f(x_2) \Rightarrow x_* \in [a, x_2]
    $$
    $$
    f(x_1) \geq f(x_2) \Rightarrow x_* \in [x_1, b]
    $$
    где $x_*$ — точка, в которой достигается минимум

    Алгоритм:

    ![Алгоритм дихотомии](Dichotomy4.pdf)

    Можно заметить, что на каждой итерации требуется не более 2-х вычислений значения функции.

    **Сходимость метода дихотомии**

    Длина отрезка на $k$-ой итерации:
    $$
    \Delta_{k} = b_{k} - a_{k} = \frac{1}{2^k}(b - a)
    $$

    Если будем выбирать середину отрезка как выход $k$-ой итерации:
    $$
    |x_{k} - x_*| \leq \frac{\Delta_{k}}{2}
    $$

    Подставим полученное ранее выражение для длины отрезка:
    $$
    |x_{k} - x_*| \leq \frac{1}{2^{k+1}}(b - a)
    $$
    $$
    |x_{k} - x_*| \leq (0.5)^{k+1}(b - a)
    $$

    Получили выражение для сходимости по итерациям. Отсюда также можно выразить необходимое количество итераций для достижения точности $\varepsilon$:

    $$
    K = \left\lceil \log_2 \frac{b-a}{\varepsilon} - 1 \right\rceil
    $$

    Теперь получим выражение для сходимости по количеству вычислений значения функции. Знаем, что на каждой итерации вычисляем значение не более 2-х раз, значит количество вычислений значения функции возьмём $N = 2k$:

    $$
    |x_{k} - x_*| \leq (0.5)^{\frac{N}{2}+1}(b - a)
    $$
    $$
    |x_{k} - x_*| \leq (0.707)^{N}\frac{b - a}{2}
    $$

    **Метод золотого сечения**

    Идея такая же, как и в методе дихотомии, но хотим уменьшить количество вычислений значения функции. Для этого будем вычислять значения в точках золотого сечения. Так на каждой итерации нам нужно будет вычислять значение только в одной точке, так как для нового отрезка в одной из точек золотого сечения значение будет уже посчитано:

    ![Золотое сечение](golden_search.pdf)

    Алгоритм:

    ```python
    def golden_search(f, a, b, epsilon):
        tau = (sqrt(5) + 1) / 2
        y = a + (b - a) / tau**2
        z = a + (b - a) / tau
        while b - a > epsilon:
            if f(y) <= f(z):
                b = z
                z = y
                y = a + (b - a) / tau**2
            else:
                a = y
                y = z
                z = a + (b - a) / tau
        return (a + b) / 2
    ```

    **Сходимость метода золотого сечения**

    На каждой итерации длина отрезка будет уменьшаться в $\tau = \frac{\sqrt{5} + 1}{2}$ раз. Тогда оценка сходимости (и по итерациям, и по вычислениям значений функции):
    $$
    |x_{k} - x_*| \leq \frac{b_{k} - a_{k}}{2} = \left( \frac{1}{\tau} \right)^{N} \frac{b - a}{2} \approx 0.618^k\frac{b - a}{2}
    $$

    Получили сходимость по итерациям хуже, чем у дихотомии, так как отрезки уменьшаются слабее на каждой итерации. Но по количеству вычислений значения функции сходимость у метода золотого сечения быстрее.
1.  Теорема сходимости градиентного спуска для гладких выпуклых функций. 

    :::{.callout-note appearance="simple"}
    Рассматриваем задачу 
    $$
    f(x) \rightarrow \min_{x \in \mathbb{R}^d}
    $$
    и предполагаем, что $f$ - выпуклая, $L$-гладкая, $L > 0$.

    Пусть $(x_k)_{k \in \mathbb{N}}$ это последовательность, созданная градиентным спуском с постоянным шагом $\alpha$, $0<\alpha \leqslant \frac{1}{L}$. Тогда градиентный спуск сходится сублинейно, то есть:
    $$
    f(x_k)-f^* \leq \frac{\|x_0-x^*\| ^2}{2 \alpha k}.
    $$
    :::
    1. Формулировка метода:
        $$
        x_{k + 1} = x_k  - \alpha \nabla f(x_k) \Rightarrow x_{k + 1} - x_k = -\alpha \nabla f(x_k)
        $$
    1. $L$-гладкость: $\forall x, y: f(y) \leqslant f(x) + \langle \nabla f(x), y - x \rangle + \frac{L}{2} \| y - x \|^2$
        $$
        y := x_{k+1}, x := x_k \Rightarrow f(x_{k + 1}) \leqslant f(x_k) + \langle \nabla f(x_k), - \alpha \nabla f(x_k) \rangle + \frac{L}{2} \alpha^2 \| \nabla f(x_k) \|^2
        $$
        $$
        f(x_{k + 1}) \leqslant f(x_k) - \alpha \| \nabla f(x_k) \|^2 + \frac{L}{2} \alpha^2 \| \nabla f(x_k) \|^2 \qquad (1)
        $$
    1. Решим задачу оптимизации для поиска оптимального постоянного шага $\left(\frac{L}{2} \alpha^2 - \alpha\right) \rightarrow \min\limits_{\alpha}$. Получаем оптимальный шаг: $\alpha = \frac{1}{L}$ и $f(x_k) - f(x_{k + 1}) \geqslant \frac{1}{2L} \| \nabla f(x_k) \|^2$

    1. Выпуклость: $f(y) \geqslant f(x) + \nabla f(x)^T(y - x)$
        $$
        y := x^*, x := x_k \Rightarrow f(x^*) \geqslant f(x_k) + \nabla f(x_k)^T(x^* - x_k) \Rightarrow
        $$
        $$
        \Rightarrow f(x_k) \leqslant f(x^*) + \nabla f(x_k)^T(x_k - x^*) \Rightarrow f(x_k) - f(x^*) \leqslant \nabla f(x_k)^T(x_k - x^*)
        $$

    1. Подставим выпуклость в (1):
        $$
        \begin{split}
        f(x_{k+1}) &\leq f(x_k) -\frac{\alpha}{2} \Vert \nabla f(x_k)\Vert^2 \leq f^* + \langle \nabla f(x_k), x_k-x^*\rangle - \frac{\alpha}{2} \Vert \nabla f(x_k)\Vert^2 \\
        &= f^* + \langle \nabla f(x_k), x_k-x^* - \frac{\alpha}{2} \nabla f(x_k)\rangle \\
        &= f^* + \frac{1}{2 \alpha}\left\langle \alpha \nabla f(x_k), 2\left(x_k-x^* - \frac{\alpha}{2} \nabla f(x_k)\right)\right\rangle 
        \end{split}
        $$
    1. Пусть $a = x_k-x^*$ и $b =x_k-x^* - \alpha\nabla f(x_k)$. Тогда $a-b = \alpha \nabla f(x_k)$ и $a+b=2\left(x_k-x^* - \frac{\alpha}{2} \nabla f(x_k)\right)$.
        $$
        \begin{split}
        f(x_{k+1}) &\leq f^* + \frac{1}{2 \alpha}\left[ \|x_k-x^*\|_2^2 - \|x_k-x^* - \alpha\nabla f(x_k)\|_2^2\right] \\
        &\leq f^* + \frac{1}{2 \alpha}\left[ \|x_k-x^*\|_2^2 - \|x_{k+1}-x^*\|_2^2\right] \\
        2\alpha \left(f(x_{k+1}) - f^*\right) &\leq \|x_k-x^*\|_2^2 - \|x_{k+1}-x^*\|_2^2 
        \end{split}
        $$
    1. Предположим, что последняя строка определена для некоторого индекса $i$ и мы суммируем по $i \in [0, k-1]$. Большинство слагаемых будут равны нулю из-за телескопической природы суммы:
        $$
        \begin{split}
        2\alpha \sum\limits_{i=0}^{k-1} \left(f(x_{i+1}) - f^*\right) &\leq \|x_0-x^*\|_2^2 - \|x_{k}-x^*\|_2^2 \leq \|x_0-x^*\|_2^2 
        \end{split}
        $$ {#eq-gd-sc-telescopic}
    1. Из-за монотонного убывания на каждой итерации $f(x_{i+1}) < f(x_i)$:
        $$
        kf(x_k) \leq \sum\limits_{i=0}^{k-1}f(x_{i+1})
        $$
    1. Подставим в (-@eq-gd-sc-telescopic):
        $$
        \begin{split}
        2\alpha kf(x_k) - 2\alpha kf^* &\leq 2\alpha \sum\limits_{i=0}^{k-1} \left(f(x_{i+1}) - f^*\right)  \leq \|x_0-x^*\|_2^2 \\
        f(x_k) - f^* &\leq \frac{\|x_0-x^*\|_2^2}{2 \alpha k} \leq  \frac{L \|x_0-x^*\|_2^2}{2 k} 
        \end{split}
        $$
        То есть сходимость сублинейная.
1.  Теорема сходимости градиентного спуска для гладких PL функций.

    :::{.callout-note appearance="simple"}
    Рассмотрим задачу 
    $$
    f(x) \to \min_{x \in \mathbb{R}^d}
    $$
    и предположим, что $f$ удовлетворяет условию Поляка-Лоясиевича с константой $\mu$ и $L$-гладкости, для некоторых $L\geq \mu >0$.
    Пусть $(x_k)_{k \in \mathbb{N}}$ - последовательность, созданная градиентным спуском с постоянным шагом $\alpha$, $0<\alpha \leq \frac{1}{L}$. Тогда имеется линейная сходимость:
    $$
    f(x_k)-f^* \leq (1-\alpha \mu)^k (f(x_0)-f^*).
    $$
    :::

    1. Используя $L$-гладкость, вместе с правилом обновления алгоритма, можно записать:
        $$
        \begin{split}
        f(x_{k+1})& \leq f(x_k) + \langle \nabla f(x_k), x_{k+1}-x_k \rangle +\frac{L}{2} \| x_{k+1}-x_k\|^2\\ 
        &= f(x_k)-\alpha\Vert \nabla f(x_k) \Vert^2 +\frac{L \alpha^2}{2} \| \nabla f(x_k)\|^2 \\ 
        &= f(x_k) - \frac{\alpha}{2} \left(2 - L \alpha \right)\Vert \nabla f(x_k) \Vert^2 \\ 
        & \leq f(x_k) - \frac{\alpha}{2}\Vert \nabla f(x_k)\Vert^2,
        \end{split}
        $$
        В последнем неравенстве использовали предположение о шаге $0 < \alpha L \leq 1$.
    1. Используя свойство Поляка-Лоясиевича, можно записать:
        $$
        f(x_{k+1}) \leq f(x_k) - \alpha \mu (f(x_k) - f^*).
        $$
    1. Вычитая $f^*$ с обеих сторон и используя рекурсию, получаем:
        $$
        f(x_k) - f^* \leq (1-\alpha \mu)^k (f(x_0) - f^*).
        $$
1.  Теорема сходимости градиентного спуска для сильно выпуклых квадратичных функций. Оптимальные гиперпараметры.

    :::{.callout-note appearance="simple"}
    $$ f(x) \rightarrow \min\limits_{x \in \mathbb{R}^d} $$
    $$ f(x) = \frac{1}{2}x^TAx - b^Tx + c, \ A \in \mathbb{S}_{++} $$
    Тогда градиентный спуск с шагом $\alpha = \frac{2}{\mu + L}$ сходится линейно с показателем $\frac{L-\mu}{L+\mu}$
    $$
    f(x_k) - f^* \leqslant \left(\frac{L-\mu}{L+\mu}\right)^k(f(x_0) - f^*).
    $$
    :::

    $$
    \nabla f(x) = Ax - b \overset{\nabla f(x^*) = 0}{\Rightarrow} Ax^* = b
    $$
    Тогда шаг градиентного спуска имеет вид
    $$
    x_{k+1} = x_k - \alpha (Ax - b)
    $$
    Найдем $\alpha^*$. Воспользуемся $A = Q\Lambda Q^T$, где $\Lambda = \operatorname{diag}\{\lambda_1, \ldots, \lambda_n\}, \ Q = \|q_1, \ldots, q_n\|$, $\lambda_i, q_i$ - собственное значение и собственный вектор соответственно.
    $$ x_{k+1}  = (I - \alpha A)x_k + \alpha A x^* \ | \ -x^*$$
    $$ x_{k + 1} - x^* = (I - \alpha A)(x_k - x^*) $$
    $$ x_{k + 1} - x^* = (I - \alpha Q\Lambda Q^T)(x_k - x^*) \ | \ \cdot Q^T $$
    $$ Q^T(x_{k+1} - x^*) = (Q^T - \alpha \Lambda Q^T)(x_k - x^*) = (I - \alpha \Lambda)Q^T(x_k - x^*) $$
    $$ \text{Замена: } \tilde{x} = Q^T(x - x^*) \Rightarrow \tilde{x}_{k+1} = (I - \alpha \Lambda)\tilde{x}_{k} \Leftrightarrow \tilde{x}_i^{(k+1)} = (1 - \alpha \lambda_i)\tilde{x}_i^{(k)} \ i=\overline{1,d} $$
    $$ \lambda_{\min} = \mu, \quad \lambda_{\max} = L $$
    Сходимость есть $\Leftrightarrow \max\limits_{i}|1 - \alpha \lambda_i| < 1$
    $$
    \left\{ 
    \begin{array}{rl}
        |1 - \lambda \mu| < 1 \Rightarrow &1 - \lambda \mu < 1 \Rightarrow \alpha > 0 \\
        &\alpha \mu - 1 < 1 \Rightarrow \alpha < \frac{2}{\mu} \\
        |1 - \alpha L| < 1 \Rightarrow &1 - \alpha L < 1 \Rightarrow \alpha > 0 \\
        &\alpha L - 1 < 1 \Rightarrow \alpha < \frac{2}{L}
    \end{array}
    \right\} \Rightarrow \alpha < \frac{2}{L}
    $$
    Радиус сходимости $\rho = \max (|1 - \alpha \mu|, |1 - \alpha L|)$ и $\rho \rightarrow \min \Leftrightarrow \alpha^*L - 1 = 1 - \alpha^*\mu \Rightarrow \alpha^* = \frac{2}{\mu + L}$ и $\rho^* = \frac{L - \mu}{L + \mu}$

    Итого получаем, что для градиентного спуска выполняется $\|x_k - x^*\| \leqslant \left(\frac{L-\mu}{L+\mu}\right)^{k}\|x_0 - x^*\|$.
1.  Вывод ускоренного метода для квадратичной функции с помощью полиномов Чебышёва.

    :::{.callout-note appearance="simple"}
    Решаем квадратичную задачу:
    $$
    f(x) = \frac{1}{2} x^T A x - b^T x \qquad x_{k+1} = x_k - \alpha_k \nabla f(x_k)
    $$
    Можно показать, что метод, полученный с помощью полиномов Чебышёва, имеет вид:
    $$
    x_{k+1} = x_k - \alpha_k \nabla f(x_k) + \beta_k \left(x_k - x_{k-1}\right)
    $$
    обладает ускоренной линейной сходимостью.
    :::

    1. Пусть $x^*$ - единственное решение линейной системы $Ax=b$ и пусть $e_k = x_k-x^*$, где $x_{k+1}=x_k - \alpha_k (Ax_k-b)$ определяется рекурсивно, начиная с некоторого $x_0,$ и $\alpha_k$ - шаг, который мы определим позже.
        $$
        e_{k+1} = (I-\alpha_k A)e_k.
        $$
    1.Вышеуказанный расчет дает нам $e_k = p_k(A)e_0,$ где $p_k$ - полином
        $$
        p_k(a) = \prod_{i=1}^k (1-\alpha_ia).
        $$
        Мы можем ограничить сверху норму ошибки как
        $$
        \|e_k\|\le \|p_k(A)\|\cdot\|e_0\|\,.
        $$
        Поскольку $A$ - симметричная матрица с собственными значениями в $[\mu,L]$,:
        $$
        \|p_k(A)\|\le \max_{\mu\le a\le L} \left|p_k(a)\right|\,.
        $$
        Это приводит к интересной проблеме: среди всех полиномов, удовлетворяющих $p_k(0)=1$, мы ищем полином, величина которого наименьшая в интервале $[\mu,L]$.


    1. Наивный подход состоит в выборе равномерного шага $\alpha_k=\frac{2}{\mu+L}$ в выражении. Этот выбор делает $|p_k(\mu)| = |p_k(L)|$.
        $$
        \|e_k\|\le \left(\dfrac{L - \mu}{L + \mu}\right)^k\|e_0\|
        $$
        Это точно такой же результат, который мы доказали для сходимости градиентного спуска в случае квадратичной функции.

        Давайте взглянем на этот полином поближе. На правом рисунке мы выбрали $\alpha=1$ и $\beta=10$ так, что $\kappa=10.$ Соответствующий интервал, таким образом, равен $[1,10].$

        Можем ли мы сделать лучше? Ответ - да.

        ![](gd_polynom_5.pdf){fig-align="center" width="60%"}


    1. Полиномы Чебышёва оказываются оптимальным ответом на вопрос, который мы задавали. Соответствующим образом масштабированные, они минимизируют абсолютное значение в желаемом интервале $[\mu,L]$ при условии, что значение равно 1 в начале.

        $$
        \begin{aligned}
        T_0(x) &= 1\\
        T_1(x) &= x\\
        T_k(x) &=2xT_{k-1}(x)-T_{k-2}(x),\qquad k\ge 2.\\
        \end{aligned}
        $$
    
    1. Давайте построим стандартные полиномы Чебышёва (без масштабирования):

        ![](gd_polynom_cheb_4.pdf){fig-align="center" width="30%"}

    1. Исходные полиномы Чебышёва определяются на интервале $[-1,1]$. Чтобы использовать их для наших целей, нам нужно их масштабировать на интервал $[\mu,L]$. 

    1. Мы будем использовать следующую аффинную трансформацию:
        $$
        x = \frac{L + \mu - 2a}{L - \mu}, \quad a \in [\mu,L], \quad x \in [-1,1]. 
        $$
    1. Обратите внимание, что $x=1$ соответствует $a=\mu$, $x=-1$ соответствует $a=L$ и $x=0$ соответствует $a=\frac{\mu+L}{2}$. Эта трансформация гарантирует, что поведение полинома Чебышёва на интервале $[-1,1]$ отражается в интервал $[\mu, L]$

    1. В нашем анализе ошибок мы требуем, чтобы полином был равен 1 в 0 (т.е., $p_k(0)=1$). После применения трансформации значение $T_k$ в точке, соответствующей $a=0$, может не быть 1. Таким образом, мы нормируем полином $T_k$, деля его на значение $T_k\left(\frac{L+\mu}{L-\mu}\right)$:
        $$
        \frac{L+\mu}{L-\mu}, \qquad \text{гарантируя, что} \qquad P_k(0)= T_k\left(\frac{L+\mu-0}{L-\mu}\right) \cdot T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1} = 1.
        $$
    1. Давайте построим масштабированные полиномы Чебышёва
        $$
        P_k(a) = T_k\left(\frac{L+\mu-2a}{L-\mu}\right) \cdot T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1}
        $$
        и наблюдаем, что они значительно лучше ведут себя в интервале $[\mu,L]$ по сравнению с наивными полиномами.

        ![Масштабированные полиномы](gd_polynoms_1.pdf){fig-align="center" width="30%"}
        ![Масштабированные полиномы](gd_polynoms_2.pdf){fig-align="center" width="30%"}
        ![Масштабированные полиномы](gd_polynoms_3.pdf){fig-align="center" width="30%"}
        ![Масштабированные полиномы](gd_polynoms_4.pdf){fig-align="center" width="30%"}
        ![Масштабированные полиномы](gd_polynoms_5.pdf){fig-align="center" width="30%"}
        ![Масштабированные полиномы](gd_polynoms_6.pdf){fig-align="center" width="30%"}
    1. Мы видим, что максимальное значение полинома Чебышёва на интервале $[\mu,L]$ достигается в точке $a=\mu$. Следовательно, мы можем использовать следующую верхнюю границу:
        $$
        \|P_k(A)\|_2 \le P_k(\mu) = T_k\left(\frac{L+\mu-2\mu}{L-\mu}\right) \cdot T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1} = T_k\left(1\right) \cdot T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1} = T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1}
        $$
    1. Используя определение числа обусловленности $\varkappa = \frac{L}{\mu}$, мы получаем:
        $$
        \|P_k(A)\|_2 \le T_k\left(\frac{\varkappa+1}{\varkappa-1}\right)^{-1} = T_k\left(1 + \frac{2}{\varkappa-1}\right)^{-1} = T_k\left(1 + \epsilon\right)^{-1}, \quad \epsilon = \frac{2}{\varkappa-1}.
        $$
    1. Следовательно, нам нужно только понять значение $T_k$ в $1+\epsilon$. Это то, откуда берется ускорение. Мы будем ограничивать это значение сверху величиной $\mathcal{O}\left(\frac{1}{\sqrt{\epsilon}}\right)$.
    1. Чтобы ограничить $|P_k|$ сверху, нам нужно оценить $|T_k(1 + \epsilon)|$ снизу.

    1. Для любого $x\ge 1$, полином Чебышёва первого рода может быть записан как
        $$
        \begin{aligned}
        T_k(x)&=\cosh\left(k\,\mathrm{arccosh}(x)\right)\\
        T_k(1+\epsilon)&=\cosh\left(k\,\mathrm{arccosh}(1+\epsilon)\right).
        \end{aligned}
        $$

    1. Помним, что:
        $$
        \cosh(x)=\frac{e^x+e^{-x}}{2} \quad \mathrm{arccosh}(x) = \ln(x + \sqrt{x^2-1}).
        $$
    1. Пусть $\phi=\mathrm{arccosh}(1+\epsilon)$,
        $$
        e^{\phi}=1+\epsilon + \sqrt{2\epsilon+\epsilon^2} \geq 1+\sqrt{\epsilon}.
        $$
    1. Следовательно,
        $$
        \begin{aligned}
        T_k(1+\epsilon)&=\cosh\left(k\,\mathrm{arccosh}(1+\epsilon)\right) \\
        &= \cosh\left(k\phi\right) \\
        &= \frac{e^{k\phi} + e^{-k\phi}}{2} \geq\frac{e^{k\phi}}{2} \\
        &= \frac{\left(1+\sqrt{\epsilon}\right)^k}{2}.
        \end{aligned}
        $$
    1. Наконец, мы получаем:
        $$
        \begin{aligned}
        \|e_k\| &\leq \|P_k(A)\| \|e_0\| \leq \frac{2}{\left(1 + \sqrt{\epsilon}\right)^k} \|e_0\| \\ 
        &\leq 2 \left(1 + \sqrt{\frac{2}{\varkappa-1}}\right)^{-k} \|e_0\| \\
        &\leq 2 \exp\left( - \sqrt{\frac{2}{\varkappa-1}} k\right) \|e_0\|
        \end{aligned}
        $$
    1. Из-за рекурсивного определения полиномов Чебышёва, мы непосредственно получаем итерационную схему ускорения. Переформулируя рекуррентное соотношение в терминах наших масштабированных полиномов Чебышёва, мы получаем:
        $$
        T_{k+1}(x) =2xT_{k}(x)-T_{k-1}(x)
        $$
        Поскольку $x = \frac{L+\mu-2a}{L-\mu}$, и:
        $$
        \begin{aligned}
        P_k(a) &= T_k\left(\frac{L+\mu-2a}{L-\mu}\right) T_k\left(\frac{L+\mu}{L-\mu}\right)^{-1}\\
        T_k\left(\frac{L+\mu-2a}{L-\mu}\right) &= P_k(a) T_k\left(\frac{L+\mu}{L-\mu}\right) 
        \end{aligned}
        $$
        $$
        \begin{aligned}
        T_{k-1}\left(\frac{L+\mu-2a}{L-\mu}\right) &= P_{k-1}(a) T_{k-1}\left(\frac{L+\mu}{L-\mu}\right) \\
        T_{k+1}\left(\frac{L+\mu-2a}{L-\mu}\right) &= P_{k+1}(a) T_{k+1}\left(\frac{L+\mu}{L-\mu}\right)
        \end{aligned}
        $$
        $$
        \begin{aligned}
        P_{k+1}(a) t_{k+1} &= 2 \frac{L+\mu-2a}{L-\mu} P_{k}(a) t_{k} - P_{k-1}(a) t_{k-1} \text{, where } t_{k} = T_{k}\left(\frac{L+\mu}{L-\mu}\right) \\
        P_{k+1}(a) &= 2 \frac{L+\mu-2a}{L-\mu} P_{k}(a) \frac{t_{k}}{t_{k+1}} - P_{k-1}(a) \frac{t_{k-1}}{t_{k+1}}
        \end{aligned}
        $$
    1. Поскольку мы имеем $P_{k+1}(0) = P_{k}(0) = P_{k-1}(0) = 1$, мы можем записать метод в следующей форме:
        $$
        P_{k+1}(a) = (1 - \alpha_k a) P_k(a) + \beta_k \left(P_{k}(a) - P_{k-1}(a) \right).
        $$
    1. Перегруппируя члены, мы получаем:
        $$
        \begin{aligned}
        P_{k+1}(a) &= (1 + \beta_k) P_k(a) - \alpha_k a P_k(a) - \beta_k P_{k-1}(a),\\
        P_{k+1}(a) &= 2 \frac{L+\mu}{L-\mu}  \frac{t_{k}}{t_{k+1}} P_{k}(a) - \frac{4a}{L-\mu}  \frac{t_{k}}{t_{k+1}}P_{k}(a) - \frac{t_{k-1}}{t_{k+1}} P_{k-1}(a)
        \end{aligned}
        $$
        $$
        \begin{cases}
        \beta_k = \dfrac{t_{k-1}}{t_{k+1}}, \\[6pt]
        \alpha_k = \dfrac{4}{L-\mu} \dfrac{t_k}{t_{k+1}}, \\[6pt]
        1 + \beta_k = 2 \dfrac{L + \mu}{L - \mu} \dfrac{t_k}{t_{k+1}}
        \end{cases}
        $$
    1. Мы почти закончили \faSmile. Помним, что $e_{k+1} = P_{k+1}(A) e_0$. Также отметим, что мы работаем с квадратичной задачей, поэтому мы можем предположить $x^* = 0$ без потери общности. В этом случае $e_0 = x_0$ и $e_{k+1} = x_{k+1}$.
        $$
        \begin{aligned}
        x_{k+1} &= P_{k+1}(A) x_0 =  (I - \alpha_k A) P_k(A) x_0 + \beta_k \left(P_{k}(A) - P_{k-1}(A) \right) x_0 \\
        &= (I - \alpha_k A) x_k + \beta_k \left(x_k - x_{k-1}\right)
        \end{aligned}
        $$
    1. Для квадратичной задачи мы имеем $\nabla f(x_k) = A x_k$, поэтому мы можем переписать обновление как:
    $$
    \boxed{
    x_{k+1} = x_k - \alpha_k \nabla f(x_k) + \beta_k \left(x_k - x_{k-1}\right)
    }
    $$
    
    [![](chebyshev_gd.pdf)](https://fmin.xyz/docs/visualizations/chebyshev_gd.mp4)

1.  Доказательство сходимости метода сопряженных градиентов и вывод формул метода (В этом вопросе необходимо доказать за какое количество шагов сходится метод, как выбираются направления и почему в $A$-ортогонализации достаточно хранить лишь предыдущий шаг метода, а не все предыдущие).

    :::{.callout-note appearance="simple"}
    $$
    \begin{aligned}
    & \mathbf{r}_0 := \mathbf{b} - \mathbf{A x}_0 \\
    & \hbox{Если } \mathbf{r}_{0} \text{ достаточно мала, то вернуть } \mathbf{x}_{0} \text{ как результат}\\
    & \mathbf{d}_0 := \mathbf{r}_0 \\
    & k := 0 \\
    & \text{повторить} \\
    & \qquad \alpha_k := \frac{\mathbf{r}_k^\mathsf{T} \mathbf{r}_k}{\mathbf{d}_k^\mathsf{T} \mathbf{A d}_k}  \\
    & \qquad \mathbf{x}_{k+1} := \mathbf{x}_k + \alpha_k \mathbf{d}_k \\
    & \qquad \mathbf{r}_{k+1} := \mathbf{r}_k - \alpha_k \mathbf{A d}_k \\
    & \qquad \hbox{Если } \mathbf{r}_{k+1} \text{ достаточно мала, то выйти из цикла} \\
    & \qquad \beta_k := \frac{\mathbf{r}_{k+1}^\mathsf{T} \mathbf{r}_{k+1}}{\mathbf{r}_k^\mathsf{T} \mathbf{r}_k} \\
    & \qquad \mathbf{d}_{k+1} := \mathbf{r}_{k+1} + \beta_k \mathbf{d}_k \\
    & \qquad k := k + 1 \\
    & \text{конец повторения} \\
    & \text{вернуть } \mathbf{x}_{k+1} \text{ как результат}
    \end{aligned}
    $$
    :::

    Рассмотрим следующую квадратичную задачу оптимизации:
    $$
    \min\limits_{x \in \mathbb{R}^n} f(x) =  \min\limits_{x \in \mathbb{R}^n} \dfrac{1}{2} x^\top  A x - b^\top  x + c, \text{ где }A \in \mathbb{S}^n_{++}.
    $$ {#eq-main_problem}

    Берём в арсенал процесс Грам-Шмидта:
    $$
    d_k = u_k + \sum\limits_{i=0}^{k-1}\beta_{ik} d_i \qquad \beta_{ik} = - \dfrac{\langle d_i, u_k \rangle}{\langle d_i, d_i \rangle}
    $$ {#eq-GS}

    **Лемма 1. Линейная независимость $A$-ортогональных векторов.**

    Если множество векторов $d_1, \ldots, d_n$ - попарно $A$-ортогональны (каждая пара векторов $A$-ортогональна), то эти векторы линейно независимы. $A \in \mathbb{S}^n_{++}$.

    **Доказательство**

    Покажем, что если $\sum\limits_{i=1}^n\alpha_i d_i = 0$, то все коэффициенты должны быть равны нулю:
    $$
    \begin{aligned}
    0 &= \sum\limits_{i=1}^n\alpha_i d_i \\
    \text{Умножаем на } d_j^T A \cdot \qquad &= d_j^\top A \left( \sum\limits_{i=1}^n\alpha_i d_i \right)
    =  \sum\limits_{i=1}^n \alpha_i d_j^\top A d_i  \\
    &=  \alpha_j d_j^\top A d_j  + 0 + \ldots + 0
    \end{aligned}
    $$
    Таким образом, $\alpha_j = 0$, для всех остальных индексов нужно проделать тот же процесс

    Введем следующие обозначения:

    * $r_k = b - Ax_k$ - невязка
    * $e_k = x_k - x^*$ - ошибка
    * Поскольку $Ax^* = b$, имеем $r_k = b - Ax_k = Ax^* - Ax_k = -A (x_k - x^*)$
        $$
        r_k = -Ae_k.
        $$ {#eq-res_error}
    * Также заметим, что поскольку $x_{k+1} = x_0 + \sum\limits_{i=1}^k\alpha_i d_i$, имеем 
        $$
        e_{k+1} = e_0 + \sum\limits_{i=1}^k\alpha_i d_i.
        $$ {#eq-err_decomposition}

    **Лемма 2. Сходимость метода сопряженных направлений.**

    Предположим, мы решаем $n$-мерную квадратичную сильно выпуклую задачу оптимизации ([-@eq-main_problem]). Метод сопряженных направлений
        $$
        x_{k+1} = x_0 + \sum\limits_{i=0}^k\alpha_i d_i
        $$
    с $\alpha_i = \frac{\langle d_i, r_i \rangle}{\langle d_i, Ad_i \rangle}$ взятым из точного линейного поиска, сходится за не более $n$ шагов алгоритма.

    **Доказательство**

    1. Нужно доказать, что $\delta_i = - \alpha_i$:
        $$
        e_0 = x_0 - x^* =  \sum\limits_{i=0}^{n-1}\delta_i d_i
        $$
    2. Умножаем обе части слева на $d_k^T A$:
        $$
        \begin{aligned}
        d_k^T Ae_0 &= \sum\limits_{i=0}^{n-1}\delta_i d_k^T A d_i = \delta_k d_k^T A d_k \\
        d_k^T A\left(e_0 + \sum\limits_{i=0}^{k-1}\alpha_i d_i \right) = d_k^T A e_k &= \delta_k d_k^T A d_k \quad \left(A-\text{ ортогональность}\right)\\
        \delta_k = \frac{ d_k^T A e_k}{d_k^T A d_k } &= -\frac{ d_k^T r_k}{d_k^T A d_k } \Leftrightarrow \delta_k = - \alpha_k
        \end{aligned}
        $$

    **Лемма 3. Разложение ошибки.**

    $$
    e_i = \sum\limits_{j=i}^{n-1}-\alpha_j d_j
    $$ {#eq-err_decomposition}

    **Доказательство**

    По определению
    $$
    e_{i} = e_0 + \sum\limits_{j=0}^{i-1}\alpha_j d_j = x_0 - x^* + \sum\limits_{j=0}^{i-1}\alpha_j d_j = -\sum\limits_{j=0}^{n-1}\alpha_j d_j + \sum\limits_{j=0}^{i-1}\alpha_j d_j = \sum\limits_{j=i}^{n-1}-\alpha_j d_j
    $$

    **Лемма 4. Невязка ортогональна всем предыдущим направлениям для CD.**

    Рассмотрим невязку метода сопряженных направлений на $k$ итерации $r_k$, тогда для любого $i < k$:
    $$
    d_i^T r_k = 0
    $$ {#eq-res_orth_dir}

    **Доказательство**

    Запишем ([-@eq-err_decomposition]) для некоторого фиксированного индекса $k$:
    $$
    e_k = \sum\limits_{j=k}^{n-1}-\alpha_j d_j 
    $$
    Умножаем обе части на $-d_i^TA \cdot$
    $$
    -d_i^TA e_k = \sum\limits_{j=k}^{n-1}\alpha_j d_i^TA d_j  = 0
    $$

    ![](CG_lem1.pdf){fig-align="center"}

    Таким образом, $d_i^T r_k = 0$ и невязка $r_k$ ортогональна всем предыдущим направлениям $d_i$ для метода CD.

    **Идея метода сопряженных градиентов (CG)**

    * Это буквально метод сопряженных направлений, в котором мы выбираем специальный набор $d_0, \ldots, d_{n-1}$, позволяющий значительно ускорить процесс Грама-Шмидта.
    * Используется процесс Грама-Шмидта с $A$-ортогональностью вместо Евклидовой ортогональности, чтобы получить их из набора начальных векторов.
    * На каждой итерации $r_0, \ldots, r_{n-1}$ используются в качестве начальных векторов для процесса Грама-Шмидта.
    * Основная идея заключается в том, что для произвольного метода CD процесс Грама-Шмидта вычислительно дорогой и требует квадратичного числа операций сложения векторов и скалярных произведений $\mathcal{O}\left( n^2\right)$, в то время как в случае CG мы покажем, что сложность этой процедуры может быть уменьшена до линейной $\mathcal{O}\left( n\right)$.

    $$
    \text{CG} = \text{CD} + r_0, \ldots, r_{n-1} \text{ как начальные векторы для процесса Грама-Шмидта} + A\text{-ортогональность.}
    $$

    **Лемма 5. Невязки ортогональны друг другу в методе CG**

    Все невязки в методе CG ортогональны друг другу:
    $$
    r_i^T r_k = 0 \qquad \forall i \neq k
    $$ {#eq-res_orth_cg}

    **Доказательство**

    Запишем процесс Грама-Шмидта ([-@eq-GS]) с $\langle \cdot, \cdot \rangle$ замененным на $\langle \cdot, \cdot \rangle_A = x^T A y$
    $$
    d_i = u_i + \sum\limits_{j=0}^{i-1}\beta_{ji} d_j \;\; \beta_{ji} = - \dfrac{\langle d_j, u_i \rangle_A}{\langle d_j, d_j \rangle_A}
    $$ {#eq-gs_cg1}

    Тогда, мы используем невязки в качестве начальных векторов для процесса и $u_i = r_i$.

    $$ 
    d_i = r_i + \sum\limits_{j=0}^{i-1}\beta_{ji} d_j \;\; \beta_{ji} = - \dfrac{\langle d_j, r_i \rangle_A}{\langle d_j, d_j \rangle_A}
    $$ {#eq-gs_cg2}

    ![](CG_lem1.pdf){fig-align="center"}

    Умножаем обе части ([-@eq-gs_cg1]) на $r_k^T \cdot$ для некоторого индекса $k$:
    $$
    r_k^Td_i = r_k^Tu_i + \sum\limits_{j=0}^{i-1}\beta_{ji} r_k^Td_j 
    $$

    Если $j < i < k$, то имеем лемму 4 с $d_i^T r_k = 0$ и $d_j^T r_k = 0$. Имеем:
    $$
    r_k^Tu_i= 0 \;\text{ для CD} \;\; r_k^Tr_i = 0 \;\text{ для CG}
    $$
    Более того, если $k=i$:
    $$
    r_k^Td_k = r_k^Tu_k + \sum\limits_{j=0}^{k-1}\beta_{jk} r_k^Td_j = r_k^Tu_k + 0,
    $$
    и мы имеем для любого $k$ (из-за произвольного выбора $i$):
    $$
    r_k^Td_k = r_k^Tu_k.
    $$ {#eq-lemma5}

    **Лемма 6. Пересчет невязки**
    $$
    r_{k+1} = r_k - \alpha_k A d_k 
    $$ {#eq-res_recalculation}
    $$
    r_{k+1} = -A e_{k+1} = -A \left( e_{k} + \alpha_k d_k \right) = -A e_{k} - \alpha_k A d_k = r_k - \alpha_k A d_k 
    $$
    Наконец, все эти вышеуказанные леммы достаточны для доказательства, что $\beta_{ji} = 0$ для всех $i,j$, кроме соседних.

    **Грам-Шмидт в методе CG**

    Рассмотрим процесс Грам-Шмидта в методе CG:
    $$
    \beta_{ji} = - \dfrac{\langle d_j, u_i \rangle_A}{\langle d_j, d_j \rangle_A} = - \dfrac{ d_j^T A u_i }{ d_j^T A d_j } = - \dfrac{ d_j^T A r_i }{ d_j^T A d_j } = - \dfrac{r_i^T A d_j}{ d_j^T A d_j }.
    $$
    Рассмотрим скалярное произведение $\langle r_i, r_{j+1} \rangle$ используя ([-@eq-res_recalculation]):
    $$
    \begin{aligned}
    \langle r_i, r_{j+1} \rangle &= \langle r_i, r_j - \alpha_j A d_j  \rangle = \langle r_i, r_j \rangle - \alpha_j\langle r_i, A d_j  \rangle \\
    \alpha_j\langle r_i, A d_j  \rangle &= \langle r_i, r_j \rangle - \langle r_i, r_{j+1} \rangle
    \end{aligned}
    $$

    * Если $i=j$: $\alpha_i\langle r_i, A d_i  \rangle = \langle r_i, r_i \rangle - \langle r_i, r_{i+1} \rangle = \langle r_i, r_i \rangle$. Этот случай не интересен по построению процесса Грам-Шмидта.
    * Соседний случай $i=j + 1$: $\alpha_j\langle r_i, A d_j \rangle = \langle r_i, r_{i-1} \rangle - \langle r_i, r_{i} \rangle = - \langle r_i, r_i \rangle$
    * Для любого другого случая: $\alpha_j\langle r_i, A d_j \rangle = 0$, потому что все невязки ортогональны друг другу.

    Наконец, мы имеем формулу для $i=j + 1$:
    $$
    \beta_{ji} = - \dfrac{r_i^T A d_j}{ d_j^T A d_j} = \dfrac{1}{\alpha_j}\dfrac{\langle r_i, r_i \rangle}{ d_j^T A d_j} =  \dfrac{d_j^T A d_j}{d_j^T r_j}\dfrac{\langle r_i, r_i \rangle}{ d_j^T A d_j} = \dfrac{\langle r_i, r_i \rangle}{\langle r_j, r_j \rangle} = \dfrac{\langle r_i, r_i \rangle}{\langle r_{i-1}, r_{i-1} \rangle}
    $$

    И для направления $d_{k+1} = r_{k+1} + \beta_{k,k+1} d_k, \qquad  \beta_{k,k+1} = \beta_k = \dfrac{\langle r_{k+1}, r_{k+1} \rangle}{\langle r_{k}, r_{k} \rangle}.$